
# AUTOGENERATED ON 2020-01-10 15:27:57.598837
## DO NOT MODIFY THIS FILE MANUALLY

from autogoal.grammar import Continuous, Discrete, Categorical, Boolean
from numpy import inf, nan

from sklearn.cluster._affinity_propagation import AffinityPropagation as _AffinityPropagation

class AffinityPropagation(_AffinityPropagation):
    def __init__(
        self,
        damping: Continuous(min=1e-06, max=1),
        convergence_iter: Discrete(min=7, max=30)
    ):
        self.damping=damping
        self.convergence_iter=convergence_iter

        super().__init__(
            damping=damping,
            convergence_iter=convergence_iter
        )


from sklearn.cluster._bicluster import SpectralBiclustering as _SpectralBiclustering

class SpectralBiclustering(_SpectralBiclustering):
    def __init__(
        self,
        n_clusters: Discrete(min=1, max=6),
        method: Categorical('bistochastic', 'log', 'scale'),
        n_components: Discrete(min=3, max=12),
        n_best: Discrete(min=1, max=6),
        svd_method: Categorical('arpack', 'randomized'),
        mini_batch: Boolean(),
        init: Categorical('random'),
        n_init: Discrete(min=5, max=20)
    ):
        self.n_clusters=n_clusters
        self.method=method
        self.n_components=n_components
        self.n_best=n_best
        self.svd_method=svd_method
        self.mini_batch=mini_batch
        self.init=init
        self.n_init=n_init

        super().__init__(
            n_clusters=n_clusters,
            method=method,
            n_components=n_components,
            n_best=n_best,
            svd_method=svd_method,
            mini_batch=mini_batch,
            init=init,
            n_init=n_init
        )


from sklearn.cluster._bicluster import SpectralCoclustering as _SpectralCoclustering

class SpectralCoclustering(_SpectralCoclustering):
    def __init__(
        self,
        n_clusters: Discrete(min=1, max=6),
        svd_method: Categorical('arpack', 'randomized'),
        mini_batch: Boolean(),
        init: Categorical('random'),
        n_init: Discrete(min=5, max=20)
    ):
        self.n_clusters=n_clusters
        self.svd_method=svd_method
        self.mini_batch=mini_batch
        self.init=init
        self.n_init=n_init

        super().__init__(
            n_clusters=n_clusters,
            svd_method=svd_method,
            mini_batch=mini_batch,
            init=init,
            n_init=n_init
        )


from sklearn.cluster._birch import Birch as _Birch

class Birch(_Birch):
    def __init__(
        self,
        threshold: Continuous(min=1e-06, max=1),
        branching_factor: Discrete(min=25, max=100),
        n_clusters: Discrete(min=1, max=6),
        compute_labels: Boolean()
    ):
        self.threshold=threshold
        self.branching_factor=branching_factor
        self.n_clusters=n_clusters
        self.compute_labels=compute_labels

        super().__init__(
            threshold=threshold,
            branching_factor=branching_factor,
            n_clusters=n_clusters,
            compute_labels=compute_labels
        )


from sklearn.cluster._dbscan import DBSCAN as _DBSCAN

class DBSCAN(_DBSCAN):
    def __init__(
        self,
        min_samples: Discrete(min=2, max=10),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60)
    ):
        self.min_samples=min_samples
        self.algorithm=algorithm
        self.leaf_size=leaf_size

        super().__init__(
            min_samples=min_samples,
            algorithm=algorithm,
            leaf_size=leaf_size
        )


from sklearn.cluster._hierarchical import AgglomerativeClustering as _AgglomerativeClustering

class AgglomerativeClustering(_AgglomerativeClustering):
    def __init__(
        self,
        n_clusters: Discrete(min=1, max=4),
        compute_full_tree: Categorical('auto')
    ):
        self.n_clusters=n_clusters
        self.compute_full_tree=compute_full_tree

        super().__init__(
            n_clusters=n_clusters,
            compute_full_tree=compute_full_tree
        )


from sklearn.cluster._hierarchical import FeatureAgglomeration as _FeatureAgglomeration

class FeatureAgglomeration(_FeatureAgglomeration):
    def __init__(
        self,
        n_clusters: Discrete(min=1, max=4),
        compute_full_tree: Categorical('auto')
    ):
        self.n_clusters=n_clusters
        self.compute_full_tree=compute_full_tree

        super().__init__(
            n_clusters=n_clusters,
            compute_full_tree=compute_full_tree
        )


from sklearn.cluster._k_means import KMeans as _KMeans

class KMeans(_KMeans):
    def __init__(
        self,
        n_clusters: Discrete(min=4, max=16),
        init: Categorical('random'),
        n_init: Discrete(min=5, max=20),
        tol: Continuous(min=1e-06, max=1),
        precompute_distances: Categorical('auto')
    ):
        self.n_clusters=n_clusters
        self.init=init
        self.n_init=n_init
        self.tol=tol
        self.precompute_distances=precompute_distances

        super().__init__(
            n_clusters=n_clusters,
            init=init,
            n_init=n_init,
            tol=tol,
            precompute_distances=precompute_distances
        )


from sklearn.cluster._k_means import MiniBatchKMeans as _MiniBatchKMeans

class MiniBatchKMeans(_MiniBatchKMeans):
    def __init__(
        self,
        n_clusters: Discrete(min=4, max=16),
        init: Categorical('random'),
        batch_size: Discrete(min=50, max=200),
        compute_labels: Boolean(),
        tol: Continuous(min=-1, max=1),
        max_no_improvement: Discrete(min=5, max=20),
        n_init: Discrete(min=1, max=6),
        reassignment_ratio: Continuous(min=0.0001, max=1)
    ):
        self.n_clusters=n_clusters
        self.init=init
        self.batch_size=batch_size
        self.compute_labels=compute_labels
        self.tol=tol
        self.max_no_improvement=max_no_improvement
        self.n_init=n_init
        self.reassignment_ratio=reassignment_ratio

        super().__init__(
            n_clusters=n_clusters,
            init=init,
            batch_size=batch_size,
            compute_labels=compute_labels,
            tol=tol,
            max_no_improvement=max_no_improvement,
            n_init=n_init,
            reassignment_ratio=reassignment_ratio
        )


from sklearn.cluster._mean_shift import MeanShift as _MeanShift

class MeanShift(_MeanShift):
    def __init__(
        self,
        bin_seeding: Boolean(),
        min_bin_freq: Discrete(min=0, max=2),
        cluster_all: Boolean()
    ):
        self.bin_seeding=bin_seeding
        self.min_bin_freq=min_bin_freq
        self.cluster_all=cluster_all

        super().__init__(
            bin_seeding=bin_seeding,
            min_bin_freq=min_bin_freq,
            cluster_all=cluster_all
        )


from sklearn.cluster._optics import OPTICS as _OPTICS

class OPTICS(_OPTICS):
    def __init__(
        self,
        min_samples: Discrete(min=2, max=10),
        max_eps: Continuous(min=inf, max=inf),
        metric: Categorical('braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'l1', 'l2', 'manhattan', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'),
        p: Discrete(min=1, max=4),
        cluster_method: Categorical('xi'),
        xi: Continuous(min=0.0005, max=1),
        predecessor_correction: Boolean(),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60)
    ):
        self.min_samples=min_samples
        self.max_eps=max_eps
        self.metric=metric
        self.p=p
        self.cluster_method=cluster_method
        self.xi=xi
        self.predecessor_correction=predecessor_correction
        self.algorithm=algorithm
        self.leaf_size=leaf_size

        super().__init__(
            min_samples=min_samples,
            max_eps=max_eps,
            metric=metric,
            p=p,
            cluster_method=cluster_method,
            xi=xi,
            predecessor_correction=predecessor_correction,
            algorithm=algorithm,
            leaf_size=leaf_size
        )


from sklearn.cluster._spectral import SpectralClustering as _SpectralClustering

class SpectralClustering(_SpectralClustering):
    def __init__(
        self,
        n_clusters: Discrete(min=4, max=16),
        n_init: Discrete(min=5, max=20),
        gamma: Continuous(min=1e-06, max=1),
        affinity: Categorical('nearest_neighbors', 'precomputed_nearest_neighbors', 'rbf'),
        n_neighbors: Discrete(min=5, max=20),
        eigen_tol: Continuous(min=-1, max=1),
        assign_labels: Categorical('discretize', 'kmeans'),
        degree: Discrete(min=1, max=6),
        coef0: Discrete(min=0, max=2)
    ):
        self.n_clusters=n_clusters
        self.n_init=n_init
        self.gamma=gamma
        self.affinity=affinity
        self.n_neighbors=n_neighbors
        self.eigen_tol=eigen_tol
        self.assign_labels=assign_labels
        self.degree=degree
        self.coef0=coef0

        super().__init__(
            n_clusters=n_clusters,
            n_init=n_init,
            gamma=gamma,
            affinity=affinity,
            n_neighbors=n_neighbors,
            eigen_tol=eigen_tol,
            assign_labels=assign_labels,
            degree=degree,
            coef0=coef0
        )


from sklearn.cross_decomposition._cca import CCA as _CCA

class CCA(_CCA):
    def __init__(
        self,
        n_components: Discrete(min=1, max=4),
        scale: Boolean(),
        tol: Continuous(min=1e-08, max=1)
    ):
        self.n_components=n_components
        self.scale=scale
        self.tol=tol

        super().__init__(
            n_components=n_components,
            scale=scale,
            tol=tol
        )


from sklearn.cross_decomposition._pls import PLSCanonical as _PLSCanonical

class PLSCanonical(_PLSCanonical):
    def __init__(
        self,
        n_components: Discrete(min=1, max=4),
        scale: Boolean(),
        tol: Continuous(min=1e-08, max=1)
    ):
        self.n_components=n_components
        self.scale=scale
        self.tol=tol

        super().__init__(
            n_components=n_components,
            scale=scale,
            tol=tol
        )


from sklearn.cross_decomposition._pls import PLSRegression as _PLSRegression

class PLSRegression(_PLSRegression):
    def __init__(
        self,
        n_components: Discrete(min=1, max=4),
        scale: Boolean(),
        tol: Continuous(min=1e-08, max=1)
    ):
        self.n_components=n_components
        self.scale=scale
        self.tol=tol

        super().__init__(
            n_components=n_components,
            scale=scale,
            tol=tol
        )


from sklearn.cross_decomposition._pls import PLSSVD as _PLSSVD

class PLSSVD(_PLSSVD):
    def __init__(
        self,
        n_components: Discrete(min=1, max=4),
        scale: Boolean()
    ):
        self.n_components=n_components
        self.scale=scale

        super().__init__(
            n_components=n_components,
            scale=scale
        )


from sklearn.decomposition._dict_learning import DictionaryLearning as _DictionaryLearning

class DictionaryLearning(_DictionaryLearning):
    def __init__(
        self,
        alpha: Discrete(min=0, max=2),
        tol: Continuous(min=1e-10, max=1),
        fit_algorithm: Categorical('cd', 'lars'),
        transform_algorithm: Categorical('lars', 'lasso_cd', 'lasso_lars', 'omp', 'threshold'),
        split_sign: Boolean(),
        positive_code: Boolean(),
        positive_dict: Boolean(),
        transform_max_iter: Discrete(min=500, max=2000)
    ):
        self.alpha=alpha
        self.tol=tol
        self.fit_algorithm=fit_algorithm
        self.transform_algorithm=transform_algorithm
        self.split_sign=split_sign
        self.positive_code=positive_code
        self.positive_dict=positive_dict
        self.transform_max_iter=transform_max_iter

        super().__init__(
            alpha=alpha,
            tol=tol,
            fit_algorithm=fit_algorithm,
            transform_algorithm=transform_algorithm,
            split_sign=split_sign,
            positive_code=positive_code,
            positive_dict=positive_dict,
            transform_max_iter=transform_max_iter
        )


from sklearn.decomposition._dict_learning import MiniBatchDictionaryLearning as _MiniBatchDictionaryLearning

class MiniBatchDictionaryLearning(_MiniBatchDictionaryLearning):
    def __init__(
        self,
        alpha: Discrete(min=0, max=2),
        n_iter: Discrete(min=500, max=2000),
        fit_algorithm: Categorical('cd', 'lars'),
        batch_size: Discrete(min=1, max=6),
        shuffle: Boolean(),
        transform_algorithm: Categorical('lars', 'lasso_cd', 'lasso_lars', 'omp', 'threshold'),
        split_sign: Boolean(),
        positive_code: Boolean(),
        positive_dict: Boolean(),
        transform_max_iter: Discrete(min=500, max=2000)
    ):
        self.alpha=alpha
        self.n_iter=n_iter
        self.fit_algorithm=fit_algorithm
        self.batch_size=batch_size
        self.shuffle=shuffle
        self.transform_algorithm=transform_algorithm
        self.split_sign=split_sign
        self.positive_code=positive_code
        self.positive_dict=positive_dict
        self.transform_max_iter=transform_max_iter

        super().__init__(
            alpha=alpha,
            n_iter=n_iter,
            fit_algorithm=fit_algorithm,
            batch_size=batch_size,
            shuffle=shuffle,
            transform_algorithm=transform_algorithm,
            split_sign=split_sign,
            positive_code=positive_code,
            positive_dict=positive_dict,
            transform_max_iter=transform_max_iter
        )


from sklearn.decomposition._dict_learning import SparseCoder as _SparseCoder

class SparseCoder(_SparseCoder):
    def __init__(
        self,
        split_sign: Boolean(),
        positive_code: Boolean(),
        transform_max_iter: Discrete(min=500, max=2000)
    ):
        self.split_sign=split_sign
        self.positive_code=positive_code
        self.transform_max_iter=transform_max_iter

        super().__init__(
            split_sign=split_sign,
            positive_code=positive_code,
            transform_max_iter=transform_max_iter
        )


from sklearn.decomposition._factor_analysis import FactorAnalysis as _FactorAnalysis

class FactorAnalysis(_FactorAnalysis):
    def __init__(
        self,
        tol: Continuous(min=0.0001, max=1),
        svd_method: Categorical('lapack', 'randomized'),
        iterated_power: Discrete(min=1, max=6)
    ):
        self.tol=tol
        self.svd_method=svd_method
        self.iterated_power=iterated_power

        super().__init__(
            tol=tol,
            svd_method=svd_method,
            iterated_power=iterated_power
        )


from sklearn.decomposition._fastica import FastICA as _FastICA

class FastICA(_FastICA):
    def __init__(
        self,
        whiten: Boolean(),
        tol: Continuous(min=1e-06, max=1)
    ):
        self.whiten=whiten
        self.tol=tol

        super().__init__(
            whiten=whiten,
            tol=tol
        )


from sklearn.decomposition._incremental_pca import IncrementalPCA as _IncrementalPCA

class IncrementalPCA(_IncrementalPCA):
    def __init__(
        self,
        whiten: Boolean()
    ):
        self.whiten=whiten

        super().__init__(
            whiten=whiten
        )


from sklearn.decomposition._kernel_pca import KernelPCA as _KernelPCA

class KernelPCA(_KernelPCA):
    def __init__(
        self,
        degree: Discrete(min=1, max=6),
        coef0: Discrete(min=0, max=2),
        alpha: Continuous(min=1e-06, max=1),
        fit_inverse_transform: Boolean(),
        eigen_solver: Categorical('arpack', 'auto', 'dense'),
        tol: Discrete(min=-100, max=100),
        remove_zero_eig: Boolean()
    ):
        self.degree=degree
        self.coef0=coef0
        self.alpha=alpha
        self.fit_inverse_transform=fit_inverse_transform
        self.eigen_solver=eigen_solver
        self.tol=tol
        self.remove_zero_eig=remove_zero_eig

        super().__init__(
            degree=degree,
            coef0=coef0,
            alpha=alpha,
            fit_inverse_transform=fit_inverse_transform,
            eigen_solver=eigen_solver,
            tol=tol,
            remove_zero_eig=remove_zero_eig
        )


from sklearn.decomposition._nmf import NMF as _NMF

class NMF(_NMF):
    def __init__(
        self,
        solver: Categorical('cd', 'mu'),
        beta_loss: Categorical('frobenius'),
        tol: Continuous(min=1e-06, max=1),
        alpha: Continuous(min=-1, max=1),
        l1_ratio: Continuous(min=-1, max=1),
        shuffle: Boolean()
    ):
        self.solver=solver
        self.beta_loss=beta_loss
        self.tol=tol
        self.alpha=alpha
        self.l1_ratio=l1_ratio
        self.shuffle=shuffle

        super().__init__(
            solver=solver,
            beta_loss=beta_loss,
            tol=tol,
            alpha=alpha,
            l1_ratio=l1_ratio,
            shuffle=shuffle
        )


from sklearn.decomposition._online_lda import LatentDirichletAllocation as _LatentDirichletAllocation

class LatentDirichletAllocation(_LatentDirichletAllocation):
    def __init__(
        self,
        n_components: Discrete(min=5, max=20),
        learning_method: Categorical('batch', 'online'),
        learning_decay: Continuous(min=1e-06, max=1),
        learning_offset: Continuous(min=5.0, max=20.0),
        batch_size: Discrete(min=64, max=256),
        evaluate_every: Discrete(min=-1, max=-2),
        total_samples: Continuous(min=500000.0, max=2000000.0),
        perp_tol: Continuous(min=0.001, max=1),
        mean_change_tol: Continuous(min=1e-05, max=1),
        max_doc_update_iter: Discrete(min=50, max=200)
    ):
        self.n_components=n_components
        self.learning_method=learning_method
        self.learning_decay=learning_decay
        self.learning_offset=learning_offset
        self.batch_size=batch_size
        self.evaluate_every=evaluate_every
        self.total_samples=total_samples
        self.perp_tol=perp_tol
        self.mean_change_tol=mean_change_tol
        self.max_doc_update_iter=max_doc_update_iter

        super().__init__(
            n_components=n_components,
            learning_method=learning_method,
            learning_decay=learning_decay,
            learning_offset=learning_offset,
            batch_size=batch_size,
            evaluate_every=evaluate_every,
            total_samples=total_samples,
            perp_tol=perp_tol,
            mean_change_tol=mean_change_tol,
            max_doc_update_iter=max_doc_update_iter
        )


from sklearn.decomposition._pca import PCA as _PCA

class PCA(_PCA):
    def __init__(
        self,
        whiten: Boolean(),
        svd_solver: Categorical('arpack', 'auto', 'full', 'randomized'),
        tol: Continuous(min=-1, max=1),
        iterated_power: Categorical('auto', 'randomized')
    ):
        self.whiten=whiten
        self.svd_solver=svd_solver
        self.tol=tol
        self.iterated_power=iterated_power

        super().__init__(
            whiten=whiten,
            svd_solver=svd_solver,
            tol=tol,
            iterated_power=iterated_power
        )


from sklearn.decomposition._sparse_pca import MiniBatchSparsePCA as _MiniBatchSparsePCA

class MiniBatchSparsePCA(_MiniBatchSparsePCA):
    def __init__(
        self,
        alpha: Discrete(min=0, max=2),
        ridge_alpha: Continuous(min=0.0001, max=1),
        n_iter: Discrete(min=50, max=200),
        batch_size: Discrete(min=1, max=6),
        shuffle: Boolean(),
        method: Categorical('cd', 'lars')
    ):
        self.alpha=alpha
        self.ridge_alpha=ridge_alpha
        self.n_iter=n_iter
        self.batch_size=batch_size
        self.shuffle=shuffle
        self.method=method

        super().__init__(
            alpha=alpha,
            ridge_alpha=ridge_alpha,
            n_iter=n_iter,
            batch_size=batch_size,
            shuffle=shuffle,
            method=method
        )


from sklearn.decomposition._sparse_pca import SparsePCA as _SparsePCA

class SparsePCA(_SparsePCA):
    def __init__(
        self,
        alpha: Discrete(min=0, max=2),
        ridge_alpha: Continuous(min=0.0001, max=1),
        tol: Continuous(min=1e-10, max=1),
        method: Categorical('cd', 'lars')
    ):
        self.alpha=alpha
        self.ridge_alpha=ridge_alpha
        self.tol=tol
        self.method=method

        super().__init__(
            alpha=alpha,
            ridge_alpha=ridge_alpha,
            tol=tol,
            method=method
        )


from sklearn.decomposition._truncated_svd import TruncatedSVD as _TruncatedSVD

class TruncatedSVD(_TruncatedSVD):
    def __init__(
        self,
        n_components: Discrete(min=1, max=4),
        n_iter: Discrete(min=2, max=10),
        tol: Continuous(min=-1, max=1)
    ):
        self.n_components=n_components
        self.n_iter=n_iter
        self.tol=tol

        super().__init__(
            n_components=n_components,
            n_iter=n_iter,
            tol=tol
        )


from sklearn.feature_extraction._dict_vectorizer import DictVectorizer as _DictVectorizer

class DictVectorizer(_DictVectorizer):
    def __init__(
        self,
        sparse: Boolean(),
        sort: Boolean()
    ):
        self.sparse=sparse
        self.sort=sort

        super().__init__(
            sparse=sparse,
            sort=sort
        )


from sklearn.feature_extraction._hashing import FeatureHasher as _FeatureHasher

class FeatureHasher(_FeatureHasher):
    def __init__(
        self,
        n_features: Discrete(min=524288, max=2097152),
        alternate_sign: Boolean()
    ):
        self.n_features=n_features
        self.alternate_sign=alternate_sign

        super().__init__(
            n_features=n_features,
            alternate_sign=alternate_sign
        )


from sklearn.feature_extraction.image import PatchExtractor as _PatchExtractor

class PatchExtractor(_PatchExtractor):
    def __init__(
        self,

    ):


        super().__init__(

        )


from sklearn.feature_extraction.text import CountVectorizer as _CountVectorizer

class CountVectorizer(_CountVectorizer):
    def __init__(
        self,
        lowercase: Boolean(),
        max_df: Continuous(min=1e-06, max=1),
        min_df: Discrete(min=0, max=2),
        binary: Boolean()
    ):
        self.lowercase=lowercase
        self.max_df=max_df
        self.min_df=min_df
        self.binary=binary

        super().__init__(
            lowercase=lowercase,
            max_df=max_df,
            min_df=min_df,
            binary=binary
        )


from sklearn.feature_extraction.text import HashingVectorizer as _HashingVectorizer

class HashingVectorizer(_HashingVectorizer):
    def __init__(
        self,
        input: Categorical('content', 'file', 'filename', 'read'),
        decode_error: Categorical('ignore', 'replace', 'strict'),
        lowercase: Boolean(),
        token_pattern: Categorical('word'),
        analyzer: Categorical('char', 'char_wb', 'word'),
        n_features: Discrete(min=524288, max=2097152),
        binary: Boolean(),
        norm: Categorical('l1'),
        alternate_sign: Boolean()
    ):
        self.input=input
        self.decode_error=decode_error
        self.lowercase=lowercase
        self.token_pattern=token_pattern
        self.analyzer=analyzer
        self.n_features=n_features
        self.binary=binary
        self.norm=norm
        self.alternate_sign=alternate_sign

        super().__init__(
            input=input,
            decode_error=decode_error,
            lowercase=lowercase,
            token_pattern=token_pattern,
            analyzer=analyzer,
            n_features=n_features,
            binary=binary,
            norm=norm,
            alternate_sign=alternate_sign
        )


from sklearn.feature_extraction.text import TfidfTransformer as _TfidfTransformer

class TfidfTransformer(_TfidfTransformer):
    def __init__(
        self,
        norm: Categorical('l1', 'l2'),
        use_idf: Boolean(),
        smooth_idf: Boolean(),
        sublinear_tf: Boolean()
    ):
        self.norm=norm
        self.use_idf=use_idf
        self.smooth_idf=smooth_idf
        self.sublinear_tf=sublinear_tf

        super().__init__(
            norm=norm,
            use_idf=use_idf,
            smooth_idf=smooth_idf,
            sublinear_tf=sublinear_tf
        )


from sklearn.feature_extraction.text import TfidfVectorizer as _TfidfVectorizer

class TfidfVectorizer(_TfidfVectorizer):
    def __init__(
        self,
        lowercase: Boolean(),
        max_df: Continuous(min=1e-06, max=1),
        min_df: Discrete(min=0, max=2),
        binary: Boolean(),
        use_idf: Boolean(),
        smooth_idf: Boolean(),
        sublinear_tf: Boolean()
    ):
        self.lowercase=lowercase
        self.max_df=max_df
        self.min_df=min_df
        self.binary=binary
        self.use_idf=use_idf
        self.smooth_idf=smooth_idf
        self.sublinear_tf=sublinear_tf

        super().__init__(
            lowercase=lowercase,
            max_df=max_df,
            min_df=min_df,
            binary=binary,
            use_idf=use_idf,
            smooth_idf=smooth_idf,
            sublinear_tf=sublinear_tf
        )


from sklearn.impute._base import MissingIndicator as _MissingIndicator

class MissingIndicator(_MissingIndicator):
    def __init__(
        self,
        missing_values: Continuous(min=nan, max=nan),
        error_on_new: Boolean()
    ):
        self.missing_values=missing_values
        self.error_on_new=error_on_new

        super().__init__(
            missing_values=missing_values,
            error_on_new=error_on_new
        )


from sklearn.impute._base import SimpleImputer as _SimpleImputer

class SimpleImputer(_SimpleImputer):
    def __init__(
        self,
        missing_values: Continuous(min=nan, max=nan),
        add_indicator: Boolean()
    ):
        self.missing_values=missing_values
        self.add_indicator=add_indicator

        super().__init__(
            missing_values=missing_values,
            add_indicator=add_indicator
        )


from sklearn.impute._knn import KNNImputer as _KNNImputer

class KNNImputer(_KNNImputer):
    def __init__(
        self,
        missing_values: Continuous(min=nan, max=nan),
        n_neighbors: Discrete(min=2, max=10),
        weights: Categorical('distance', 'uniform'),
        metric: Categorical('nan_euclidean'),
        add_indicator: Boolean()
    ):
        self.missing_values=missing_values
        self.n_neighbors=n_neighbors
        self.weights=weights
        self.metric=metric
        self.add_indicator=add_indicator

        super().__init__(
            missing_values=missing_values,
            n_neighbors=n_neighbors,
            weights=weights,
            metric=metric,
            add_indicator=add_indicator
        )


from sklearn.isotonic import IsotonicRegression as _IsotonicRegression

class IsotonicRegression(_IsotonicRegression):
    def __init__(
        self,
        increasing: Boolean()
    ):
        self.increasing=increasing

        super().__init__(
            increasing=increasing
        )


from sklearn.linear_model._base import LinearRegression as _LinearRegression

class LinearRegression(_LinearRegression):
    def __init__(
        self,
        fit_intercept: Boolean(),
        normalize: Boolean()
    ):
        self.fit_intercept=fit_intercept
        self.normalize=normalize

        super().__init__(
            fit_intercept=fit_intercept,
            normalize=normalize
        )


from sklearn.linear_model._bayes import ARDRegression as _ARDRegression

class ARDRegression(_ARDRegression):
    def __init__(
        self,
        n_iter: Discrete(min=150, max=600),
        tol: Continuous(min=1e-05, max=1),
        alpha_1: Continuous(min=1e-08, max=1),
        alpha_2: Continuous(min=1e-08, max=1),
        lambda_1: Continuous(min=1e-08, max=1),
        lambda_2: Continuous(min=1e-08, max=1),
        compute_score: Boolean(),
        threshold_lambda: Continuous(min=5000.0, max=20000.0),
        fit_intercept: Boolean(),
        normalize: Boolean()
    ):
        self.n_iter=n_iter
        self.tol=tol
        self.alpha_1=alpha_1
        self.alpha_2=alpha_2
        self.lambda_1=lambda_1
        self.lambda_2=lambda_2
        self.compute_score=compute_score
        self.threshold_lambda=threshold_lambda
        self.fit_intercept=fit_intercept
        self.normalize=normalize

        super().__init__(
            n_iter=n_iter,
            tol=tol,
            alpha_1=alpha_1,
            alpha_2=alpha_2,
            lambda_1=lambda_1,
            lambda_2=lambda_2,
            compute_score=compute_score,
            threshold_lambda=threshold_lambda,
            fit_intercept=fit_intercept,
            normalize=normalize
        )


from sklearn.linear_model._bayes import BayesianRidge as _BayesianRidge

class BayesianRidge(_BayesianRidge):
    def __init__(
        self,
        n_iter: Discrete(min=150, max=600),
        tol: Continuous(min=1e-05, max=1),
        alpha_1: Continuous(min=1e-08, max=1),
        alpha_2: Continuous(min=1e-08, max=1),
        lambda_1: Continuous(min=1e-08, max=1),
        lambda_2: Continuous(min=1e-08, max=1),
        compute_score: Boolean(),
        fit_intercept: Boolean(),
        normalize: Boolean()
    ):
        self.n_iter=n_iter
        self.tol=tol
        self.alpha_1=alpha_1
        self.alpha_2=alpha_2
        self.lambda_1=lambda_1
        self.lambda_2=lambda_2
        self.compute_score=compute_score
        self.fit_intercept=fit_intercept
        self.normalize=normalize

        super().__init__(
            n_iter=n_iter,
            tol=tol,
            alpha_1=alpha_1,
            alpha_2=alpha_2,
            lambda_1=lambda_1,
            lambda_2=lambda_2,
            compute_score=compute_score,
            fit_intercept=fit_intercept,
            normalize=normalize
        )


from sklearn.linear_model._coordinate_descent import ElasticNet as _ElasticNet

class ElasticNet(_ElasticNet):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        l1_ratio: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Boolean(),
        tol: Continuous(min=1e-06, max=1),
        positive: Boolean(),
        selection: Categorical('cyclic', 'random')
    ):
        self.alpha=alpha
        self.l1_ratio=l1_ratio
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute
        self.tol=tol
        self.positive=positive
        self.selection=selection

        super().__init__(
            alpha=alpha,
            l1_ratio=l1_ratio,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            tol=tol,
            positive=positive,
            selection=selection
        )


from sklearn.linear_model._coordinate_descent import Lasso as _Lasso

class Lasso(_Lasso):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Boolean(),
        tol: Continuous(min=1e-06, max=1),
        positive: Boolean(),
        selection: Categorical('cyclic', 'random')
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute
        self.tol=tol
        self.positive=positive
        self.selection=selection

        super().__init__(
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            tol=tol,
            positive=positive,
            selection=selection
        )


from sklearn.linear_model._coordinate_descent import MultiTaskElasticNet as _MultiTaskElasticNet

class MultiTaskElasticNet(_MultiTaskElasticNet):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        l1_ratio: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        tol: Continuous(min=1e-06, max=1)
    ):
        self.alpha=alpha
        self.l1_ratio=l1_ratio
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.tol=tol

        super().__init__(
            alpha=alpha,
            l1_ratio=l1_ratio,
            fit_intercept=fit_intercept,
            normalize=normalize,
            tol=tol
        )


from sklearn.linear_model._coordinate_descent import MultiTaskLasso as _MultiTaskLasso

class MultiTaskLasso(_MultiTaskLasso):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        tol: Continuous(min=1e-06, max=1)
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.tol=tol

        super().__init__(
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            tol=tol
        )


from sklearn.linear_model._huber import HuberRegressor as _HuberRegressor

class HuberRegressor(_HuberRegressor):
    def __init__(
        self,
        epsilon: Continuous(min=0.675, max=2.7),
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1.0000000000000001e-07, max=1)
    ):
        self.epsilon=epsilon
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.tol=tol

        super().__init__(
            epsilon=epsilon,
            alpha=alpha,
            fit_intercept=fit_intercept,
            tol=tol
        )


from sklearn.linear_model._least_angle import Lars as _Lars

class Lars(_Lars):
    def __init__(
        self,
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Categorical('auto'),
        n_nonzero_coefs: Discrete(min=250, max=1000),
        fit_path: Boolean()
    ):
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute
        self.n_nonzero_coefs=n_nonzero_coefs
        self.fit_path=fit_path

        super().__init__(
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            n_nonzero_coefs=n_nonzero_coefs,
            fit_path=fit_path
        )


from sklearn.linear_model._least_angle import LassoLars as _LassoLars

class LassoLars(_LassoLars):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Categorical('auto'),
        fit_path: Boolean(),
        positive: Boolean()
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute
        self.fit_path=fit_path
        self.positive=positive

        super().__init__(
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            fit_path=fit_path,
            positive=positive
        )


from sklearn.linear_model._least_angle import LassoLarsIC as _LassoLarsIC

class LassoLarsIC(_LassoLarsIC):
    def __init__(
        self,
        criterion: Categorical('aic', 'bic'),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Categorical('auto'),
        positive: Boolean()
    ):
        self.criterion=criterion
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute
        self.positive=positive

        super().__init__(
            criterion=criterion,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            positive=positive
        )


from sklearn.linear_model._logistic import LogisticRegression as _LogisticRegression

class LogisticRegression(_LogisticRegression):
    def __init__(
        self,
        penalty: Categorical('l2', 'none'),
        dual: Boolean(),
        tol: Continuous(min=1e-06, max=1),
        C: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        intercept_scaling: Discrete(min=0, max=2),
        solver: Categorical('lbfgs', 'liblinear', 'sag', 'saga'),
        multi_class: Categorical('auto', 'multinomial', 'ovr')
    ):
        self.penalty=penalty
        self.dual=dual
        self.tol=tol
        self.C=C
        self.fit_intercept=fit_intercept
        self.intercept_scaling=intercept_scaling
        self.solver=solver
        self.multi_class=multi_class

        super().__init__(
            penalty=penalty,
            dual=dual,
            tol=tol,
            C=C,
            fit_intercept=fit_intercept,
            intercept_scaling=intercept_scaling,
            solver=solver,
            multi_class=multi_class
        )


from sklearn.linear_model._omp import OrthogonalMatchingPursuit as _OrthogonalMatchingPursuit

class OrthogonalMatchingPursuit(_OrthogonalMatchingPursuit):
    def __init__(
        self,
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Categorical('auto')
    ):
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute

        super().__init__(
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute
        )


from sklearn.linear_model._passive_aggressive import PassiveAggressiveClassifier as _PassiveAggressiveClassifier

class PassiveAggressiveClassifier(_PassiveAggressiveClassifier):
    def __init__(
        self,
        C: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        early_stopping: Boolean(),
        validation_fraction: Continuous(min=0.001, max=1),
        n_iter_no_change: Discrete(min=2, max=10),
        shuffle: Boolean(),
        average: Boolean()
    ):
        self.C=C
        self.fit_intercept=fit_intercept
        self.tol=tol
        self.early_stopping=early_stopping
        self.validation_fraction=validation_fraction
        self.n_iter_no_change=n_iter_no_change
        self.shuffle=shuffle
        self.average=average

        super().__init__(
            C=C,
            fit_intercept=fit_intercept,
            tol=tol,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            shuffle=shuffle,
            average=average
        )


from sklearn.linear_model._passive_aggressive import PassiveAggressiveRegressor as _PassiveAggressiveRegressor

class PassiveAggressiveRegressor(_PassiveAggressiveRegressor):
    def __init__(
        self,
        C: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        early_stopping: Boolean(),
        validation_fraction: Continuous(min=0.001, max=1),
        n_iter_no_change: Discrete(min=2, max=10),
        shuffle: Boolean(),
        epsilon: Continuous(min=0.001, max=1),
        average: Boolean()
    ):
        self.C=C
        self.fit_intercept=fit_intercept
        self.tol=tol
        self.early_stopping=early_stopping
        self.validation_fraction=validation_fraction
        self.n_iter_no_change=n_iter_no_change
        self.shuffle=shuffle
        self.epsilon=epsilon
        self.average=average

        super().__init__(
            C=C,
            fit_intercept=fit_intercept,
            tol=tol,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            shuffle=shuffle,
            epsilon=epsilon,
            average=average
        )


from sklearn.linear_model._perceptron import Perceptron as _Perceptron

class Perceptron(_Perceptron):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        shuffle: Boolean(),
        eta0: Continuous(min=1e-06, max=1),
        early_stopping: Boolean(),
        validation_fraction: Continuous(min=0.001, max=1),
        n_iter_no_change: Discrete(min=2, max=10)
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.tol=tol
        self.shuffle=shuffle
        self.eta0=eta0
        self.early_stopping=early_stopping
        self.validation_fraction=validation_fraction
        self.n_iter_no_change=n_iter_no_change

        super().__init__(
            alpha=alpha,
            fit_intercept=fit_intercept,
            tol=tol,
            shuffle=shuffle,
            eta0=eta0,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change
        )


from sklearn.linear_model._ransac import RANSACRegressor as _RANSACRegressor

class RANSACRegressor(_RANSACRegressor):
    def __init__(
        self,
        max_trials: Discrete(min=50, max=200),
        max_skips: Continuous(min=inf, max=inf),
        stop_n_inliers: Continuous(min=inf, max=inf),
        stop_score: Continuous(min=inf, max=inf),
        stop_probability: Continuous(min=1e-06, max=1)
    ):
        self.max_trials=max_trials
        self.max_skips=max_skips
        self.stop_n_inliers=stop_n_inliers
        self.stop_score=stop_score
        self.stop_probability=stop_probability

        super().__init__(
            max_trials=max_trials,
            max_skips=max_skips,
            stop_n_inliers=stop_n_inliers,
            stop_score=stop_score,
            stop_probability=stop_probability
        )


from sklearn.linear_model._ridge import Ridge as _Ridge

class Ridge(_Ridge):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        solver: Categorical('auto', 'cholesky', 'lsqr', 'saga', 'sparse_cg', 'svd')
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.tol=tol
        self.solver=solver

        super().__init__(
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            tol=tol,
            solver=solver
        )


from sklearn.linear_model._ridge import RidgeClassifier as _RidgeClassifier

class RidgeClassifier(_RidgeClassifier):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        solver: Categorical('auto', 'cholesky', 'lsqr', 'saga', 'sparse_cg', 'svd')
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.tol=tol
        self.solver=solver

        super().__init__(
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            tol=tol,
            solver=solver
        )


from sklearn.linear_model._stochastic_gradient import SGDClassifier as _SGDClassifier

class SGDClassifier(_SGDClassifier):
    def __init__(
        self,
        loss: Categorical('epsilon_insensitive', 'hinge', 'huber', 'log', 'modified_huber', 'perceptron', 'squared_epsilon_insensitive', 'squared_hinge', 'squared_loss'),
        penalty: Categorical('elasticnet', 'l1', 'l2', 'none'),
        alpha: Continuous(min=1e-06, max=1),
        l1_ratio: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        shuffle: Boolean(),
        epsilon: Continuous(min=0.001, max=1),
        learning_rate: Categorical('optimal'),
        eta0: Continuous(min=-1, max=1),
        power_t: Continuous(min=1e-06, max=1),
        early_stopping: Boolean(),
        validation_fraction: Continuous(min=0.001, max=1),
        n_iter_no_change: Discrete(min=2, max=10),
        average: Boolean()
    ):
        self.loss=loss
        self.penalty=penalty
        self.alpha=alpha
        self.l1_ratio=l1_ratio
        self.fit_intercept=fit_intercept
        self.tol=tol
        self.shuffle=shuffle
        self.epsilon=epsilon
        self.learning_rate=learning_rate
        self.eta0=eta0
        self.power_t=power_t
        self.early_stopping=early_stopping
        self.validation_fraction=validation_fraction
        self.n_iter_no_change=n_iter_no_change
        self.average=average

        super().__init__(
            loss=loss,
            penalty=penalty,
            alpha=alpha,
            l1_ratio=l1_ratio,
            fit_intercept=fit_intercept,
            tol=tol,
            shuffle=shuffle,
            epsilon=epsilon,
            learning_rate=learning_rate,
            eta0=eta0,
            power_t=power_t,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            average=average
        )


from sklearn.linear_model._stochastic_gradient import SGDRegressor as _SGDRegressor

class SGDRegressor(_SGDRegressor):
    def __init__(
        self,
        loss: Categorical('epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_loss'),
        penalty: Categorical('elasticnet', 'l1', 'l2', 'none'),
        alpha: Continuous(min=1e-06, max=1),
        l1_ratio: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        shuffle: Boolean(),
        epsilon: Continuous(min=0.001, max=1),
        learning_rate: Categorical('adaptive', 'constant', 'invscaling', 'optimal'),
        eta0: Continuous(min=0.0001, max=1),
        power_t: Continuous(min=1e-06, max=1),
        early_stopping: Boolean(),
        validation_fraction: Continuous(min=0.001, max=1),
        n_iter_no_change: Discrete(min=2, max=10),
        average: Boolean()
    ):
        self.loss=loss
        self.penalty=penalty
        self.alpha=alpha
        self.l1_ratio=l1_ratio
        self.fit_intercept=fit_intercept
        self.tol=tol
        self.shuffle=shuffle
        self.epsilon=epsilon
        self.learning_rate=learning_rate
        self.eta0=eta0
        self.power_t=power_t
        self.early_stopping=early_stopping
        self.validation_fraction=validation_fraction
        self.n_iter_no_change=n_iter_no_change
        self.average=average

        super().__init__(
            loss=loss,
            penalty=penalty,
            alpha=alpha,
            l1_ratio=l1_ratio,
            fit_intercept=fit_intercept,
            tol=tol,
            shuffle=shuffle,
            epsilon=epsilon,
            learning_rate=learning_rate,
            eta0=eta0,
            power_t=power_t,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            average=average
        )


from sklearn.linear_model._theil_sen import TheilSenRegressor as _TheilSenRegressor

class TheilSenRegressor(_TheilSenRegressor):
    def __init__(
        self,
        fit_intercept: Boolean(),
        max_subpopulation: Continuous(min=5000.0, max=20000.0),
        tol: Continuous(min=1e-05, max=1)
    ):
        self.fit_intercept=fit_intercept
        self.max_subpopulation=max_subpopulation
        self.tol=tol

        super().__init__(
            fit_intercept=fit_intercept,
            max_subpopulation=max_subpopulation,
            tol=tol
        )


from sklearn.manifold._isomap import Isomap as _Isomap

class Isomap(_Isomap):
    def __init__(
        self,
        n_neighbors: Discrete(min=2, max=10),
        n_components: Discrete(min=1, max=4),
        eigen_solver: Categorical('arpack', 'auto', 'dense'),
        tol: Discrete(min=-100, max=100),
        path_method: Categorical('auto'),
        neighbors_algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        p: Discrete(min=1, max=4)
    ):
        self.n_neighbors=n_neighbors
        self.n_components=n_components
        self.eigen_solver=eigen_solver
        self.tol=tol
        self.path_method=path_method
        self.neighbors_algorithm=neighbors_algorithm
        self.p=p

        super().__init__(
            n_neighbors=n_neighbors,
            n_components=n_components,
            eigen_solver=eigen_solver,
            tol=tol,
            path_method=path_method,
            neighbors_algorithm=neighbors_algorithm,
            p=p
        )


from sklearn.manifold._locally_linear import LocallyLinearEmbedding as _LocallyLinearEmbedding

class LocallyLinearEmbedding(_LocallyLinearEmbedding):
    def __init__(
        self,
        n_neighbors: Discrete(min=2, max=10),
        n_components: Discrete(min=1, max=4),
        reg: Continuous(min=1e-05, max=1),
        eigen_solver: Categorical('arpack', 'auto', 'dense'),
        tol: Continuous(min=1e-08, max=1),
        method: Categorical('ltsa', 'modified', 'standard'),
        hessian_tol: Continuous(min=1e-06, max=1),
        modified_tol: Continuous(min=1e-14, max=1),
        neighbors_algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree')
    ):
        self.n_neighbors=n_neighbors
        self.n_components=n_components
        self.reg=reg
        self.eigen_solver=eigen_solver
        self.tol=tol
        self.method=method
        self.hessian_tol=hessian_tol
        self.modified_tol=modified_tol
        self.neighbors_algorithm=neighbors_algorithm

        super().__init__(
            n_neighbors=n_neighbors,
            n_components=n_components,
            reg=reg,
            eigen_solver=eigen_solver,
            tol=tol,
            method=method,
            hessian_tol=hessian_tol,
            modified_tol=modified_tol,
            neighbors_algorithm=neighbors_algorithm
        )


from sklearn.manifold._mds import MDS as _MDS

class MDS(_MDS):
    def __init__(
        self,
        n_components: Discrete(min=1, max=4),
        metric: Boolean(),
        n_init: Discrete(min=2, max=8),
        dissimilarity: Categorical('euclidean')
    ):
        self.n_components=n_components
        self.metric=metric
        self.n_init=n_init
        self.dissimilarity=dissimilarity

        super().__init__(
            n_components=n_components,
            metric=metric,
            n_init=n_init,
            dissimilarity=dissimilarity
        )


from sklearn.manifold._spectral_embedding import SpectralEmbedding as _SpectralEmbedding

class SpectralEmbedding(_SpectralEmbedding):
    def __init__(
        self,
        n_components: Discrete(min=1, max=4),
        affinity: Categorical('nearest_neighbors', 'rbf')
    ):
        self.n_components=n_components
        self.affinity=affinity

        super().__init__(
            n_components=n_components,
            affinity=affinity
        )


from sklearn.manifold._t_sne import TSNE as _TSNE

class TSNE(_TSNE):
    def __init__(
        self,
        n_components: Discrete(min=1, max=4),
        perplexity: Continuous(min=15.0, max=60.0),
        early_exaggeration: Continuous(min=6.0, max=24.0),
        learning_rate: Continuous(min=100.0, max=400.0),
        n_iter: Discrete(min=500, max=2000),
        n_iter_without_progress: Discrete(min=150, max=600),
        min_grad_norm: Continuous(min=9.999999999999999e-10, max=1),
        init: Categorical('pca', 'random'),
        angle: Continuous(min=1e-06, max=1)
    ):
        self.n_components=n_components
        self.perplexity=perplexity
        self.early_exaggeration=early_exaggeration
        self.learning_rate=learning_rate
        self.n_iter=n_iter
        self.n_iter_without_progress=n_iter_without_progress
        self.min_grad_norm=min_grad_norm
        self.init=init
        self.angle=angle

        super().__init__(
            n_components=n_components,
            perplexity=perplexity,
            early_exaggeration=early_exaggeration,
            learning_rate=learning_rate,
            n_iter=n_iter,
            n_iter_without_progress=n_iter_without_progress,
            min_grad_norm=min_grad_norm,
            init=init,
            angle=angle
        )


from sklearn.naive_bayes import BernoulliNB as _BernoulliNB

class BernoulliNB(_BernoulliNB):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        binarize: Continuous(min=-1, max=1),
        fit_prior: Boolean()
    ):
        self.alpha=alpha
        self.binarize=binarize
        self.fit_prior=fit_prior

        super().__init__(
            alpha=alpha,
            binarize=binarize,
            fit_prior=fit_prior
        )


from sklearn.naive_bayes import CategoricalNB as _CategoricalNB

class CategoricalNB(_CategoricalNB):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_prior: Boolean()
    ):
        self.alpha=alpha
        self.fit_prior=fit_prior

        super().__init__(
            alpha=alpha,
            fit_prior=fit_prior
        )


from sklearn.naive_bayes import ComplementNB as _ComplementNB

class ComplementNB(_ComplementNB):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_prior: Boolean(),
        norm: Boolean()
    ):
        self.alpha=alpha
        self.fit_prior=fit_prior
        self.norm=norm

        super().__init__(
            alpha=alpha,
            fit_prior=fit_prior,
            norm=norm
        )


from sklearn.naive_bayes import GaussianNB as _GaussianNB

class GaussianNB(_GaussianNB):
    def __init__(
        self,
        var_smoothing: Continuous(min=1.0000000000000001e-11, max=1)
    ):
        self.var_smoothing=var_smoothing

        super().__init__(
            var_smoothing=var_smoothing
        )


from sklearn.naive_bayes import MultinomialNB as _MultinomialNB

class MultinomialNB(_MultinomialNB):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_prior: Boolean()
    ):
        self.alpha=alpha
        self.fit_prior=fit_prior

        super().__init__(
            alpha=alpha,
            fit_prior=fit_prior
        )


from sklearn.neighbors._classification import KNeighborsClassifier as _KNeighborsClassifier

class KNeighborsClassifier(_KNeighborsClassifier):
    def __init__(
        self,
        n_neighbors: Discrete(min=2, max=10),
        weights: Categorical('distance', 'uniform'),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        p: Discrete(min=1, max=4),
        metric: Categorical('minkowski')
    ):
        self.n_neighbors=n_neighbors
        self.weights=weights
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.p=p
        self.metric=metric

        super().__init__(
            n_neighbors=n_neighbors,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )


from sklearn.neighbors._classification import RadiusNeighborsClassifier as _RadiusNeighborsClassifier

class RadiusNeighborsClassifier(_RadiusNeighborsClassifier):
    def __init__(
        self,
        radius: Continuous(min=1e-06, max=1),
        weights: Categorical('distance', 'uniform'),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        p: Discrete(min=1, max=4),
        metric: Categorical('minkowski')
    ):
        self.radius=radius
        self.weights=weights
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.p=p
        self.metric=metric

        super().__init__(
            radius=radius,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )


from sklearn.neighbors._graph import KNeighborsTransformer as _KNeighborsTransformer

class KNeighborsTransformer(_KNeighborsTransformer):
    def __init__(
        self,
        mode: Categorical('connectivity', 'distance'),
        n_neighbors: Discrete(min=2, max=10),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        metric: Categorical('braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'l1', 'l2', 'mahalanobis', 'manhattan', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'),
        p: Discrete(min=1, max=4)
    ):
        self.mode=mode
        self.n_neighbors=n_neighbors
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.metric=metric
        self.p=p

        super().__init__(
            mode=mode,
            n_neighbors=n_neighbors,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p
        )


from sklearn.neighbors._graph import RadiusNeighborsTransformer as _RadiusNeighborsTransformer

class RadiusNeighborsTransformer(_RadiusNeighborsTransformer):
    def __init__(
        self,
        mode: Categorical('connectivity', 'distance'),
        radius: Continuous(min=1e-06, max=1),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        metric: Categorical('braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'l1', 'l2', 'manhattan', 'minkowski', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'),
        p: Discrete(min=1, max=4)
    ):
        self.mode=mode
        self.radius=radius
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.metric=metric
        self.p=p

        super().__init__(
            mode=mode,
            radius=radius,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p
        )


from sklearn.neighbors._kde import KernelDensity as _KernelDensity

class KernelDensity(_KernelDensity):
    def __init__(
        self,
        bandwidth: Continuous(min=1e-06, max=1),
        algorithm: Categorical('auto', 'ball_tree', 'kd_tree'),
        kernel: Categorical('cosine', 'epanechnikov', 'exponential', 'gaussian', 'linear', 'tophat'),
        metric: Categorical('euclidean'),
        atol: Discrete(min=-100, max=100),
        rtol: Discrete(min=-100, max=100),
        breadth_first: Boolean(),
        leaf_size: Discrete(min=20, max=80)
    ):
        self.bandwidth=bandwidth
        self.algorithm=algorithm
        self.kernel=kernel
        self.metric=metric
        self.atol=atol
        self.rtol=rtol
        self.breadth_first=breadth_first
        self.leaf_size=leaf_size

        super().__init__(
            bandwidth=bandwidth,
            algorithm=algorithm,
            kernel=kernel,
            metric=metric,
            atol=atol,
            rtol=rtol,
            breadth_first=breadth_first,
            leaf_size=leaf_size
        )


from sklearn.neighbors._lof import LocalOutlierFactor as _LocalOutlierFactor

class LocalOutlierFactor(_LocalOutlierFactor):
    def __init__(
        self,
        n_neighbors: Discrete(min=10, max=40),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        metric: Categorical('braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'l1', 'l2', 'manhattan', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'),
        p: Discrete(min=1, max=4),
        contamination: Categorical('auto'),
        novelty: Boolean()
    ):
        self.n_neighbors=n_neighbors
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.metric=metric
        self.p=p
        self.contamination=contamination
        self.novelty=novelty

        super().__init__(
            n_neighbors=n_neighbors,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p,
            contamination=contamination,
            novelty=novelty
        )


from sklearn.neighbors._nca import NeighborhoodComponentsAnalysis as _NeighborhoodComponentsAnalysis

class NeighborhoodComponentsAnalysis(_NeighborhoodComponentsAnalysis):
    def __init__(
        self,
        init: Categorical('auto', 'identity', 'pca', 'random'),
        tol: Continuous(min=1.0000000000000001e-07, max=1)
    ):
        self.init=init
        self.tol=tol

        super().__init__(
            init=init,
            tol=tol
        )


from sklearn.neighbors._nearest_centroid import NearestCentroid as _NearestCentroid

class NearestCentroid(_NearestCentroid):
    def __init__(
        self,

    ):


        super().__init__(

        )


from sklearn.neighbors._regression import KNeighborsRegressor as _KNeighborsRegressor

class KNeighborsRegressor(_KNeighborsRegressor):
    def __init__(
        self,
        n_neighbors: Discrete(min=2, max=10),
        weights: Categorical('distance', 'uniform'),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        p: Discrete(min=1, max=4),
        metric: Categorical('minkowski')
    ):
        self.n_neighbors=n_neighbors
        self.weights=weights
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.p=p
        self.metric=metric

        super().__init__(
            n_neighbors=n_neighbors,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )


from sklearn.neighbors._regression import RadiusNeighborsRegressor as _RadiusNeighborsRegressor

class RadiusNeighborsRegressor(_RadiusNeighborsRegressor):
    def __init__(
        self,
        radius: Continuous(min=1e-06, max=1),
        weights: Categorical('distance', 'uniform'),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        p: Discrete(min=1, max=4),
        metric: Categorical('minkowski')
    ):
        self.radius=radius
        self.weights=weights
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.p=p
        self.metric=metric

        super().__init__(
            radius=radius,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )


from sklearn.neighbors._unsupervised import NearestNeighbors as _NearestNeighbors

class NearestNeighbors(_NearestNeighbors):
    def __init__(
        self,
        n_neighbors: Discrete(min=2, max=10),
        radius: Continuous(min=1e-06, max=1),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        metric: Categorical('minkowski'),
        p: Discrete(min=1, max=4)
    ):
        self.n_neighbors=n_neighbors
        self.radius=radius
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.metric=metric
        self.p=p

        super().__init__(
            n_neighbors=n_neighbors,
            radius=radius,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p
        )


from sklearn.preprocessing._data import Binarizer as _Binarizer

class Binarizer(_Binarizer):
    def __init__(
        self,
        threshold: Continuous(min=-1, max=1)
    ):
        self.threshold=threshold

        super().__init__(
            threshold=threshold
        )


from sklearn.preprocessing._data import KernelCenterer as _KernelCenterer

class KernelCenterer(_KernelCenterer):
    def __init__(
        self,

    ):


        super().__init__(

        )


from sklearn.preprocessing._data import MaxAbsScaler as _MaxAbsScaler

class MaxAbsScaler(_MaxAbsScaler):
    def __init__(
        self,

    ):


        super().__init__(

        )


from sklearn.preprocessing._data import MinMaxScaler as _MinMaxScaler

class MinMaxScaler(_MinMaxScaler):
    def __init__(
        self,

    ):


        super().__init__(

        )


from sklearn.preprocessing._data import Normalizer as _Normalizer

class Normalizer(_Normalizer):
    def __init__(
        self,
        norm: Categorical('l1', 'l2', 'max')
    ):
        self.norm=norm

        super().__init__(
            norm=norm
        )


from sklearn.preprocessing._data import PolynomialFeatures as _PolynomialFeatures

class PolynomialFeatures(_PolynomialFeatures):
    def __init__(
        self,
        degree: Discrete(min=1, max=4),
        interaction_only: Boolean(),
        include_bias: Boolean(),
        order: Categorical('c', 'f')
    ):
        self.degree=degree
        self.interaction_only=interaction_only
        self.include_bias=include_bias
        self.order=order

        super().__init__(
            degree=degree,
            interaction_only=interaction_only,
            include_bias=include_bias,
            order=order
        )


from sklearn.preprocessing._data import PowerTransformer as _PowerTransformer

class PowerTransformer(_PowerTransformer):
    def __init__(
        self,
        standardize: Boolean()
    ):
        self.standardize=standardize

        super().__init__(
            standardize=standardize
        )


from sklearn.preprocessing._data import QuantileTransformer as _QuantileTransformer

class QuantileTransformer(_QuantileTransformer):
    def __init__(
        self,
        n_quantiles: Discrete(min=500, max=2000),
        output_distribution: Categorical('normal', 'uniform'),
        ignore_implicit_zeros: Boolean(),
        subsample: Discrete(min=50000, max=200000)
    ):
        self.n_quantiles=n_quantiles
        self.output_distribution=output_distribution
        self.ignore_implicit_zeros=ignore_implicit_zeros
        self.subsample=subsample

        super().__init__(
            n_quantiles=n_quantiles,
            output_distribution=output_distribution,
            ignore_implicit_zeros=ignore_implicit_zeros,
            subsample=subsample
        )


from sklearn.preprocessing._data import RobustScaler as _RobustScaler

class RobustScaler(_RobustScaler):
    def __init__(
        self,
        with_centering: Boolean(),
        with_scaling: Boolean()
    ):
        self.with_centering=with_centering
        self.with_scaling=with_scaling

        super().__init__(
            with_centering=with_centering,
            with_scaling=with_scaling
        )


from sklearn.preprocessing._data import StandardScaler as _StandardScaler

class StandardScaler(_StandardScaler):
    def __init__(
        self,
        with_mean: Boolean(),
        with_std: Boolean()
    ):
        self.with_mean=with_mean
        self.with_std=with_std

        super().__init__(
            with_mean=with_mean,
            with_std=with_std
        )


from sklearn.preprocessing._discretization import KBinsDiscretizer as _KBinsDiscretizer

class KBinsDiscretizer(_KBinsDiscretizer):
    def __init__(
        self,
        n_bins: Discrete(min=2, max=10),
        encode: Categorical('onehot', 'ordinal'),
        strategy: Categorical('kmeans', 'quantile', 'uniform')
    ):
        self.n_bins=n_bins
        self.encode=encode
        self.strategy=strategy

        super().__init__(
            n_bins=n_bins,
            encode=encode,
            strategy=strategy
        )


from sklearn.preprocessing._encoders import OneHotEncoder as _OneHotEncoder

class OneHotEncoder(_OneHotEncoder):
    def __init__(
        self,
        categories: Categorical('auto'),
        sparse: Boolean(),
        handle_unknown: Categorical('error', 'ignore')
    ):
        self.categories=categories
        self.sparse=sparse
        self.handle_unknown=handle_unknown

        super().__init__(
            categories=categories,
            sparse=sparse,
            handle_unknown=handle_unknown
        )


from sklearn.preprocessing._encoders import OrdinalEncoder as _OrdinalEncoder

class OrdinalEncoder(_OrdinalEncoder):
    def __init__(
        self,
        categories: Categorical('auto')
    ):
        self.categories=categories

        super().__init__(
            categories=categories
        )


from sklearn.preprocessing._function_transformer import FunctionTransformer as _FunctionTransformer

class FunctionTransformer(_FunctionTransformer):
    def __init__(
        self,
        validate: Boolean(),
        accept_sparse: Boolean(),
        check_inverse: Boolean()
    ):
        self.validate=validate
        self.accept_sparse=accept_sparse
        self.check_inverse=check_inverse

        super().__init__(
            validate=validate,
            accept_sparse=accept_sparse,
            check_inverse=check_inverse
        )


from sklearn.preprocessing._label import LabelBinarizer as _LabelBinarizer

class LabelBinarizer(_LabelBinarizer):
    def __init__(
        self,
        neg_label: Discrete(min=-100, max=100),
        pos_label: Discrete(min=0, max=2),
        sparse_output: Boolean()
    ):
        self.neg_label=neg_label
        self.pos_label=pos_label
        self.sparse_output=sparse_output

        super().__init__(
            neg_label=neg_label,
            pos_label=pos_label,
            sparse_output=sparse_output
        )


from sklearn.preprocessing._label import LabelEncoder as _LabelEncoder

class LabelEncoder(_LabelEncoder):
    def __init__(
        self,

    ):


        super().__init__(

        )


from sklearn.preprocessing._label import MultiLabelBinarizer as _MultiLabelBinarizer

class MultiLabelBinarizer(_MultiLabelBinarizer):
    def __init__(
        self,
        sparse_output: Boolean()
    ):
        self.sparse_output=sparse_output

        super().__init__(
            sparse_output=sparse_output
        )


from sklearn.svm._classes import LinearSVC as _LinearSVC

class LinearSVC(_LinearSVC):
    def __init__(
        self,
        penalty: Categorical('l2'),
        loss: Categorical('hinge', 'squared_hinge'),
        dual: Boolean(),
        tol: Continuous(min=1e-06, max=1),
        C: Continuous(min=1e-06, max=1),
        multi_class: Categorical('crammer_singer', 'ovr'),
        fit_intercept: Boolean(),
        intercept_scaling: Discrete(min=0, max=2)
    ):
        self.penalty=penalty
        self.loss=loss
        self.dual=dual
        self.tol=tol
        self.C=C
        self.multi_class=multi_class
        self.fit_intercept=fit_intercept
        self.intercept_scaling=intercept_scaling

        super().__init__(
            penalty=penalty,
            loss=loss,
            dual=dual,
            tol=tol,
            C=C,
            multi_class=multi_class,
            fit_intercept=fit_intercept,
            intercept_scaling=intercept_scaling
        )


from sklearn.svm._classes import LinearSVR as _LinearSVR

class LinearSVR(_LinearSVR):
    def __init__(
        self,
        epsilon: Continuous(min=-1, max=1),
        tol: Continuous(min=1e-06, max=1),
        C: Continuous(min=1e-06, max=1),
        loss: Categorical('epsilon_insensitive', 'squared_epsilon_insensitive'),
        fit_intercept: Boolean(),
        intercept_scaling: Continuous(min=1e-06, max=1),
        dual: Boolean()
    ):
        self.epsilon=epsilon
        self.tol=tol
        self.C=C
        self.loss=loss
        self.fit_intercept=fit_intercept
        self.intercept_scaling=intercept_scaling
        self.dual=dual

        super().__init__(
            epsilon=epsilon,
            tol=tol,
            C=C,
            loss=loss,
            fit_intercept=fit_intercept,
            intercept_scaling=intercept_scaling,
            dual=dual
        )


from sklearn.svm._classes import NuSVC as _NuSVC

class NuSVC(_NuSVC):
    def __init__(
        self,
        nu: Continuous(min=1e-06, max=1),
        kernel: Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree: Discrete(min=1, max=6),
        gamma: Categorical('auto', 'scale'),
        coef0: Continuous(min=-1, max=1),
        shrinking: Boolean(),
        probability: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        cache_size: Discrete(min=100, max=400),
        decision_function_shape: Categorical('ovo', 'ovr'),
        break_ties: Boolean()
    ):
        self.nu=nu
        self.kernel=kernel
        self.degree=degree
        self.gamma=gamma
        self.coef0=coef0
        self.shrinking=shrinking
        self.probability=probability
        self.tol=tol
        self.cache_size=cache_size
        self.decision_function_shape=decision_function_shape
        self.break_ties=break_ties

        super().__init__(
            nu=nu,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            shrinking=shrinking,
            probability=probability,
            tol=tol,
            cache_size=cache_size,
            decision_function_shape=decision_function_shape,
            break_ties=break_ties
        )


from sklearn.svm._classes import NuSVR as _NuSVR

class NuSVR(_NuSVR):
    def __init__(
        self,
        nu: Continuous(min=1e-06, max=1),
        C: Continuous(min=1e-06, max=1),
        kernel: Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree: Discrete(min=1, max=6),
        gamma: Categorical('auto', 'scale'),
        coef0: Continuous(min=-1, max=1),
        shrinking: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        cache_size: Discrete(min=100, max=400)
    ):
        self.nu=nu
        self.C=C
        self.kernel=kernel
        self.degree=degree
        self.gamma=gamma
        self.coef0=coef0
        self.shrinking=shrinking
        self.tol=tol
        self.cache_size=cache_size

        super().__init__(
            nu=nu,
            C=C,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            shrinking=shrinking,
            tol=tol,
            cache_size=cache_size
        )


from sklearn.svm._classes import OneClassSVM as _OneClassSVM

class OneClassSVM(_OneClassSVM):
    def __init__(
        self,
        kernel: Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree: Discrete(min=1, max=6),
        gamma: Categorical('auto', 'scale'),
        coef0: Continuous(min=-1, max=1),
        tol: Continuous(min=1e-05, max=1),
        nu: Continuous(min=1e-06, max=1),
        shrinking: Boolean(),
        cache_size: Discrete(min=100, max=400)
    ):
        self.kernel=kernel
        self.degree=degree
        self.gamma=gamma
        self.coef0=coef0
        self.tol=tol
        self.nu=nu
        self.shrinking=shrinking
        self.cache_size=cache_size

        super().__init__(
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            tol=tol,
            nu=nu,
            shrinking=shrinking,
            cache_size=cache_size
        )


from sklearn.svm._classes import SVC as _SVC

class SVC(_SVC):
    def __init__(
        self,
        C: Continuous(min=1e-06, max=1),
        degree: Discrete(min=1, max=6),
        gamma: Categorical('auto', 'scale'),
        coef0: Continuous(min=-1, max=1),
        shrinking: Boolean(),
        probability: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        cache_size: Discrete(min=100, max=400),
        decision_function_shape: Categorical('ovo', 'ovr'),
        break_ties: Boolean()
    ):
        self.C=C
        self.degree=degree
        self.gamma=gamma
        self.coef0=coef0
        self.shrinking=shrinking
        self.probability=probability
        self.tol=tol
        self.cache_size=cache_size
        self.decision_function_shape=decision_function_shape
        self.break_ties=break_ties

        super().__init__(
            C=C,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            shrinking=shrinking,
            probability=probability,
            tol=tol,
            cache_size=cache_size,
            decision_function_shape=decision_function_shape,
            break_ties=break_ties
        )


from sklearn.svm._classes import SVR as _SVR

class SVR(_SVR):
    def __init__(
        self,
        kernel: Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree: Discrete(min=1, max=6),
        gamma: Categorical('auto', 'scale'),
        coef0: Continuous(min=-1, max=1),
        tol: Continuous(min=1e-05, max=1),
        C: Continuous(min=1e-06, max=1),
        epsilon: Continuous(min=0.001, max=1),
        shrinking: Boolean(),
        cache_size: Discrete(min=100, max=400)
    ):
        self.kernel=kernel
        self.degree=degree
        self.gamma=gamma
        self.coef0=coef0
        self.tol=tol
        self.C=C
        self.epsilon=epsilon
        self.shrinking=shrinking
        self.cache_size=cache_size

        super().__init__(
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            tol=tol,
            C=C,
            epsilon=epsilon,
            shrinking=shrinking,
            cache_size=cache_size
        )


from sklearn.tree._classes import BaseDecisionTree as _BaseDecisionTree

class BaseDecisionTree(_BaseDecisionTree):
    def __init__(
        self,
        ccp_alpha: Continuous(min=-1, max=1)
    ):
        self.ccp_alpha=ccp_alpha

        super().__init__(
            ccp_alpha=ccp_alpha
        )


from sklearn.tree._classes import DecisionTreeClassifier as _DecisionTreeClassifier

class DecisionTreeClassifier(_DecisionTreeClassifier):
    def __init__(
        self,
        min_samples_split: Discrete(min=1, max=4),
        min_samples_leaf: Discrete(min=0, max=2),
        min_weight_fraction_leaf: Continuous(min=-1, max=1),
        min_impurity_decrease: Continuous(min=-1, max=1),
        ccp_alpha: Continuous(min=-1, max=1)
    ):
        self.min_samples_split=min_samples_split
        self.min_samples_leaf=min_samples_leaf
        self.min_weight_fraction_leaf=min_weight_fraction_leaf
        self.min_impurity_decrease=min_impurity_decrease
        self.ccp_alpha=ccp_alpha

        super().__init__(
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )


from sklearn.tree._classes import DecisionTreeRegressor as _DecisionTreeRegressor

class DecisionTreeRegressor(_DecisionTreeRegressor):
    def __init__(
        self,
        min_samples_split: Discrete(min=1, max=4),
        min_samples_leaf: Discrete(min=0, max=2),
        min_weight_fraction_leaf: Continuous(min=-1, max=1),
        min_impurity_decrease: Continuous(min=-1, max=1),
        ccp_alpha: Continuous(min=-1, max=1)
    ):
        self.min_samples_split=min_samples_split
        self.min_samples_leaf=min_samples_leaf
        self.min_weight_fraction_leaf=min_weight_fraction_leaf
        self.min_impurity_decrease=min_impurity_decrease
        self.ccp_alpha=ccp_alpha

        super().__init__(
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )


from sklearn.tree._classes import ExtraTreeClassifier as _ExtraTreeClassifier

class ExtraTreeClassifier(_ExtraTreeClassifier):
    def __init__(
        self,
        min_samples_split: Discrete(min=1, max=4),
        min_samples_leaf: Discrete(min=0, max=2),
        min_weight_fraction_leaf: Continuous(min=-1, max=1),
        min_impurity_decrease: Continuous(min=-1, max=1),
        ccp_alpha: Continuous(min=-1, max=1)
    ):
        self.min_samples_split=min_samples_split
        self.min_samples_leaf=min_samples_leaf
        self.min_weight_fraction_leaf=min_weight_fraction_leaf
        self.min_impurity_decrease=min_impurity_decrease
        self.ccp_alpha=ccp_alpha

        super().__init__(
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )


from sklearn.tree._classes import ExtraTreeRegressor as _ExtraTreeRegressor

class ExtraTreeRegressor(_ExtraTreeRegressor):
    def __init__(
        self,
        min_samples_split: Discrete(min=1, max=4),
        min_samples_leaf: Discrete(min=0, max=2),
        min_weight_fraction_leaf: Continuous(min=-1, max=1),
        min_impurity_decrease: Continuous(min=-1, max=1),
        ccp_alpha: Continuous(min=-1, max=1)
    ):
        self.min_samples_split=min_samples_split
        self.min_samples_leaf=min_samples_leaf
        self.min_weight_fraction_leaf=min_weight_fraction_leaf
        self.min_impurity_decrease=min_impurity_decrease
        self.ccp_alpha=ccp_alpha

        super().__init__(
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )

