
# AUTOGENERATED ON 2020-01-10 14:12:37.238310
## DO NOT MODIFY THIS FILE MANUALLY


from sklearn.cluster._affinity_propagation import AffinityPropagation

class AffinityPropagation(AffinityPropagation):
    def __init__(
        self,
        damping:Discrete(min=1e-06, max=1),
        convergence_iter:Discrete(min=7, max=30)
    ):
        self.damping=damping
        self.convergence_iter=convergence_iter

        super().__init__(
            self,
            damping=damping,
            convergence_iter=convergence_iter
        )


from sklearn.cluster._bicluster import SpectralBiclustering

class SpectralBiclustering(SpectralBiclustering):
    def __init__(
        self,
        n_clusters:Discrete(min=1, max=6),
        method:Categorical('bistochastic', 'log', 'scale'),
        n_components:Discrete(min=3, max=12),
        n_best:Discrete(min=1, max=6),
        svd_method:Categorical('arpack', 'randomized'),
        mini_batch:Boolean(),
        init:Categorical('random'),
        n_init:Discrete(min=5, max=20)
    ):
        self.n_clusters=n_clusters
        self.method=method
        self.n_components=n_components
        self.n_best=n_best
        self.svd_method=svd_method
        self.mini_batch=mini_batch
        self.init=init
        self.n_init=n_init

        super().__init__(
            self,
            n_clusters=n_clusters,
            method=method,
            n_components=n_components,
            n_best=n_best,
            svd_method=svd_method,
            mini_batch=mini_batch,
            init=init,
            n_init=n_init
        )


from sklearn.cluster._bicluster import SpectralCoclustering

class SpectralCoclustering(SpectralCoclustering):
    def __init__(
        self,
        n_clusters:Discrete(min=1, max=6),
        svd_method:Categorical('arpack', 'randomized'),
        mini_batch:Boolean(),
        init:Categorical('random'),
        n_init:Discrete(min=5, max=20)
    ):
        self.n_clusters=n_clusters
        self.svd_method=svd_method
        self.mini_batch=mini_batch
        self.init=init
        self.n_init=n_init

        super().__init__(
            self,
            n_clusters=n_clusters,
            svd_method=svd_method,
            mini_batch=mini_batch,
            init=init,
            n_init=n_init
        )


from sklearn.cluster._birch import Birch

class Birch(Birch):
    def __init__(
        self,
        threshold:Discrete(min=1e-06, max=1),
        branching_factor:Discrete(min=25, max=100),
        n_clusters:Discrete(min=1, max=6),
        compute_labels:Boolean()
    ):
        self.threshold=threshold
        self.branching_factor=branching_factor
        self.n_clusters=n_clusters
        self.compute_labels=compute_labels

        super().__init__(
            self,
            threshold=threshold,
            branching_factor=branching_factor,
            n_clusters=n_clusters,
            compute_labels=compute_labels
        )


from sklearn.cluster._dbscan import DBSCAN

class DBSCAN(DBSCAN):
    def __init__(
        self,
        min_samples:Discrete(min=2, max=10),
        algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size:Discrete(min=15, max=60)
    ):
        self.min_samples=min_samples
        self.algorithm=algorithm
        self.leaf_size=leaf_size

        super().__init__(
            self,
            min_samples=min_samples,
            algorithm=algorithm,
            leaf_size=leaf_size
        )


from sklearn.cluster._hierarchical import AgglomerativeClustering

class AgglomerativeClustering(AgglomerativeClustering):
    def __init__(
        self,
        n_clusters:Discrete(min=1, max=4),
        compute_full_tree:Categorical('auto')
    ):
        self.n_clusters=n_clusters
        self.compute_full_tree=compute_full_tree

        super().__init__(
            self,
            n_clusters=n_clusters,
            compute_full_tree=compute_full_tree
        )


from sklearn.cluster._hierarchical import FeatureAgglomeration

class FeatureAgglomeration(FeatureAgglomeration):
    def __init__(
        self,
        n_clusters:Discrete(min=1, max=4),
        compute_full_tree:Categorical('auto')
    ):
        self.n_clusters=n_clusters
        self.compute_full_tree=compute_full_tree

        super().__init__(
            self,
            n_clusters=n_clusters,
            compute_full_tree=compute_full_tree
        )


from sklearn.cluster._k_means import KMeans

class KMeans(KMeans):
    def __init__(
        self,
        n_clusters:Discrete(min=4, max=16),
        init:Categorical('random'),
        n_init:Discrete(min=5, max=20),
        tol:Discrete(min=1e-06, max=1),
        precompute_distances:Categorical('auto')
    ):
        self.n_clusters=n_clusters
        self.init=init
        self.n_init=n_init
        self.tol=tol
        self.precompute_distances=precompute_distances

        super().__init__(
            self,
            n_clusters=n_clusters,
            init=init,
            n_init=n_init,
            tol=tol,
            precompute_distances=precompute_distances
        )


from sklearn.cluster._k_means import MiniBatchKMeans

class MiniBatchKMeans(MiniBatchKMeans):
    def __init__(
        self,
        n_clusters:Discrete(min=4, max=16),
        init:Categorical('random'),
        batch_size:Discrete(min=50, max=200),
        compute_labels:Boolean(),
        tol:Discrete(min=-1, max=1),
        max_no_improvement:Discrete(min=5, max=20),
        n_init:Discrete(min=1, max=6),
        reassignment_ratio:Discrete(min=0.0001, max=1)
    ):
        self.n_clusters=n_clusters
        self.init=init
        self.batch_size=batch_size
        self.compute_labels=compute_labels
        self.tol=tol
        self.max_no_improvement=max_no_improvement
        self.n_init=n_init
        self.reassignment_ratio=reassignment_ratio

        super().__init__(
            self,
            n_clusters=n_clusters,
            init=init,
            batch_size=batch_size,
            compute_labels=compute_labels,
            tol=tol,
            max_no_improvement=max_no_improvement,
            n_init=n_init,
            reassignment_ratio=reassignment_ratio
        )


from sklearn.cluster._mean_shift import MeanShift

class MeanShift(MeanShift):
    def __init__(
        self,
        bin_seeding:Boolean(),
        min_bin_freq:Discrete(min=0, max=2),
        cluster_all:Boolean()
    ):
        self.bin_seeding=bin_seeding
        self.min_bin_freq=min_bin_freq
        self.cluster_all=cluster_all

        super().__init__(
            self,
            bin_seeding=bin_seeding,
            min_bin_freq=min_bin_freq,
            cluster_all=cluster_all
        )


from sklearn.cluster._optics import OPTICS

class OPTICS(OPTICS):
    def __init__(
        self,
        min_samples:Discrete(min=2, max=10),
        max_eps:Discrete(min=inf, max=inf),
        metric:Categorical('braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'l1', 'l2', 'manhattan', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'),
        p:Discrete(min=1, max=4),
        cluster_method:Categorical('xi'),
        xi:Discrete(min=0.0005, max=1),
        predecessor_correction:Boolean(),
        algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size:Discrete(min=15, max=60)
    ):
        self.min_samples=min_samples
        self.max_eps=max_eps
        self.metric=metric
        self.p=p
        self.cluster_method=cluster_method
        self.xi=xi
        self.predecessor_correction=predecessor_correction
        self.algorithm=algorithm
        self.leaf_size=leaf_size

        super().__init__(
            self,
            min_samples=min_samples,
            max_eps=max_eps,
            metric=metric,
            p=p,
            cluster_method=cluster_method,
            xi=xi,
            predecessor_correction=predecessor_correction,
            algorithm=algorithm,
            leaf_size=leaf_size
        )


from sklearn.cluster._spectral import SpectralClustering

class SpectralClustering(SpectralClustering):
    def __init__(
        self,
        n_clusters:Discrete(min=4, max=16),
        n_init:Discrete(min=5, max=20),
        gamma:Discrete(min=1e-06, max=1),
        affinity:Categorical('nearest_neighbors', 'precomputed_nearest_neighbors', 'rbf'),
        n_neighbors:Discrete(min=5, max=20),
        eigen_tol:Discrete(min=-1, max=1),
        assign_labels:Categorical('discretize', 'kmeans'),
        degree:Discrete(min=1, max=6),
        coef0:Discrete(min=0, max=2)
    ):
        self.n_clusters=n_clusters
        self.n_init=n_init
        self.gamma=gamma
        self.affinity=affinity
        self.n_neighbors=n_neighbors
        self.eigen_tol=eigen_tol
        self.assign_labels=assign_labels
        self.degree=degree
        self.coef0=coef0

        super().__init__(
            self,
            n_clusters=n_clusters,
            n_init=n_init,
            gamma=gamma,
            affinity=affinity,
            n_neighbors=n_neighbors,
            eigen_tol=eigen_tol,
            assign_labels=assign_labels,
            degree=degree,
            coef0=coef0
        )


from sklearn.cross_decomposition._cca import CCA

class CCA(CCA):
    def __init__(
        self,
        n_components:Discrete(min=1, max=4),
        scale:Boolean(),
        tol:Discrete(min=1e-08, max=1)
    ):
        self.n_components=n_components
        self.scale=scale
        self.tol=tol

        super().__init__(
            self,
            n_components=n_components,
            scale=scale,
            tol=tol
        )


from sklearn.cross_decomposition._pls import PLSCanonical

class PLSCanonical(PLSCanonical):
    def __init__(
        self,
        n_components:Discrete(min=1, max=4),
        scale:Boolean(),
        tol:Discrete(min=1e-08, max=1)
    ):
        self.n_components=n_components
        self.scale=scale
        self.tol=tol

        super().__init__(
            self,
            n_components=n_components,
            scale=scale,
            tol=tol
        )


from sklearn.cross_decomposition._pls import PLSRegression

class PLSRegression(PLSRegression):
    def __init__(
        self,
        n_components:Discrete(min=1, max=4),
        scale:Boolean(),
        tol:Discrete(min=1e-08, max=1)
    ):
        self.n_components=n_components
        self.scale=scale
        self.tol=tol

        super().__init__(
            self,
            n_components=n_components,
            scale=scale,
            tol=tol
        )


from sklearn.cross_decomposition._pls import PLSSVD

class PLSSVD(PLSSVD):
    def __init__(
        self,
        n_components:Discrete(min=1, max=4),
        scale:Boolean()
    ):
        self.n_components=n_components
        self.scale=scale

        super().__init__(
            self,
            n_components=n_components,
            scale=scale
        )


from sklearn.decomposition._dict_learning import DictionaryLearning

class DictionaryLearning(DictionaryLearning):
    def __init__(
        self,
        alpha:Discrete(min=0, max=2),
        tol:Discrete(min=1e-10, max=1),
        fit_algorithm:Categorical('cd', 'lars'),
        transform_algorithm:Categorical('lars', 'lasso_cd', 'lasso_lars', 'omp', 'threshold'),
        split_sign:Boolean(),
        positive_code:Boolean(),
        positive_dict:Boolean(),
        transform_max_iter:Discrete(min=500, max=2000)
    ):
        self.alpha=alpha
        self.tol=tol
        self.fit_algorithm=fit_algorithm
        self.transform_algorithm=transform_algorithm
        self.split_sign=split_sign
        self.positive_code=positive_code
        self.positive_dict=positive_dict
        self.transform_max_iter=transform_max_iter

        super().__init__(
            self,
            alpha=alpha,
            tol=tol,
            fit_algorithm=fit_algorithm,
            transform_algorithm=transform_algorithm,
            split_sign=split_sign,
            positive_code=positive_code,
            positive_dict=positive_dict,
            transform_max_iter=transform_max_iter
        )


from sklearn.decomposition._dict_learning import MiniBatchDictionaryLearning

class MiniBatchDictionaryLearning(MiniBatchDictionaryLearning):
    def __init__(
        self,
        alpha:Discrete(min=0, max=2),
        n_iter:Discrete(min=500, max=2000),
        fit_algorithm:Categorical('cd', 'lars'),
        batch_size:Discrete(min=1, max=6),
        shuffle:Boolean(),
        transform_algorithm:Categorical('lars', 'lasso_cd', 'lasso_lars', 'omp', 'threshold'),
        split_sign:Boolean(),
        positive_code:Boolean(),
        positive_dict:Boolean(),
        transform_max_iter:Discrete(min=500, max=2000)
    ):
        self.alpha=alpha
        self.n_iter=n_iter
        self.fit_algorithm=fit_algorithm
        self.batch_size=batch_size
        self.shuffle=shuffle
        self.transform_algorithm=transform_algorithm
        self.split_sign=split_sign
        self.positive_code=positive_code
        self.positive_dict=positive_dict
        self.transform_max_iter=transform_max_iter

        super().__init__(
            self,
            alpha=alpha,
            n_iter=n_iter,
            fit_algorithm=fit_algorithm,
            batch_size=batch_size,
            shuffle=shuffle,
            transform_algorithm=transform_algorithm,
            split_sign=split_sign,
            positive_code=positive_code,
            positive_dict=positive_dict,
            transform_max_iter=transform_max_iter
        )


from sklearn.decomposition._dict_learning import SparseCoder

class SparseCoder(SparseCoder):
    def __init__(
        self,
        split_sign:Boolean(),
        positive_code:Boolean(),
        transform_max_iter:Discrete(min=500, max=2000)
    ):
        self.split_sign=split_sign
        self.positive_code=positive_code
        self.transform_max_iter=transform_max_iter

        super().__init__(
            self,
            split_sign=split_sign,
            positive_code=positive_code,
            transform_max_iter=transform_max_iter
        )


from sklearn.decomposition._factor_analysis import FactorAnalysis

class FactorAnalysis(FactorAnalysis):
    def __init__(
        self,
        tol:Discrete(min=0.0001, max=1),
        svd_method:Categorical('lapack', 'randomized'),
        iterated_power:Discrete(min=1, max=6)
    ):
        self.tol=tol
        self.svd_method=svd_method
        self.iterated_power=iterated_power

        super().__init__(
            self,
            tol=tol,
            svd_method=svd_method,
            iterated_power=iterated_power
        )


from sklearn.decomposition._fastica import FastICA

class FastICA(FastICA):
    def __init__(
        self,
        whiten:Boolean(),
        tol:Discrete(min=1e-06, max=1)
    ):
        self.whiten=whiten
        self.tol=tol

        super().__init__(
            self,
            whiten=whiten,
            tol=tol
        )


from sklearn.decomposition._incremental_pca import IncrementalPCA

class IncrementalPCA(IncrementalPCA):
    def __init__(
        self,
        whiten:Boolean()
    ):
        self.whiten=whiten

        super().__init__(
            self,
            whiten=whiten
        )


from sklearn.decomposition._kernel_pca import KernelPCA

class KernelPCA(KernelPCA):
    def __init__(
        self,
        degree:Discrete(min=1, max=6),
        coef0:Discrete(min=0, max=2),
        alpha:Discrete(min=1e-06, max=1),
        fit_inverse_transform:Boolean(),
        eigen_solver:Categorical('arpack', 'auto', 'dense'),
        tol:Discrete(min=-100, max=100),
        remove_zero_eig:Boolean()
    ):
        self.degree=degree
        self.coef0=coef0
        self.alpha=alpha
        self.fit_inverse_transform=fit_inverse_transform
        self.eigen_solver=eigen_solver
        self.tol=tol
        self.remove_zero_eig=remove_zero_eig

        super().__init__(
            self,
            degree=degree,
            coef0=coef0,
            alpha=alpha,
            fit_inverse_transform=fit_inverse_transform,
            eigen_solver=eigen_solver,
            tol=tol,
            remove_zero_eig=remove_zero_eig
        )


from sklearn.decomposition._nmf import NMF

class NMF(NMF):
    def __init__(
        self,
        solver:Categorical('cd', 'mu'),
        beta_loss:Categorical('frobenius'),
        tol:Discrete(min=1e-06, max=1),
        alpha:Discrete(min=-1, max=1),
        l1_ratio:Discrete(min=-1, max=1),
        shuffle:Boolean()
    ):
        self.solver=solver
        self.beta_loss=beta_loss
        self.tol=tol
        self.alpha=alpha
        self.l1_ratio=l1_ratio
        self.shuffle=shuffle

        super().__init__(
            self,
            solver=solver,
            beta_loss=beta_loss,
            tol=tol,
            alpha=alpha,
            l1_ratio=l1_ratio,
            shuffle=shuffle
        )


from sklearn.decomposition._online_lda import LatentDirichletAllocation

class LatentDirichletAllocation(LatentDirichletAllocation):
    def __init__(
        self,
        n_components:Discrete(min=5, max=20),
        learning_method:Categorical('batch', 'online'),
        learning_decay:Discrete(min=1e-06, max=1),
        learning_offset:Discrete(min=5.0, max=20.0),
        batch_size:Discrete(min=64, max=256),
        evaluate_every:Discrete(min=-1, max=-2),
        total_samples:Discrete(min=500000.0, max=2000000.0),
        perp_tol:Discrete(min=0.001, max=1),
        mean_change_tol:Discrete(min=1e-05, max=1),
        max_doc_update_iter:Discrete(min=50, max=200)
    ):
        self.n_components=n_components
        self.learning_method=learning_method
        self.learning_decay=learning_decay
        self.learning_offset=learning_offset
        self.batch_size=batch_size
        self.evaluate_every=evaluate_every
        self.total_samples=total_samples
        self.perp_tol=perp_tol
        self.mean_change_tol=mean_change_tol
        self.max_doc_update_iter=max_doc_update_iter

        super().__init__(
            self,
            n_components=n_components,
            learning_method=learning_method,
            learning_decay=learning_decay,
            learning_offset=learning_offset,
            batch_size=batch_size,
            evaluate_every=evaluate_every,
            total_samples=total_samples,
            perp_tol=perp_tol,
            mean_change_tol=mean_change_tol,
            max_doc_update_iter=max_doc_update_iter
        )


from sklearn.decomposition._pca import PCA

class PCA(PCA):
    def __init__(
        self,
        whiten:Boolean(),
        svd_solver:Categorical('arpack', 'auto', 'full', 'randomized'),
        tol:Discrete(min=-1, max=1),
        iterated_power:Categorical('auto', 'randomized')
    ):
        self.whiten=whiten
        self.svd_solver=svd_solver
        self.tol=tol
        self.iterated_power=iterated_power

        super().__init__(
            self,
            whiten=whiten,
            svd_solver=svd_solver,
            tol=tol,
            iterated_power=iterated_power
        )


from sklearn.decomposition._sparse_pca import MiniBatchSparsePCA

class MiniBatchSparsePCA(MiniBatchSparsePCA):
    def __init__(
        self,
        alpha:Discrete(min=0, max=2),
        ridge_alpha:Discrete(min=0.0001, max=1),
        n_iter:Discrete(min=50, max=200),
        batch_size:Discrete(min=1, max=6),
        shuffle:Boolean(),
        method:Categorical('cd', 'lars')
    ):
        self.alpha=alpha
        self.ridge_alpha=ridge_alpha
        self.n_iter=n_iter
        self.batch_size=batch_size
        self.shuffle=shuffle
        self.method=method

        super().__init__(
            self,
            alpha=alpha,
            ridge_alpha=ridge_alpha,
            n_iter=n_iter,
            batch_size=batch_size,
            shuffle=shuffle,
            method=method
        )


from sklearn.decomposition._sparse_pca import SparsePCA

class SparsePCA(SparsePCA):
    def __init__(
        self,
        alpha:Discrete(min=0, max=2),
        ridge_alpha:Discrete(min=0.0001, max=1),
        tol:Discrete(min=1e-10, max=1),
        method:Categorical('cd', 'lars')
    ):
        self.alpha=alpha
        self.ridge_alpha=ridge_alpha
        self.tol=tol
        self.method=method

        super().__init__(
            self,
            alpha=alpha,
            ridge_alpha=ridge_alpha,
            tol=tol,
            method=method
        )


from sklearn.decomposition._truncated_svd import TruncatedSVD

class TruncatedSVD(TruncatedSVD):
    def __init__(
        self,
        n_components:Discrete(min=1, max=4),
        n_iter:Discrete(min=2, max=10),
        tol:Discrete(min=-1, max=1)
    ):
        self.n_components=n_components
        self.n_iter=n_iter
        self.tol=tol

        super().__init__(
            self,
            n_components=n_components,
            n_iter=n_iter,
            tol=tol
        )


from sklearn.feature_extraction._dict_vectorizer import DictVectorizer

class DictVectorizer(DictVectorizer):
    def __init__(
        self,
        sparse:Boolean(),
        sort:Boolean()
    ):
        self.sparse=sparse
        self.sort=sort

        super().__init__(
            self,
            sparse=sparse,
            sort=sort
        )


from sklearn.feature_extraction._hashing import FeatureHasher

class FeatureHasher(FeatureHasher):
    def __init__(
        self,
        n_features:Discrete(min=524288, max=2097152),
        alternate_sign:Boolean()
    ):
        self.n_features=n_features
        self.alternate_sign=alternate_sign

        super().__init__(
            self,
            n_features=n_features,
            alternate_sign=alternate_sign
        )


from sklearn.feature_extraction.image import PatchExtractor

class PatchExtractor(PatchExtractor):
    def __init__(
        self,

    ):


        super().__init__(
            self,

        )


from sklearn.feature_extraction.text import CountVectorizer

class CountVectorizer(CountVectorizer):
    def __init__(
        self,
        lowercase:Boolean(),
        max_df:Discrete(min=1e-06, max=1),
        min_df:Discrete(min=0, max=2),
        binary:Boolean()
    ):
        self.lowercase=lowercase
        self.max_df=max_df
        self.min_df=min_df
        self.binary=binary

        super().__init__(
            self,
            lowercase=lowercase,
            max_df=max_df,
            min_df=min_df,
            binary=binary
        )


from sklearn.feature_extraction.text import HashingVectorizer

class HashingVectorizer(HashingVectorizer):
    def __init__(
        self,
        input:Categorical('content', 'file', 'filename', 'read'),
        decode_error:Categorical('ignore', 'replace', 'strict'),
        lowercase:Boolean(),
        token_pattern:Categorical('word'),
        analyzer:Categorical('char', 'char_wb', 'word'),
        n_features:Discrete(min=524288, max=2097152),
        binary:Boolean(),
        norm:Categorical('l1'),
        alternate_sign:Boolean()
    ):
        self.input=input
        self.decode_error=decode_error
        self.lowercase=lowercase
        self.token_pattern=token_pattern
        self.analyzer=analyzer
        self.n_features=n_features
        self.binary=binary
        self.norm=norm
        self.alternate_sign=alternate_sign

        super().__init__(
            self,
            input=input,
            decode_error=decode_error,
            lowercase=lowercase,
            token_pattern=token_pattern,
            analyzer=analyzer,
            n_features=n_features,
            binary=binary,
            norm=norm,
            alternate_sign=alternate_sign
        )


from sklearn.feature_extraction.text import TfidfTransformer

class TfidfTransformer(TfidfTransformer):
    def __init__(
        self,
        norm:Categorical('l1', 'l2'),
        use_idf:Boolean(),
        smooth_idf:Boolean(),
        sublinear_tf:Boolean()
    ):
        self.norm=norm
        self.use_idf=use_idf
        self.smooth_idf=smooth_idf
        self.sublinear_tf=sublinear_tf

        super().__init__(
            self,
            norm=norm,
            use_idf=use_idf,
            smooth_idf=smooth_idf,
            sublinear_tf=sublinear_tf
        )


from sklearn.feature_extraction.text import TfidfVectorizer

class TfidfVectorizer(TfidfVectorizer):
    def __init__(
        self,
        lowercase:Boolean(),
        max_df:Discrete(min=1e-06, max=1),
        min_df:Discrete(min=0, max=2),
        binary:Boolean(),
        use_idf:Boolean(),
        smooth_idf:Boolean(),
        sublinear_tf:Boolean()
    ):
        self.lowercase=lowercase
        self.max_df=max_df
        self.min_df=min_df
        self.binary=binary
        self.use_idf=use_idf
        self.smooth_idf=smooth_idf
        self.sublinear_tf=sublinear_tf

        super().__init__(
            self,
            lowercase=lowercase,
            max_df=max_df,
            min_df=min_df,
            binary=binary,
            use_idf=use_idf,
            smooth_idf=smooth_idf,
            sublinear_tf=sublinear_tf
        )


from sklearn.isotonic import IsotonicRegression

class IsotonicRegression(IsotonicRegression):
    def __init__(
        self,
        increasing:Boolean()
    ):
        self.increasing=increasing

        super().__init__(
            self,
            increasing=increasing
        )


from sklearn.linear_model._base import LinearRegression

class LinearRegression(LinearRegression):
    def __init__(
        self,
        fit_intercept:Boolean(),
        normalize:Boolean()
    ):
        self.fit_intercept=fit_intercept
        self.normalize=normalize

        super().__init__(
            self,
            fit_intercept=fit_intercept,
            normalize=normalize
        )


from sklearn.linear_model._bayes import ARDRegression

class ARDRegression(ARDRegression):
    def __init__(
        self,
        n_iter:Discrete(min=150, max=600),
        tol:Discrete(min=1e-05, max=1),
        alpha_1:Discrete(min=1e-08, max=1),
        alpha_2:Discrete(min=1e-08, max=1),
        lambda_1:Discrete(min=1e-08, max=1),
        lambda_2:Discrete(min=1e-08, max=1),
        compute_score:Boolean(),
        threshold_lambda:Discrete(min=5000.0, max=20000.0),
        fit_intercept:Boolean(),
        normalize:Boolean()
    ):
        self.n_iter=n_iter
        self.tol=tol
        self.alpha_1=alpha_1
        self.alpha_2=alpha_2
        self.lambda_1=lambda_1
        self.lambda_2=lambda_2
        self.compute_score=compute_score
        self.threshold_lambda=threshold_lambda
        self.fit_intercept=fit_intercept
        self.normalize=normalize

        super().__init__(
            self,
            n_iter=n_iter,
            tol=tol,
            alpha_1=alpha_1,
            alpha_2=alpha_2,
            lambda_1=lambda_1,
            lambda_2=lambda_2,
            compute_score=compute_score,
            threshold_lambda=threshold_lambda,
            fit_intercept=fit_intercept,
            normalize=normalize
        )


from sklearn.linear_model._bayes import BayesianRidge

class BayesianRidge(BayesianRidge):
    def __init__(
        self,
        n_iter:Discrete(min=150, max=600),
        tol:Discrete(min=1e-05, max=1),
        alpha_1:Discrete(min=1e-08, max=1),
        alpha_2:Discrete(min=1e-08, max=1),
        lambda_1:Discrete(min=1e-08, max=1),
        lambda_2:Discrete(min=1e-08, max=1),
        compute_score:Boolean(),
        fit_intercept:Boolean(),
        normalize:Boolean()
    ):
        self.n_iter=n_iter
        self.tol=tol
        self.alpha_1=alpha_1
        self.alpha_2=alpha_2
        self.lambda_1=lambda_1
        self.lambda_2=lambda_2
        self.compute_score=compute_score
        self.fit_intercept=fit_intercept
        self.normalize=normalize

        super().__init__(
            self,
            n_iter=n_iter,
            tol=tol,
            alpha_1=alpha_1,
            alpha_2=alpha_2,
            lambda_1=lambda_1,
            lambda_2=lambda_2,
            compute_score=compute_score,
            fit_intercept=fit_intercept,
            normalize=normalize
        )


from sklearn.linear_model._coordinate_descent import ElasticNet

class ElasticNet(ElasticNet):
    def __init__(
        self,
        alpha:Discrete(min=1e-06, max=1),
        l1_ratio:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        normalize:Boolean(),
        precompute:Boolean(),
        tol:Discrete(min=1e-06, max=1),
        positive:Boolean(),
        selection:Categorical('cyclic', 'random')
    ):
        self.alpha=alpha
        self.l1_ratio=l1_ratio
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute
        self.tol=tol
        self.positive=positive
        self.selection=selection

        super().__init__(
            self,
            alpha=alpha,
            l1_ratio=l1_ratio,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            tol=tol,
            positive=positive,
            selection=selection
        )


from sklearn.linear_model._coordinate_descent import Lasso

class Lasso(Lasso):
    def __init__(
        self,
        alpha:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        normalize:Boolean(),
        precompute:Boolean(),
        tol:Discrete(min=1e-06, max=1),
        positive:Boolean(),
        selection:Categorical('cyclic', 'random')
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute
        self.tol=tol
        self.positive=positive
        self.selection=selection

        super().__init__(
            self,
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            tol=tol,
            positive=positive,
            selection=selection
        )


from sklearn.linear_model._coordinate_descent import MultiTaskElasticNet

class MultiTaskElasticNet(MultiTaskElasticNet):
    def __init__(
        self,
        alpha:Discrete(min=1e-06, max=1),
        l1_ratio:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        normalize:Boolean(),
        tol:Discrete(min=1e-06, max=1)
    ):
        self.alpha=alpha
        self.l1_ratio=l1_ratio
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.tol=tol

        super().__init__(
            self,
            alpha=alpha,
            l1_ratio=l1_ratio,
            fit_intercept=fit_intercept,
            normalize=normalize,
            tol=tol
        )


from sklearn.linear_model._coordinate_descent import MultiTaskLasso

class MultiTaskLasso(MultiTaskLasso):
    def __init__(
        self,
        alpha:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        normalize:Boolean(),
        tol:Discrete(min=1e-06, max=1)
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.tol=tol

        super().__init__(
            self,
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            tol=tol
        )


from sklearn.linear_model._huber import HuberRegressor

class HuberRegressor(HuberRegressor):
    def __init__(
        self,
        epsilon:Discrete(min=0.675, max=2.7),
        alpha:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        tol:Discrete(min=1.0000000000000001e-07, max=1)
    ):
        self.epsilon=epsilon
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.tol=tol

        super().__init__(
            self,
            epsilon=epsilon,
            alpha=alpha,
            fit_intercept=fit_intercept,
            tol=tol
        )


from sklearn.linear_model._least_angle import Lars

class Lars(Lars):
    def __init__(
        self,
        fit_intercept:Boolean(),
        normalize:Boolean(),
        precompute:Categorical('auto'),
        n_nonzero_coefs:Discrete(min=250, max=1000),
        fit_path:Boolean()
    ):
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute
        self.n_nonzero_coefs=n_nonzero_coefs
        self.fit_path=fit_path

        super().__init__(
            self,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            n_nonzero_coefs=n_nonzero_coefs,
            fit_path=fit_path
        )


from sklearn.linear_model._least_angle import LassoLars

class LassoLars(LassoLars):
    def __init__(
        self,
        alpha:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        normalize:Boolean(),
        precompute:Categorical('auto'),
        fit_path:Boolean(),
        positive:Boolean()
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute
        self.fit_path=fit_path
        self.positive=positive

        super().__init__(
            self,
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            fit_path=fit_path,
            positive=positive
        )


from sklearn.linear_model._least_angle import LassoLarsIC

class LassoLarsIC(LassoLarsIC):
    def __init__(
        self,
        criterion:Categorical('aic', 'bic'),
        fit_intercept:Boolean(),
        normalize:Boolean(),
        precompute:Categorical('auto'),
        positive:Boolean()
    ):
        self.criterion=criterion
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute
        self.positive=positive

        super().__init__(
            self,
            criterion=criterion,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            positive=positive
        )


from sklearn.linear_model._logistic import LogisticRegression

class LogisticRegression(LogisticRegression):
    def __init__(
        self,
        penalty:Categorical('l2', 'none'),
        dual:Boolean(),
        tol:Discrete(min=1e-06, max=1),
        C:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        intercept_scaling:Discrete(min=0, max=2),
        solver:Categorical('lbfgs', 'liblinear', 'sag', 'saga'),
        multi_class:Categorical('auto', 'multinomial', 'ovr')
    ):
        self.penalty=penalty
        self.dual=dual
        self.tol=tol
        self.C=C
        self.fit_intercept=fit_intercept
        self.intercept_scaling=intercept_scaling
        self.solver=solver
        self.multi_class=multi_class

        super().__init__(
            self,
            penalty=penalty,
            dual=dual,
            tol=tol,
            C=C,
            fit_intercept=fit_intercept,
            intercept_scaling=intercept_scaling,
            solver=solver,
            multi_class=multi_class
        )


from sklearn.linear_model._omp import OrthogonalMatchingPursuit

class OrthogonalMatchingPursuit(OrthogonalMatchingPursuit):
    def __init__(
        self,
        fit_intercept:Boolean(),
        normalize:Boolean(),
        precompute:Categorical('auto')
    ):
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.precompute=precompute

        super().__init__(
            self,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute
        )


from sklearn.linear_model._passive_aggressive import PassiveAggressiveClassifier

class PassiveAggressiveClassifier(PassiveAggressiveClassifier):
    def __init__(
        self,
        C:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        tol:Discrete(min=1e-05, max=1),
        early_stopping:Boolean(),
        validation_fraction:Discrete(min=0.001, max=1),
        n_iter_no_change:Discrete(min=2, max=10),
        shuffle:Boolean(),
        average:Boolean()
    ):
        self.C=C
        self.fit_intercept=fit_intercept
        self.tol=tol
        self.early_stopping=early_stopping
        self.validation_fraction=validation_fraction
        self.n_iter_no_change=n_iter_no_change
        self.shuffle=shuffle
        self.average=average

        super().__init__(
            self,
            C=C,
            fit_intercept=fit_intercept,
            tol=tol,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            shuffle=shuffle,
            average=average
        )


from sklearn.linear_model._passive_aggressive import PassiveAggressiveRegressor

class PassiveAggressiveRegressor(PassiveAggressiveRegressor):
    def __init__(
        self,
        C:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        tol:Discrete(min=1e-05, max=1),
        early_stopping:Boolean(),
        validation_fraction:Discrete(min=0.001, max=1),
        n_iter_no_change:Discrete(min=2, max=10),
        shuffle:Boolean(),
        epsilon:Discrete(min=0.001, max=1),
        average:Boolean()
    ):
        self.C=C
        self.fit_intercept=fit_intercept
        self.tol=tol
        self.early_stopping=early_stopping
        self.validation_fraction=validation_fraction
        self.n_iter_no_change=n_iter_no_change
        self.shuffle=shuffle
        self.epsilon=epsilon
        self.average=average

        super().__init__(
            self,
            C=C,
            fit_intercept=fit_intercept,
            tol=tol,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            shuffle=shuffle,
            epsilon=epsilon,
            average=average
        )


from sklearn.linear_model._perceptron import Perceptron

class Perceptron(Perceptron):
    def __init__(
        self,
        alpha:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        tol:Discrete(min=1e-05, max=1),
        shuffle:Boolean(),
        eta0:Discrete(min=1e-06, max=1),
        early_stopping:Boolean(),
        validation_fraction:Discrete(min=0.001, max=1),
        n_iter_no_change:Discrete(min=2, max=10)
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.tol=tol
        self.shuffle=shuffle
        self.eta0=eta0
        self.early_stopping=early_stopping
        self.validation_fraction=validation_fraction
        self.n_iter_no_change=n_iter_no_change

        super().__init__(
            self,
            alpha=alpha,
            fit_intercept=fit_intercept,
            tol=tol,
            shuffle=shuffle,
            eta0=eta0,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change
        )


from sklearn.linear_model._ransac import RANSACRegressor

class RANSACRegressor(RANSACRegressor):
    def __init__(
        self,
        max_trials:Discrete(min=50, max=200),
        max_skips:Discrete(min=inf, max=inf),
        stop_n_inliers:Discrete(min=inf, max=inf),
        stop_score:Discrete(min=inf, max=inf),
        stop_probability:Discrete(min=1e-06, max=1)
    ):
        self.max_trials=max_trials
        self.max_skips=max_skips
        self.stop_n_inliers=stop_n_inliers
        self.stop_score=stop_score
        self.stop_probability=stop_probability

        super().__init__(
            self,
            max_trials=max_trials,
            max_skips=max_skips,
            stop_n_inliers=stop_n_inliers,
            stop_score=stop_score,
            stop_probability=stop_probability
        )


from sklearn.linear_model._ridge import Ridge

class Ridge(Ridge):
    def __init__(
        self,
        alpha:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        normalize:Boolean(),
        tol:Discrete(min=1e-05, max=1),
        solver:Categorical('auto', 'cholesky', 'lsqr', 'saga', 'sparse_cg', 'svd')
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.tol=tol
        self.solver=solver

        super().__init__(
            self,
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            tol=tol,
            solver=solver
        )


from sklearn.linear_model._ridge import RidgeClassifier

class RidgeClassifier(RidgeClassifier):
    def __init__(
        self,
        alpha:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        normalize:Boolean(),
        tol:Discrete(min=1e-05, max=1),
        solver:Categorical('auto', 'cholesky', 'lsqr', 'saga', 'sparse_cg', 'svd')
    ):
        self.alpha=alpha
        self.fit_intercept=fit_intercept
        self.normalize=normalize
        self.tol=tol
        self.solver=solver

        super().__init__(
            self,
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            tol=tol,
            solver=solver
        )


from sklearn.linear_model._stochastic_gradient import SGDClassifier

class SGDClassifier(SGDClassifier):
    def __init__(
        self,
        loss:Categorical('epsilon_insensitive', 'hinge', 'huber', 'log', 'modified_huber', 'perceptron', 'squared_epsilon_insensitive', 'squared_hinge', 'squared_loss'),
        penalty:Categorical('elasticnet', 'l1', 'l2', 'none'),
        alpha:Discrete(min=1e-06, max=1),
        l1_ratio:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        tol:Discrete(min=1e-05, max=1),
        shuffle:Boolean(),
        epsilon:Discrete(min=0.001, max=1),
        learning_rate:Categorical('optimal'),
        eta0:Discrete(min=-1, max=1),
        power_t:Discrete(min=1e-06, max=1),
        early_stopping:Boolean(),
        validation_fraction:Discrete(min=0.001, max=1),
        n_iter_no_change:Discrete(min=2, max=10),
        average:Boolean()
    ):
        self.loss=loss
        self.penalty=penalty
        self.alpha=alpha
        self.l1_ratio=l1_ratio
        self.fit_intercept=fit_intercept
        self.tol=tol
        self.shuffle=shuffle
        self.epsilon=epsilon
        self.learning_rate=learning_rate
        self.eta0=eta0
        self.power_t=power_t
        self.early_stopping=early_stopping
        self.validation_fraction=validation_fraction
        self.n_iter_no_change=n_iter_no_change
        self.average=average

        super().__init__(
            self,
            loss=loss,
            penalty=penalty,
            alpha=alpha,
            l1_ratio=l1_ratio,
            fit_intercept=fit_intercept,
            tol=tol,
            shuffle=shuffle,
            epsilon=epsilon,
            learning_rate=learning_rate,
            eta0=eta0,
            power_t=power_t,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            average=average
        )


from sklearn.linear_model._stochastic_gradient import SGDRegressor

class SGDRegressor(SGDRegressor):
    def __init__(
        self,
        loss:Categorical('epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_loss'),
        penalty:Categorical('elasticnet', 'l1', 'l2', 'none'),
        alpha:Discrete(min=1e-06, max=1),
        l1_ratio:Discrete(min=1e-06, max=1),
        fit_intercept:Boolean(),
        tol:Discrete(min=1e-05, max=1),
        shuffle:Boolean(),
        epsilon:Discrete(min=0.001, max=1),
        learning_rate:Categorical('adaptive', 'constant', 'invscaling', 'optimal'),
        eta0:Discrete(min=0.0001, max=1),
        power_t:Discrete(min=1e-06, max=1),
        early_stopping:Boolean(),
        validation_fraction:Discrete(min=0.001, max=1),
        n_iter_no_change:Discrete(min=2, max=10),
        average:Boolean()
    ):
        self.loss=loss
        self.penalty=penalty
        self.alpha=alpha
        self.l1_ratio=l1_ratio
        self.fit_intercept=fit_intercept
        self.tol=tol
        self.shuffle=shuffle
        self.epsilon=epsilon
        self.learning_rate=learning_rate
        self.eta0=eta0
        self.power_t=power_t
        self.early_stopping=early_stopping
        self.validation_fraction=validation_fraction
        self.n_iter_no_change=n_iter_no_change
        self.average=average

        super().__init__(
            self,
            loss=loss,
            penalty=penalty,
            alpha=alpha,
            l1_ratio=l1_ratio,
            fit_intercept=fit_intercept,
            tol=tol,
            shuffle=shuffle,
            epsilon=epsilon,
            learning_rate=learning_rate,
            eta0=eta0,
            power_t=power_t,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            average=average
        )


from sklearn.linear_model._theil_sen import TheilSenRegressor

class TheilSenRegressor(TheilSenRegressor):
    def __init__(
        self,
        fit_intercept:Boolean(),
        max_subpopulation:Discrete(min=5000.0, max=20000.0),
        tol:Discrete(min=1e-05, max=1)
    ):
        self.fit_intercept=fit_intercept
        self.max_subpopulation=max_subpopulation
        self.tol=tol

        super().__init__(
            self,
            fit_intercept=fit_intercept,
            max_subpopulation=max_subpopulation,
            tol=tol
        )


from sklearn.manifold._isomap import Isomap

class Isomap(Isomap):
    def __init__(
        self,
        n_neighbors:Discrete(min=2, max=10),
        n_components:Discrete(min=1, max=4),
        eigen_solver:Categorical('arpack', 'auto', 'dense'),
        tol:Discrete(min=-100, max=100),
        path_method:Categorical('auto'),
        neighbors_algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        p:Discrete(min=1, max=4)
    ):
        self.n_neighbors=n_neighbors
        self.n_components=n_components
        self.eigen_solver=eigen_solver
        self.tol=tol
        self.path_method=path_method
        self.neighbors_algorithm=neighbors_algorithm
        self.p=p

        super().__init__(
            self,
            n_neighbors=n_neighbors,
            n_components=n_components,
            eigen_solver=eigen_solver,
            tol=tol,
            path_method=path_method,
            neighbors_algorithm=neighbors_algorithm,
            p=p
        )


from sklearn.manifold._locally_linear import LocallyLinearEmbedding

class LocallyLinearEmbedding(LocallyLinearEmbedding):
    def __init__(
        self,
        n_neighbors:Discrete(min=2, max=10),
        n_components:Discrete(min=1, max=4),
        reg:Discrete(min=1e-05, max=1),
        eigen_solver:Categorical('arpack', 'auto', 'dense'),
        tol:Discrete(min=1e-08, max=1),
        method:Categorical('ltsa', 'modified', 'standard'),
        hessian_tol:Discrete(min=1e-06, max=1),
        modified_tol:Discrete(min=1e-14, max=1),
        neighbors_algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree')
    ):
        self.n_neighbors=n_neighbors
        self.n_components=n_components
        self.reg=reg
        self.eigen_solver=eigen_solver
        self.tol=tol
        self.method=method
        self.hessian_tol=hessian_tol
        self.modified_tol=modified_tol
        self.neighbors_algorithm=neighbors_algorithm

        super().__init__(
            self,
            n_neighbors=n_neighbors,
            n_components=n_components,
            reg=reg,
            eigen_solver=eigen_solver,
            tol=tol,
            method=method,
            hessian_tol=hessian_tol,
            modified_tol=modified_tol,
            neighbors_algorithm=neighbors_algorithm
        )


from sklearn.manifold._mds import MDS

class MDS(MDS):
    def __init__(
        self,
        n_components:Discrete(min=1, max=4),
        metric:Boolean(),
        n_init:Discrete(min=2, max=8),
        dissimilarity:Categorical('euclidean')
    ):
        self.n_components=n_components
        self.metric=metric
        self.n_init=n_init
        self.dissimilarity=dissimilarity

        super().__init__(
            self,
            n_components=n_components,
            metric=metric,
            n_init=n_init,
            dissimilarity=dissimilarity
        )


from sklearn.manifold._spectral_embedding import SpectralEmbedding

class SpectralEmbedding(SpectralEmbedding):
    def __init__(
        self,
        n_components:Discrete(min=1, max=4),
        affinity:Categorical('nearest_neighbors', 'rbf')
    ):
        self.n_components=n_components
        self.affinity=affinity

        super().__init__(
            self,
            n_components=n_components,
            affinity=affinity
        )


from sklearn.manifold._t_sne import TSNE

class TSNE(TSNE):
    def __init__(
        self,
        n_components:Discrete(min=1, max=4),
        perplexity:Discrete(min=15.0, max=60.0),
        early_exaggeration:Discrete(min=6.0, max=24.0),
        learning_rate:Discrete(min=100.0, max=400.0),
        n_iter:Discrete(min=500, max=2000),
        n_iter_without_progress:Discrete(min=150, max=600),
        min_grad_norm:Discrete(min=9.999999999999999e-10, max=1),
        init:Categorical('pca', 'random'),
        angle:Discrete(min=1e-06, max=1)
    ):
        self.n_components=n_components
        self.perplexity=perplexity
        self.early_exaggeration=early_exaggeration
        self.learning_rate=learning_rate
        self.n_iter=n_iter
        self.n_iter_without_progress=n_iter_without_progress
        self.min_grad_norm=min_grad_norm
        self.init=init
        self.angle=angle

        super().__init__(
            self,
            n_components=n_components,
            perplexity=perplexity,
            early_exaggeration=early_exaggeration,
            learning_rate=learning_rate,
            n_iter=n_iter,
            n_iter_without_progress=n_iter_without_progress,
            min_grad_norm=min_grad_norm,
            init=init,
            angle=angle
        )


from sklearn.neighbors._classification import KNeighborsClassifier

class KNeighborsClassifier(KNeighborsClassifier):
    def __init__(
        self,
        n_neighbors:Discrete(min=2, max=10),
        weights:Categorical('distance', 'uniform'),
        algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size:Discrete(min=15, max=60),
        p:Discrete(min=1, max=4),
        metric:Categorical('minkowski')
    ):
        self.n_neighbors=n_neighbors
        self.weights=weights
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.p=p
        self.metric=metric

        super().__init__(
            self,
            n_neighbors=n_neighbors,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )


from sklearn.neighbors._classification import RadiusNeighborsClassifier

class RadiusNeighborsClassifier(RadiusNeighborsClassifier):
    def __init__(
        self,
        radius:Discrete(min=1e-06, max=1),
        weights:Categorical('distance', 'uniform'),
        algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size:Discrete(min=15, max=60),
        p:Discrete(min=1, max=4),
        metric:Categorical('minkowski')
    ):
        self.radius=radius
        self.weights=weights
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.p=p
        self.metric=metric

        super().__init__(
            self,
            radius=radius,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )


from sklearn.neighbors._graph import KNeighborsTransformer

class KNeighborsTransformer(KNeighborsTransformer):
    def __init__(
        self,
        mode:Categorical('connectivity', 'distance'),
        n_neighbors:Discrete(min=2, max=10),
        algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size:Discrete(min=15, max=60),
        metric:Categorical('braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'l1', 'l2', 'mahalanobis', 'manhattan', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'),
        p:Discrete(min=1, max=4)
    ):
        self.mode=mode
        self.n_neighbors=n_neighbors
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.metric=metric
        self.p=p

        super().__init__(
            self,
            mode=mode,
            n_neighbors=n_neighbors,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p
        )


from sklearn.neighbors._graph import RadiusNeighborsTransformer

class RadiusNeighborsTransformer(RadiusNeighborsTransformer):
    def __init__(
        self,
        mode:Categorical('connectivity', 'distance'),
        radius:Discrete(min=1e-06, max=1),
        algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size:Discrete(min=15, max=60),
        metric:Categorical('braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'l1', 'l2', 'manhattan', 'minkowski', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'),
        p:Discrete(min=1, max=4)
    ):
        self.mode=mode
        self.radius=radius
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.metric=metric
        self.p=p

        super().__init__(
            self,
            mode=mode,
            radius=radius,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p
        )


from sklearn.neighbors._kde import KernelDensity

class KernelDensity(KernelDensity):
    def __init__(
        self,
        bandwidth:Discrete(min=1e-06, max=1),
        algorithm:Categorical('auto', 'ball_tree', 'kd_tree'),
        kernel:Categorical('cosine', 'epanechnikov', 'exponential', 'gaussian', 'linear', 'tophat'),
        metric:Categorical('euclidean'),
        atol:Discrete(min=-100, max=100),
        rtol:Discrete(min=-100, max=100),
        breadth_first:Boolean(),
        leaf_size:Discrete(min=20, max=80)
    ):
        self.bandwidth=bandwidth
        self.algorithm=algorithm
        self.kernel=kernel
        self.metric=metric
        self.atol=atol
        self.rtol=rtol
        self.breadth_first=breadth_first
        self.leaf_size=leaf_size

        super().__init__(
            self,
            bandwidth=bandwidth,
            algorithm=algorithm,
            kernel=kernel,
            metric=metric,
            atol=atol,
            rtol=rtol,
            breadth_first=breadth_first,
            leaf_size=leaf_size
        )


from sklearn.neighbors._lof import LocalOutlierFactor

class LocalOutlierFactor(LocalOutlierFactor):
    def __init__(
        self,
        n_neighbors:Discrete(min=10, max=40),
        algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size:Discrete(min=15, max=60),
        metric:Categorical('braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'l1', 'l2', 'manhattan', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'),
        p:Discrete(min=1, max=4),
        contamination:Categorical('auto'),
        novelty:Boolean()
    ):
        self.n_neighbors=n_neighbors
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.metric=metric
        self.p=p
        self.contamination=contamination
        self.novelty=novelty

        super().__init__(
            self,
            n_neighbors=n_neighbors,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p,
            contamination=contamination,
            novelty=novelty
        )


from sklearn.neighbors._nca import NeighborhoodComponentsAnalysis

class NeighborhoodComponentsAnalysis(NeighborhoodComponentsAnalysis):
    def __init__(
        self,
        init:Categorical('auto', 'identity', 'pca', 'random'),
        tol:Discrete(min=1.0000000000000001e-07, max=1)
    ):
        self.init=init
        self.tol=tol

        super().__init__(
            self,
            init=init,
            tol=tol
        )


from sklearn.neighbors._nearest_centroid import NearestCentroid

class NearestCentroid(NearestCentroid):
    def __init__(
        self,

    ):


        super().__init__(
            self,

        )


from sklearn.neighbors._regression import KNeighborsRegressor

class KNeighborsRegressor(KNeighborsRegressor):
    def __init__(
        self,
        n_neighbors:Discrete(min=2, max=10),
        weights:Categorical('distance', 'uniform'),
        algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size:Discrete(min=15, max=60),
        p:Discrete(min=1, max=4),
        metric:Categorical('minkowski')
    ):
        self.n_neighbors=n_neighbors
        self.weights=weights
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.p=p
        self.metric=metric

        super().__init__(
            self,
            n_neighbors=n_neighbors,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )


from sklearn.neighbors._regression import RadiusNeighborsRegressor

class RadiusNeighborsRegressor(RadiusNeighborsRegressor):
    def __init__(
        self,
        radius:Discrete(min=1e-06, max=1),
        weights:Categorical('distance', 'uniform'),
        algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size:Discrete(min=15, max=60),
        p:Discrete(min=1, max=4),
        metric:Categorical('minkowski')
    ):
        self.radius=radius
        self.weights=weights
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.p=p
        self.metric=metric

        super().__init__(
            self,
            radius=radius,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )


from sklearn.neighbors._unsupervised import NearestNeighbors

class NearestNeighbors(NearestNeighbors):
    def __init__(
        self,
        n_neighbors:Discrete(min=2, max=10),
        radius:Discrete(min=1e-06, max=1),
        algorithm:Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size:Discrete(min=15, max=60),
        metric:Categorical('minkowski'),
        p:Discrete(min=1, max=4)
    ):
        self.n_neighbors=n_neighbors
        self.radius=radius
        self.algorithm=algorithm
        self.leaf_size=leaf_size
        self.metric=metric
        self.p=p

        super().__init__(
            self,
            n_neighbors=n_neighbors,
            radius=radius,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p
        )


from sklearn.preprocessing._data import Binarizer

class Binarizer(Binarizer):
    def __init__(
        self,
        threshold:Discrete(min=-1, max=1)
    ):
        self.threshold=threshold

        super().__init__(
            self,
            threshold=threshold
        )


from sklearn.preprocessing._data import KernelCenterer

class KernelCenterer(KernelCenterer):
    def __init__(
        self,

    ):


        super().__init__(
            self,

        )


from sklearn.preprocessing._data import MaxAbsScaler

class MaxAbsScaler(MaxAbsScaler):
    def __init__(
        self,

    ):


        super().__init__(
            self,

        )


from sklearn.preprocessing._data import MinMaxScaler

class MinMaxScaler(MinMaxScaler):
    def __init__(
        self,

    ):


        super().__init__(
            self,

        )


from sklearn.preprocessing._data import Normalizer

class Normalizer(Normalizer):
    def __init__(
        self,
        norm:Categorical('l1', 'l2', 'max')
    ):
        self.norm=norm

        super().__init__(
            self,
            norm=norm
        )


from sklearn.preprocessing._data import PolynomialFeatures

class PolynomialFeatures(PolynomialFeatures):
    def __init__(
        self,
        degree:Discrete(min=1, max=4),
        interaction_only:Boolean(),
        include_bias:Boolean(),
        order:Categorical('c', 'f')
    ):
        self.degree=degree
        self.interaction_only=interaction_only
        self.include_bias=include_bias
        self.order=order

        super().__init__(
            self,
            degree=degree,
            interaction_only=interaction_only,
            include_bias=include_bias,
            order=order
        )


from sklearn.preprocessing._data import PowerTransformer

class PowerTransformer(PowerTransformer):
    def __init__(
        self,
        standardize:Boolean()
    ):
        self.standardize=standardize

        super().__init__(
            self,
            standardize=standardize
        )


from sklearn.preprocessing._data import QuantileTransformer

class QuantileTransformer(QuantileTransformer):
    def __init__(
        self,
        n_quantiles:Discrete(min=500, max=2000),
        output_distribution:Categorical('normal', 'uniform'),
        ignore_implicit_zeros:Boolean(),
        subsample:Discrete(min=50000, max=200000)
    ):
        self.n_quantiles=n_quantiles
        self.output_distribution=output_distribution
        self.ignore_implicit_zeros=ignore_implicit_zeros
        self.subsample=subsample

        super().__init__(
            self,
            n_quantiles=n_quantiles,
            output_distribution=output_distribution,
            ignore_implicit_zeros=ignore_implicit_zeros,
            subsample=subsample
        )


from sklearn.preprocessing._data import RobustScaler

class RobustScaler(RobustScaler):
    def __init__(
        self,
        with_centering:Boolean(),
        with_scaling:Boolean()
    ):
        self.with_centering=with_centering
        self.with_scaling=with_scaling

        super().__init__(
            self,
            with_centering=with_centering,
            with_scaling=with_scaling
        )


from sklearn.preprocessing._data import StandardScaler

class StandardScaler(StandardScaler):
    def __init__(
        self,
        with_mean:Boolean(),
        with_std:Boolean()
    ):
        self.with_mean=with_mean
        self.with_std=with_std

        super().__init__(
            self,
            with_mean=with_mean,
            with_std=with_std
        )


from sklearn.preprocessing._discretization import KBinsDiscretizer

class KBinsDiscretizer(KBinsDiscretizer):
    def __init__(
        self,
        n_bins:Discrete(min=2, max=10),
        encode:Categorical('onehot', 'ordinal'),
        strategy:Categorical('kmeans', 'quantile', 'uniform')
    ):
        self.n_bins=n_bins
        self.encode=encode
        self.strategy=strategy

        super().__init__(
            self,
            n_bins=n_bins,
            encode=encode,
            strategy=strategy
        )


from sklearn.preprocessing._encoders import OneHotEncoder

class OneHotEncoder(OneHotEncoder):
    def __init__(
        self,
        categories:Categorical('auto'),
        sparse:Boolean(),
        handle_unknown:Categorical('error', 'ignore')
    ):
        self.categories=categories
        self.sparse=sparse
        self.handle_unknown=handle_unknown

        super().__init__(
            self,
            categories=categories,
            sparse=sparse,
            handle_unknown=handle_unknown
        )


from sklearn.preprocessing._encoders import OrdinalEncoder

class OrdinalEncoder(OrdinalEncoder):
    def __init__(
        self,
        categories:Categorical('auto')
    ):
        self.categories=categories

        super().__init__(
            self,
            categories=categories
        )


from sklearn.preprocessing._function_transformer import FunctionTransformer

class FunctionTransformer(FunctionTransformer):
    def __init__(
        self,
        validate:Boolean(),
        accept_sparse:Boolean(),
        check_inverse:Boolean()
    ):
        self.validate=validate
        self.accept_sparse=accept_sparse
        self.check_inverse=check_inverse

        super().__init__(
            self,
            validate=validate,
            accept_sparse=accept_sparse,
            check_inverse=check_inverse
        )


from sklearn.preprocessing._label import LabelBinarizer

class LabelBinarizer(LabelBinarizer):
    def __init__(
        self,
        neg_label:Discrete(min=-100, max=100),
        pos_label:Discrete(min=0, max=2),
        sparse_output:Boolean()
    ):
        self.neg_label=neg_label
        self.pos_label=pos_label
        self.sparse_output=sparse_output

        super().__init__(
            self,
            neg_label=neg_label,
            pos_label=pos_label,
            sparse_output=sparse_output
        )


from sklearn.preprocessing._label import LabelEncoder

class LabelEncoder(LabelEncoder):
    def __init__(
        self,

    ):


        super().__init__(
            self,

        )


from sklearn.preprocessing._label import MultiLabelBinarizer

class MultiLabelBinarizer(MultiLabelBinarizer):
    def __init__(
        self,
        sparse_output:Boolean()
    ):
        self.sparse_output=sparse_output

        super().__init__(
            self,
            sparse_output=sparse_output
        )


from sklearn.svm._classes import LinearSVC

class LinearSVC(LinearSVC):
    def __init__(
        self,
        penalty:Categorical('l2'),
        loss:Categorical('hinge', 'squared_hinge'),
        dual:Boolean(),
        tol:Discrete(min=1e-06, max=1),
        C:Discrete(min=1e-06, max=1),
        multi_class:Categorical('crammer_singer', 'ovr'),
        fit_intercept:Boolean(),
        intercept_scaling:Discrete(min=0, max=2)
    ):
        self.penalty=penalty
        self.loss=loss
        self.dual=dual
        self.tol=tol
        self.C=C
        self.multi_class=multi_class
        self.fit_intercept=fit_intercept
        self.intercept_scaling=intercept_scaling

        super().__init__(
            self,
            penalty=penalty,
            loss=loss,
            dual=dual,
            tol=tol,
            C=C,
            multi_class=multi_class,
            fit_intercept=fit_intercept,
            intercept_scaling=intercept_scaling
        )


from sklearn.svm._classes import LinearSVR

class LinearSVR(LinearSVR):
    def __init__(
        self,
        epsilon:Discrete(min=-1, max=1),
        tol:Discrete(min=1e-06, max=1),
        C:Discrete(min=1e-06, max=1),
        loss:Categorical('epsilon_insensitive', 'squared_epsilon_insensitive'),
        fit_intercept:Boolean(),
        intercept_scaling:Discrete(min=1e-06, max=1),
        dual:Boolean()
    ):
        self.epsilon=epsilon
        self.tol=tol
        self.C=C
        self.loss=loss
        self.fit_intercept=fit_intercept
        self.intercept_scaling=intercept_scaling
        self.dual=dual

        super().__init__(
            self,
            epsilon=epsilon,
            tol=tol,
            C=C,
            loss=loss,
            fit_intercept=fit_intercept,
            intercept_scaling=intercept_scaling,
            dual=dual
        )


from sklearn.svm._classes import NuSVC

class NuSVC(NuSVC):
    def __init__(
        self,
        nu:Discrete(min=1e-06, max=1),
        kernel:Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree:Discrete(min=1, max=6),
        gamma:Categorical('auto', 'scale'),
        coef0:Discrete(min=-1, max=1),
        shrinking:Boolean(),
        probability:Boolean(),
        tol:Discrete(min=1e-05, max=1),
        cache_size:Discrete(min=100, max=400),
        decision_function_shape:Categorical('ovo', 'ovr'),
        break_ties:Boolean()
    ):
        self.nu=nu
        self.kernel=kernel
        self.degree=degree
        self.gamma=gamma
        self.coef0=coef0
        self.shrinking=shrinking
        self.probability=probability
        self.tol=tol
        self.cache_size=cache_size
        self.decision_function_shape=decision_function_shape
        self.break_ties=break_ties

        super().__init__(
            self,
            nu=nu,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            shrinking=shrinking,
            probability=probability,
            tol=tol,
            cache_size=cache_size,
            decision_function_shape=decision_function_shape,
            break_ties=break_ties
        )


from sklearn.svm._classes import NuSVR

class NuSVR(NuSVR):
    def __init__(
        self,
        nu:Discrete(min=1e-06, max=1),
        C:Discrete(min=1e-06, max=1),
        kernel:Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree:Discrete(min=1, max=6),
        gamma:Categorical('auto', 'scale'),
        coef0:Discrete(min=-1, max=1),
        shrinking:Boolean(),
        tol:Discrete(min=1e-05, max=1),
        cache_size:Discrete(min=100, max=400)
    ):
        self.nu=nu
        self.C=C
        self.kernel=kernel
        self.degree=degree
        self.gamma=gamma
        self.coef0=coef0
        self.shrinking=shrinking
        self.tol=tol
        self.cache_size=cache_size

        super().__init__(
            self,
            nu=nu,
            C=C,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            shrinking=shrinking,
            tol=tol,
            cache_size=cache_size
        )


from sklearn.svm._classes import OneClassSVM

class OneClassSVM(OneClassSVM):
    def __init__(
        self,
        kernel:Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree:Discrete(min=1, max=6),
        gamma:Categorical('auto', 'scale'),
        coef0:Discrete(min=-1, max=1),
        tol:Discrete(min=1e-05, max=1),
        nu:Discrete(min=1e-06, max=1),
        shrinking:Boolean(),
        cache_size:Discrete(min=100, max=400)
    ):
        self.kernel=kernel
        self.degree=degree
        self.gamma=gamma
        self.coef0=coef0
        self.tol=tol
        self.nu=nu
        self.shrinking=shrinking
        self.cache_size=cache_size

        super().__init__(
            self,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            tol=tol,
            nu=nu,
            shrinking=shrinking,
            cache_size=cache_size
        )


from sklearn.svm._classes import SVC

class SVC(SVC):
    def __init__(
        self,
        C:Discrete(min=1e-06, max=1),
        degree:Discrete(min=1, max=6),
        gamma:Categorical('auto', 'scale'),
        coef0:Discrete(min=-1, max=1),
        shrinking:Boolean(),
        probability:Boolean(),
        tol:Discrete(min=1e-05, max=1),
        cache_size:Discrete(min=100, max=400),
        decision_function_shape:Categorical('ovo', 'ovr'),
        break_ties:Boolean()
    ):
        self.C=C
        self.degree=degree
        self.gamma=gamma
        self.coef0=coef0
        self.shrinking=shrinking
        self.probability=probability
        self.tol=tol
        self.cache_size=cache_size
        self.decision_function_shape=decision_function_shape
        self.break_ties=break_ties

        super().__init__(
            self,
            C=C,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            shrinking=shrinking,
            probability=probability,
            tol=tol,
            cache_size=cache_size,
            decision_function_shape=decision_function_shape,
            break_ties=break_ties
        )


from sklearn.svm._classes import SVR

class SVR(SVR):
    def __init__(
        self,
        kernel:Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree:Discrete(min=1, max=6),
        gamma:Categorical('auto', 'scale'),
        coef0:Discrete(min=-1, max=1),
        tol:Discrete(min=1e-05, max=1),
        C:Discrete(min=1e-06, max=1),
        epsilon:Discrete(min=0.001, max=1),
        shrinking:Boolean(),
        cache_size:Discrete(min=100, max=400)
    ):
        self.kernel=kernel
        self.degree=degree
        self.gamma=gamma
        self.coef0=coef0
        self.tol=tol
        self.C=C
        self.epsilon=epsilon
        self.shrinking=shrinking
        self.cache_size=cache_size

        super().__init__(
            self,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            tol=tol,
            C=C,
            epsilon=epsilon,
            shrinking=shrinking,
            cache_size=cache_size
        )


from sklearn.tree._classes import BaseDecisionTree

class BaseDecisionTree(BaseDecisionTree):
    def __init__(
        self,
        ccp_alpha:Discrete(min=-1, max=1)
    ):
        self.ccp_alpha=ccp_alpha

        super().__init__(
            self,
            ccp_alpha=ccp_alpha
        )


from sklearn.tree._classes import DecisionTreeClassifier

class DecisionTreeClassifier(DecisionTreeClassifier):
    def __init__(
        self,
        min_samples_split:Discrete(min=1, max=4),
        min_samples_leaf:Discrete(min=0, max=2),
        min_weight_fraction_leaf:Discrete(min=-1, max=1),
        min_impurity_decrease:Discrete(min=-1, max=1),
        ccp_alpha:Discrete(min=-1, max=1)
    ):
        self.min_samples_split=min_samples_split
        self.min_samples_leaf=min_samples_leaf
        self.min_weight_fraction_leaf=min_weight_fraction_leaf
        self.min_impurity_decrease=min_impurity_decrease
        self.ccp_alpha=ccp_alpha

        super().__init__(
            self,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )


from sklearn.tree._classes import DecisionTreeRegressor

class DecisionTreeRegressor(DecisionTreeRegressor):
    def __init__(
        self,
        min_samples_split:Discrete(min=1, max=4),
        min_samples_leaf:Discrete(min=0, max=2),
        min_weight_fraction_leaf:Discrete(min=-1, max=1),
        min_impurity_decrease:Discrete(min=-1, max=1),
        ccp_alpha:Discrete(min=-1, max=1)
    ):
        self.min_samples_split=min_samples_split
        self.min_samples_leaf=min_samples_leaf
        self.min_weight_fraction_leaf=min_weight_fraction_leaf
        self.min_impurity_decrease=min_impurity_decrease
        self.ccp_alpha=ccp_alpha

        super().__init__(
            self,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )


from sklearn.tree._classes import ExtraTreeClassifier

class ExtraTreeClassifier(ExtraTreeClassifier):
    def __init__(
        self,
        min_samples_split:Discrete(min=1, max=4),
        min_samples_leaf:Discrete(min=0, max=2),
        min_weight_fraction_leaf:Discrete(min=-1, max=1),
        min_impurity_decrease:Discrete(min=-1, max=1),
        ccp_alpha:Discrete(min=-1, max=1)
    ):
        self.min_samples_split=min_samples_split
        self.min_samples_leaf=min_samples_leaf
        self.min_weight_fraction_leaf=min_weight_fraction_leaf
        self.min_impurity_decrease=min_impurity_decrease
        self.ccp_alpha=ccp_alpha

        super().__init__(
            self,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )


from sklearn.tree._classes import ExtraTreeRegressor

class ExtraTreeRegressor(ExtraTreeRegressor):
    def __init__(
        self,
        min_samples_split:Discrete(min=1, max=4),
        min_samples_leaf:Discrete(min=0, max=2),
        min_weight_fraction_leaf:Discrete(min=-1, max=1),
        min_impurity_decrease:Discrete(min=-1, max=1),
        ccp_alpha:Discrete(min=-1, max=1)
    ):
        self.min_samples_split=min_samples_split
        self.min_samples_leaf=min_samples_leaf
        self.min_weight_fraction_leaf=min_weight_fraction_leaf
        self.min_impurity_decrease=min_impurity_decrease
        self.ccp_alpha=ccp_alpha

        super().__init__(
            self,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )

