
# AUTOGENERATED ON 2020-01-15 17:03:29.100859
## DO NOT MODIFY THIS FILE MANUALLY

from numpy import inf, nan

from autogoal.grammar import Continuous, Discrete, Categorical, Boolean
from autogoal.contrib.sklearn._builder import SklearnEstimator, SklearnTransformer
from autogoal.kb import *

from sklearn.cluster._affinity_propagation import AffinityPropagation as _AffinityPropagation

class AffinityPropagation(SklearnEstimator, _AffinityPropagation):
    def __init__(
        self,
        damping: Continuous(min=1e-06, max=1),
        convergence_iter: Discrete(min=7, max=30),
        affinity: Categorical('euclidean')
    ):
        SklearnEstimator.__init__(self)
        _AffinityPropagation.__init__(
            self,
            damping=damping,
            convergence_iter=convergence_iter,
            affinity=affinity
        )

    def run(self, input: MatrixContinuous()) -> DiscreteVector():
       return super().run(input)

from sklearn.cluster._agglomerative import FeatureAgglomeration as _FeatureAgglomeration

class FeatureAgglomeration(SklearnTransformer, _FeatureAgglomeration):
    def __init__(
        self,
        n_clusters: Discrete(min=1, max=4),
        affinity: Categorical('euclidean'),
        compute_full_tree: Categorical('auto'),
        linkage: Categorical('average', 'complete', 'single', 'ward')
    ):
        SklearnTransformer.__init__(self)
        _FeatureAgglomeration.__init__(
            self,
            n_clusters=n_clusters,
            affinity=affinity,
            compute_full_tree=compute_full_tree,
            linkage=linkage
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.cluster._birch import Birch as _Birch

class Birch(SklearnEstimator, _Birch):
    def __init__(
        self,
        threshold: Continuous(min=1e-06, max=1),
        branching_factor: Discrete(min=25, max=100),
        n_clusters: Discrete(min=1, max=6),
        compute_labels: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _Birch.__init__(
            self,
            threshold=threshold,
            branching_factor=branching_factor,
            n_clusters=n_clusters,
            compute_labels=compute_labels
        )

    def run(self, input: MatrixContinuous()) -> DiscreteVector():
       return super().run(input)

from sklearn.cluster._kmeans import KMeans as _KMeans

class KMeans(SklearnEstimator, _KMeans):
    def __init__(
        self,
        n_clusters: Discrete(min=4, max=16),
        init: Categorical('random'),
        n_init: Discrete(min=5, max=20),
        tol: Continuous(min=1e-06, max=1),
        precompute_distances: Categorical('auto')
    ):
        SklearnEstimator.__init__(self)
        _KMeans.__init__(
            self,
            n_clusters=n_clusters,
            init=init,
            n_init=n_init,
            tol=tol,
            precompute_distances=precompute_distances
        )

    def run(self, input: MatrixContinuous()) -> DiscreteVector():
       return super().run(input)

from sklearn.cluster._kmeans import MiniBatchKMeans as _MiniBatchKMeans

class MiniBatchKMeans(SklearnEstimator, _MiniBatchKMeans):
    def __init__(
        self,
        n_clusters: Discrete(min=4, max=16),
        init: Categorical('random'),
        batch_size: Discrete(min=50, max=200),
        compute_labels: Boolean(),
        tol: Continuous(min=-1, max=1),
        max_no_improvement: Discrete(min=5, max=20),
        n_init: Discrete(min=1, max=6),
        reassignment_ratio: Continuous(min=0.0001, max=1)
    ):
        SklearnEstimator.__init__(self)
        _MiniBatchKMeans.__init__(
            self,
            n_clusters=n_clusters,
            init=init,
            batch_size=batch_size,
            compute_labels=compute_labels,
            tol=tol,
            max_no_improvement=max_no_improvement,
            n_init=n_init,
            reassignment_ratio=reassignment_ratio
        )

    def run(self, input: MatrixContinuous()) -> DiscreteVector():
       return super().run(input)

from sklearn.cluster._mean_shift import MeanShift as _MeanShift

class MeanShift(SklearnEstimator, _MeanShift):
    def __init__(
        self,
        bin_seeding: Boolean(),
        min_bin_freq: Discrete(min=0, max=2),
        cluster_all: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _MeanShift.__init__(
            self,
            bin_seeding=bin_seeding,
            min_bin_freq=min_bin_freq,
            cluster_all=cluster_all
        )

    def run(self, input: MatrixContinuousDense()) -> DiscreteVector():
       return super().run(input)

from sklearn.decomposition._dict_learning import DictionaryLearning as _DictionaryLearning

class DictionaryLearning(SklearnTransformer, _DictionaryLearning):
    def __init__(
        self,
        alpha: Discrete(min=0, max=2),
        tol: Continuous(min=1e-10, max=1),
        fit_algorithm: Categorical('cd', 'lars'),
        transform_algorithm: Categorical('lars', 'lasso_cd', 'lasso_lars', 'omp', 'threshold'),
        split_sign: Boolean(),
        positive_code: Boolean(),
        positive_dict: Boolean(),
        transform_max_iter: Discrete(min=500, max=2000)
    ):
        SklearnTransformer.__init__(self)
        _DictionaryLearning.__init__(
            self,
            alpha=alpha,
            tol=tol,
            fit_algorithm=fit_algorithm,
            transform_algorithm=transform_algorithm,
            split_sign=split_sign,
            positive_code=positive_code,
            positive_dict=positive_dict,
            transform_max_iter=transform_max_iter
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.decomposition._dict_learning import MiniBatchDictionaryLearning as _MiniBatchDictionaryLearning

class MiniBatchDictionaryLearning(SklearnTransformer, _MiniBatchDictionaryLearning):
    def __init__(
        self,
        alpha: Discrete(min=0, max=2),
        n_iter: Discrete(min=500, max=2000),
        fit_algorithm: Categorical('cd', 'lars'),
        batch_size: Discrete(min=1, max=6),
        shuffle: Boolean(),
        transform_algorithm: Categorical('lars', 'lasso_cd', 'lasso_lars', 'omp', 'threshold'),
        split_sign: Boolean(),
        positive_code: Boolean(),
        positive_dict: Boolean(),
        transform_max_iter: Discrete(min=500, max=2000)
    ):
        SklearnTransformer.__init__(self)
        _MiniBatchDictionaryLearning.__init__(
            self,
            alpha=alpha,
            n_iter=n_iter,
            fit_algorithm=fit_algorithm,
            batch_size=batch_size,
            shuffle=shuffle,
            transform_algorithm=transform_algorithm,
            split_sign=split_sign,
            positive_code=positive_code,
            positive_dict=positive_dict,
            transform_max_iter=transform_max_iter
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.decomposition._factor_analysis import FactorAnalysis as _FactorAnalysis

class FactorAnalysis(SklearnTransformer, _FactorAnalysis):
    def __init__(
        self,
        tol: Continuous(min=0.0001, max=1),
        svd_method: Categorical('lapack', 'randomized'),
        iterated_power: Discrete(min=1, max=6)
    ):
        SklearnTransformer.__init__(self)
        _FactorAnalysis.__init__(
            self,
            tol=tol,
            svd_method=svd_method,
            iterated_power=iterated_power
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.decomposition._fastica import FastICA as _FastICA

class FastICA(SklearnTransformer, _FastICA):
    def __init__(
        self,
        whiten: Boolean(),
        tol: Continuous(min=1e-06, max=1)
    ):
        SklearnTransformer.__init__(self)
        _FastICA.__init__(
            self,
            whiten=whiten,
            tol=tol
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.decomposition._incremental_pca import IncrementalPCA as _IncrementalPCA

class IncrementalPCA(SklearnTransformer, _IncrementalPCA):
    def __init__(
        self,
        whiten: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _IncrementalPCA.__init__(
            self,
            whiten=whiten
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.decomposition._kernel_pca import KernelPCA as _KernelPCA

class KernelPCA(SklearnTransformer, _KernelPCA):
    def __init__(
        self,
        degree: Discrete(min=1, max=6),
        coef0: Discrete(min=0, max=2),
        alpha: Continuous(min=1e-06, max=1),
        fit_inverse_transform: Boolean(),
        eigen_solver: Categorical('arpack', 'auto', 'dense'),
        tol: Discrete(min=-100, max=100),
        remove_zero_eig: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _KernelPCA.__init__(
            self,
            degree=degree,
            coef0=coef0,
            alpha=alpha,
            fit_inverse_transform=fit_inverse_transform,
            eigen_solver=eigen_solver,
            tol=tol,
            remove_zero_eig=remove_zero_eig
        )

    def run(self, input: MatrixContinuous()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.decomposition._lda import LatentDirichletAllocation as _LatentDirichletAllocation

class LatentDirichletAllocation(SklearnTransformer, _LatentDirichletAllocation):
    def __init__(
        self,
        n_components: Discrete(min=5, max=20),
        learning_method: Categorical('batch', 'online'),
        learning_decay: Continuous(min=1e-06, max=1),
        learning_offset: Continuous(min=5.0, max=20.0),
        batch_size: Discrete(min=64, max=256),
        evaluate_every: Discrete(min=-1, max=-2),
        total_samples: Continuous(min=500000.0, max=2000000.0),
        perp_tol: Continuous(min=0.001, max=1),
        mean_change_tol: Continuous(min=1e-05, max=1),
        max_doc_update_iter: Discrete(min=50, max=200)
    ):
        SklearnTransformer.__init__(self)
        _LatentDirichletAllocation.__init__(
            self,
            n_components=n_components,
            learning_method=learning_method,
            learning_decay=learning_decay,
            learning_offset=learning_offset,
            batch_size=batch_size,
            evaluate_every=evaluate_every,
            total_samples=total_samples,
            perp_tol=perp_tol,
            mean_change_tol=mean_change_tol,
            max_doc_update_iter=max_doc_update_iter
        )

    def run(self, input: MatrixContinuous()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.decomposition._nmf import NMF as _NMF

class NMF(SklearnTransformer, _NMF):
    def __init__(
        self,
        solver: Categorical('cd', 'mu'),
        beta_loss: Categorical('frobenius'),
        tol: Continuous(min=1e-06, max=1),
        alpha: Continuous(min=-1, max=1),
        l1_ratio: Continuous(min=-1, max=1),
        shuffle: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _NMF.__init__(
            self,
            solver=solver,
            beta_loss=beta_loss,
            tol=tol,
            alpha=alpha,
            l1_ratio=l1_ratio,
            shuffle=shuffle
        )

    def run(self, input: MatrixContinuous()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.decomposition._pca import PCA as _PCA

class PCA(SklearnTransformer, _PCA):
    def __init__(
        self,
        whiten: Boolean(),
        svd_solver: Categorical('arpack', 'auto', 'full', 'randomized'),
        tol: Continuous(min=-1, max=1),
        iterated_power: Categorical('auto', 'randomized')
    ):
        SklearnTransformer.__init__(self)
        _PCA.__init__(
            self,
            whiten=whiten,
            svd_solver=svd_solver,
            tol=tol,
            iterated_power=iterated_power
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.decomposition._sparse_pca import MiniBatchSparsePCA as _MiniBatchSparsePCA

class MiniBatchSparsePCA(SklearnTransformer, _MiniBatchSparsePCA):
    def __init__(
        self,
        alpha: Discrete(min=0, max=2),
        ridge_alpha: Continuous(min=0.0001, max=1),
        n_iter: Discrete(min=50, max=200),
        batch_size: Discrete(min=1, max=6),
        shuffle: Boolean(),
        method: Categorical('cd', 'lars')
    ):
        SklearnTransformer.__init__(self)
        _MiniBatchSparsePCA.__init__(
            self,
            alpha=alpha,
            ridge_alpha=ridge_alpha,
            n_iter=n_iter,
            batch_size=batch_size,
            shuffle=shuffle,
            method=method
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.decomposition._sparse_pca import SparsePCA as _SparsePCA

class SparsePCA(SklearnTransformer, _SparsePCA):
    def __init__(
        self,
        alpha: Discrete(min=0, max=2),
        ridge_alpha: Continuous(min=0.0001, max=1),
        tol: Continuous(min=1e-10, max=1),
        method: Categorical('cd', 'lars')
    ):
        SklearnTransformer.__init__(self)
        _SparsePCA.__init__(
            self,
            alpha=alpha,
            ridge_alpha=ridge_alpha,
            tol=tol,
            method=method
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.decomposition._truncated_svd import TruncatedSVD as _TruncatedSVD

class TruncatedSVD(SklearnTransformer, _TruncatedSVD):
    def __init__(
        self,
        n_components: Discrete(min=1, max=4),
        n_iter: Discrete(min=2, max=10),
        tol: Continuous(min=-1, max=1)
    ):
        SklearnTransformer.__init__(self)
        _TruncatedSVD.__init__(
            self,
            n_components=n_components,
            n_iter=n_iter,
            tol=tol
        )

    def run(self, input: MatrixContinuous()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.feature_extraction.text import CountVectorizer as _CountVectorizer

class CountVectorizer(SklearnTransformer, _CountVectorizer):
    def __init__(
        self,
        lowercase: Boolean(),
        max_df: Continuous(min=1e-06, max=1),
        min_df: Discrete(min=0, max=2),
        binary: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _CountVectorizer.__init__(
            self,
            lowercase=lowercase,
            max_df=max_df,
            min_df=min_df,
            binary=binary
        )

    def run(self, input: List(Word())) -> MatrixContinuousSparse():
       return super().run(input)

from sklearn.feature_extraction.text import HashingVectorizer as _HashingVectorizer

class HashingVectorizer(SklearnTransformer, _HashingVectorizer):
    def __init__(
        self,
        input: Categorical('content', 'file', 'filename', 'read'),
        decode_error: Categorical('ignore', 'replace', 'strict'),
        lowercase: Boolean(),
        token_pattern: Categorical('word'),
        analyzer: Categorical('char', 'char_wb', 'word'),
        n_features: Discrete(min=524288, max=2097152),
        binary: Boolean(),
        norm: Categorical('l1'),
        alternate_sign: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _HashingVectorizer.__init__(
            self,
            input=input,
            decode_error=decode_error,
            lowercase=lowercase,
            token_pattern=token_pattern,
            analyzer=analyzer,
            n_features=n_features,
            binary=binary,
            norm=norm,
            alternate_sign=alternate_sign
        )

    def run(self, input: List(Word())) -> MatrixContinuousSparse():
       return super().run(input)

from sklearn.feature_extraction.text import TfidfTransformer as _TfidfTransformer

class TfidfTransformer(SklearnTransformer, _TfidfTransformer):
    def __init__(
        self,
        norm: Categorical('l1', 'l2'),
        use_idf: Boolean(),
        smooth_idf: Boolean(),
        sublinear_tf: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _TfidfTransformer.__init__(
            self,
            norm=norm,
            use_idf=use_idf,
            smooth_idf=smooth_idf,
            sublinear_tf=sublinear_tf
        )

    def run(self, input: MatrixContinuous()) -> MatrixContinuousSparse():
       return super().run(input)

from sklearn.feature_extraction.text import TfidfVectorizer as _TfidfVectorizer

class TfidfVectorizer(SklearnTransformer, _TfidfVectorizer):
    def __init__(
        self,
        lowercase: Boolean(),
        max_df: Continuous(min=1e-06, max=1),
        min_df: Discrete(min=0, max=2),
        binary: Boolean(),
        use_idf: Boolean(),
        smooth_idf: Boolean(),
        sublinear_tf: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _TfidfVectorizer.__init__(
            self,
            lowercase=lowercase,
            max_df=max_df,
            min_df=min_df,
            binary=binary,
            use_idf=use_idf,
            smooth_idf=smooth_idf,
            sublinear_tf=sublinear_tf
        )

    def run(self, input: List(Word())) -> MatrixContinuousSparse():
       return super().run(input)

from sklearn.impute._knn import KNNImputer as _KNNImputer

class KNNImputer(SklearnTransformer, _KNNImputer):
    def __init__(
        self,
        missing_values: Continuous(min=nan, max=nan),
        n_neighbors: Discrete(min=2, max=10),
        weights: Categorical('distance', 'uniform'),
        metric: Categorical('nan_euclidean'),
        add_indicator: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _KNNImputer.__init__(
            self,
            missing_values=missing_values,
            n_neighbors=n_neighbors,
            weights=weights,
            metric=metric,
            add_indicator=add_indicator
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.linear_model._base import LinearRegression as _LinearRegression

class LinearRegression(SklearnEstimator, _LinearRegression):
    def __init__(
        self,
        fit_intercept: Boolean(),
        normalize: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _LinearRegression.__init__(
            self,
            fit_intercept=fit_intercept,
            normalize=normalize
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._bayes import ARDRegression as _ARDRegression

class ARDRegression(SklearnEstimator, _ARDRegression):
    def __init__(
        self,
        n_iter: Discrete(min=150, max=600),
        tol: Continuous(min=1e-05, max=1),
        alpha_1: Continuous(min=1e-08, max=1),
        alpha_2: Continuous(min=1e-08, max=1),
        lambda_1: Continuous(min=1e-08, max=1),
        lambda_2: Continuous(min=1e-08, max=1),
        compute_score: Boolean(),
        threshold_lambda: Continuous(min=5000.0, max=20000.0),
        fit_intercept: Boolean(),
        normalize: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _ARDRegression.__init__(
            self,
            n_iter=n_iter,
            tol=tol,
            alpha_1=alpha_1,
            alpha_2=alpha_2,
            lambda_1=lambda_1,
            lambda_2=lambda_2,
            compute_score=compute_score,
            threshold_lambda=threshold_lambda,
            fit_intercept=fit_intercept,
            normalize=normalize
        )

    def run(self, input: MatrixContinuousDense()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._bayes import BayesianRidge as _BayesianRidge

class BayesianRidge(SklearnEstimator, _BayesianRidge):
    def __init__(
        self,
        n_iter: Discrete(min=150, max=600),
        tol: Continuous(min=1e-05, max=1),
        alpha_1: Continuous(min=1e-08, max=1),
        alpha_2: Continuous(min=1e-08, max=1),
        lambda_1: Continuous(min=1e-08, max=1),
        lambda_2: Continuous(min=1e-08, max=1),
        compute_score: Boolean(),
        fit_intercept: Boolean(),
        normalize: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _BayesianRidge.__init__(
            self,
            n_iter=n_iter,
            tol=tol,
            alpha_1=alpha_1,
            alpha_2=alpha_2,
            lambda_1=lambda_1,
            lambda_2=lambda_2,
            compute_score=compute_score,
            fit_intercept=fit_intercept,
            normalize=normalize
        )

    def run(self, input: MatrixContinuousDense()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._coordinate_descent import ElasticNet as _ElasticNet

class ElasticNet(SklearnEstimator, _ElasticNet):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        l1_ratio: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Boolean(),
        tol: Continuous(min=1e-06, max=1),
        positive: Boolean(),
        selection: Categorical('cyclic', 'random')
    ):
        SklearnEstimator.__init__(self)
        _ElasticNet.__init__(
            self,
            alpha=alpha,
            l1_ratio=l1_ratio,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            tol=tol,
            positive=positive,
            selection=selection
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._coordinate_descent import Lasso as _Lasso

class Lasso(SklearnEstimator, _Lasso):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Boolean(),
        tol: Continuous(min=1e-06, max=1),
        positive: Boolean(),
        selection: Categorical('cyclic', 'random')
    ):
        SklearnEstimator.__init__(self)
        _Lasso.__init__(
            self,
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            tol=tol,
            positive=positive,
            selection=selection
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._huber import HuberRegressor as _HuberRegressor

class HuberRegressor(SklearnEstimator, _HuberRegressor):
    def __init__(
        self,
        epsilon: Continuous(min=0.675, max=2.7),
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1.0000000000000001e-07, max=1)
    ):
        SklearnEstimator.__init__(self)
        _HuberRegressor.__init__(
            self,
            epsilon=epsilon,
            alpha=alpha,
            fit_intercept=fit_intercept,
            tol=tol
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._least_angle import Lars as _Lars

class Lars(SklearnEstimator, _Lars):
    def __init__(
        self,
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Categorical('auto'),
        n_nonzero_coefs: Discrete(min=250, max=1000),
        fit_path: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _Lars.__init__(
            self,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            n_nonzero_coefs=n_nonzero_coefs,
            fit_path=fit_path
        )

    def run(self, input: MatrixContinuousDense()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._least_angle import LassoLars as _LassoLars

class LassoLars(SklearnEstimator, _LassoLars):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Categorical('auto'),
        fit_path: Boolean(),
        positive: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _LassoLars.__init__(
            self,
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            fit_path=fit_path,
            positive=positive
        )

    def run(self, input: MatrixContinuousDense()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._least_angle import LassoLarsIC as _LassoLarsIC

class LassoLarsIC(SklearnEstimator, _LassoLarsIC):
    def __init__(
        self,
        criterion: Categorical('aic', 'bic'),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Categorical('auto'),
        positive: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _LassoLarsIC.__init__(
            self,
            criterion=criterion,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute,
            positive=positive
        )

    def run(self, input: MatrixContinuousDense()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._logistic import LogisticRegression as _LogisticRegression

class LogisticRegression(SklearnEstimator, _LogisticRegression):
    def __init__(
        self,
        penalty: Categorical('l2', 'none'),
        dual: Boolean(),
        tol: Continuous(min=1e-06, max=1),
        C: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        intercept_scaling: Discrete(min=0, max=2),
        solver: Categorical('lbfgs', 'liblinear', 'sag', 'saga'),
        multi_class: Categorical('auto', 'multinomial', 'ovr')
    ):
        SklearnEstimator.__init__(self)
        _LogisticRegression.__init__(
            self,
            penalty=penalty,
            dual=dual,
            tol=tol,
            C=C,
            fit_intercept=fit_intercept,
            intercept_scaling=intercept_scaling,
            solver=solver,
            multi_class=multi_class
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.linear_model._omp import OrthogonalMatchingPursuit as _OrthogonalMatchingPursuit

class OrthogonalMatchingPursuit(SklearnEstimator, _OrthogonalMatchingPursuit):
    def __init__(
        self,
        fit_intercept: Boolean(),
        normalize: Boolean(),
        precompute: Categorical('auto')
    ):
        SklearnEstimator.__init__(self)
        _OrthogonalMatchingPursuit.__init__(
            self,
            fit_intercept=fit_intercept,
            normalize=normalize,
            precompute=precompute
        )

    def run(self, input: MatrixContinuousDense()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._passive_aggressive import PassiveAggressiveClassifier as _PassiveAggressiveClassifier

class PassiveAggressiveClassifier(SklearnEstimator, _PassiveAggressiveClassifier):
    def __init__(
        self,
        C: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        early_stopping: Boolean(),
        validation_fraction: Continuous(min=0.001, max=1),
        n_iter_no_change: Discrete(min=2, max=10),
        shuffle: Boolean(),
        average: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _PassiveAggressiveClassifier.__init__(
            self,
            C=C,
            fit_intercept=fit_intercept,
            tol=tol,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            shuffle=shuffle,
            average=average
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.linear_model._passive_aggressive import PassiveAggressiveRegressor as _PassiveAggressiveRegressor

class PassiveAggressiveRegressor(SklearnEstimator, _PassiveAggressiveRegressor):
    def __init__(
        self,
        C: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        early_stopping: Boolean(),
        validation_fraction: Continuous(min=0.001, max=1),
        n_iter_no_change: Discrete(min=2, max=10),
        shuffle: Boolean(),
        epsilon: Continuous(min=0.001, max=1),
        average: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _PassiveAggressiveRegressor.__init__(
            self,
            C=C,
            fit_intercept=fit_intercept,
            tol=tol,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            shuffle=shuffle,
            epsilon=epsilon,
            average=average
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._perceptron import Perceptron as _Perceptron

class Perceptron(SklearnEstimator, _Perceptron):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        shuffle: Boolean(),
        eta0: Continuous(min=1e-06, max=1),
        early_stopping: Boolean(),
        validation_fraction: Continuous(min=0.001, max=1),
        n_iter_no_change: Discrete(min=2, max=10)
    ):
        SklearnEstimator.__init__(self)
        _Perceptron.__init__(
            self,
            alpha=alpha,
            fit_intercept=fit_intercept,
            tol=tol,
            shuffle=shuffle,
            eta0=eta0,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.linear_model._ridge import Ridge as _Ridge

class Ridge(SklearnEstimator, _Ridge):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        solver: Categorical('auto', 'cholesky', 'lsqr', 'saga', 'sparse_cg', 'svd')
    ):
        SklearnEstimator.__init__(self)
        _Ridge.__init__(
            self,
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            tol=tol,
            solver=solver
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._ridge import RidgeClassifier as _RidgeClassifier

class RidgeClassifier(SklearnEstimator, _RidgeClassifier):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        normalize: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        solver: Categorical('auto', 'cholesky', 'lsqr', 'saga', 'sparse_cg', 'svd')
    ):
        SklearnEstimator.__init__(self)
        _RidgeClassifier.__init__(
            self,
            alpha=alpha,
            fit_intercept=fit_intercept,
            normalize=normalize,
            tol=tol,
            solver=solver
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.linear_model._stochastic_gradient import SGDClassifier as _SGDClassifier

class SGDClassifier(SklearnEstimator, _SGDClassifier):
    def __init__(
        self,
        loss: Categorical('epsilon_insensitive', 'hinge', 'huber', 'log', 'modified_huber', 'perceptron', 'squared_epsilon_insensitive', 'squared_hinge', 'squared_loss'),
        penalty: Categorical('elasticnet', 'l1', 'l2'),
        alpha: Continuous(min=1e-06, max=1),
        l1_ratio: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        shuffle: Boolean(),
        epsilon: Continuous(min=0.001, max=1),
        learning_rate: Categorical('optimal'),
        eta0: Continuous(min=-1, max=1),
        power_t: Continuous(min=1e-06, max=1),
        early_stopping: Boolean(),
        validation_fraction: Continuous(min=0.001, max=1),
        n_iter_no_change: Discrete(min=2, max=10),
        average: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _SGDClassifier.__init__(
            self,
            loss=loss,
            penalty=penalty,
            alpha=alpha,
            l1_ratio=l1_ratio,
            fit_intercept=fit_intercept,
            tol=tol,
            shuffle=shuffle,
            epsilon=epsilon,
            learning_rate=learning_rate,
            eta0=eta0,
            power_t=power_t,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            average=average
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.linear_model._stochastic_gradient import SGDRegressor as _SGDRegressor

class SGDRegressor(SklearnEstimator, _SGDRegressor):
    def __init__(
        self,
        loss: Categorical('epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_loss'),
        penalty: Categorical('elasticnet', 'l1', 'l2'),
        alpha: Continuous(min=1e-06, max=1),
        l1_ratio: Continuous(min=1e-06, max=1),
        fit_intercept: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        shuffle: Boolean(),
        epsilon: Continuous(min=0.001, max=1),
        learning_rate: Categorical('adaptive', 'constant', 'invscaling', 'optimal'),
        eta0: Continuous(min=0.0001, max=1),
        power_t: Continuous(min=1e-06, max=1),
        early_stopping: Boolean(),
        validation_fraction: Continuous(min=0.001, max=1),
        n_iter_no_change: Discrete(min=2, max=10),
        average: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _SGDRegressor.__init__(
            self,
            loss=loss,
            penalty=penalty,
            alpha=alpha,
            l1_ratio=l1_ratio,
            fit_intercept=fit_intercept,
            tol=tol,
            shuffle=shuffle,
            epsilon=epsilon,
            learning_rate=learning_rate,
            eta0=eta0,
            power_t=power_t,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            average=average
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.linear_model._theil_sen import TheilSenRegressor as _TheilSenRegressor

class TheilSenRegressor(SklearnEstimator, _TheilSenRegressor):
    def __init__(
        self,
        fit_intercept: Boolean(),
        max_subpopulation: Continuous(min=5000.0, max=20000.0),
        tol: Continuous(min=1e-05, max=1)
    ):
        SklearnEstimator.__init__(self)
        _TheilSenRegressor.__init__(
            self,
            fit_intercept=fit_intercept,
            max_subpopulation=max_subpopulation,
            tol=tol
        )

    def run(self, input: MatrixContinuousDense()) -> ContinuousVector():
       return super().run(input)

from sklearn.manifold._isomap import Isomap as _Isomap

class Isomap(SklearnTransformer, _Isomap):
    def __init__(
        self,
        n_neighbors: Discrete(min=2, max=10),
        n_components: Discrete(min=1, max=4),
        eigen_solver: Categorical('arpack', 'auto', 'dense'),
        tol: Discrete(min=-100, max=100),
        path_method: Categorical('auto'),
        neighbors_algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        p: Discrete(min=1, max=4)
    ):
        SklearnTransformer.__init__(self)
        _Isomap.__init__(
            self,
            n_neighbors=n_neighbors,
            n_components=n_components,
            eigen_solver=eigen_solver,
            tol=tol,
            path_method=path_method,
            neighbors_algorithm=neighbors_algorithm,
            p=p
        )

    def run(self, input: MatrixContinuous()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.manifold._locally_linear import LocallyLinearEmbedding as _LocallyLinearEmbedding

class LocallyLinearEmbedding(SklearnTransformer, _LocallyLinearEmbedding):
    def __init__(
        self,
        n_neighbors: Discrete(min=2, max=10),
        n_components: Discrete(min=1, max=4),
        reg: Continuous(min=1e-05, max=1),
        eigen_solver: Categorical('arpack', 'auto', 'dense'),
        tol: Continuous(min=1e-08, max=1),
        method: Categorical('ltsa', 'modified', 'standard'),
        hessian_tol: Continuous(min=1e-06, max=1),
        modified_tol: Continuous(min=1e-14, max=1),
        neighbors_algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree')
    ):
        SklearnTransformer.__init__(self)
        _LocallyLinearEmbedding.__init__(
            self,
            n_neighbors=n_neighbors,
            n_components=n_components,
            reg=reg,
            eigen_solver=eigen_solver,
            tol=tol,
            method=method,
            hessian_tol=hessian_tol,
            modified_tol=modified_tol,
            neighbors_algorithm=neighbors_algorithm
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.naive_bayes import BernoulliNB as _BernoulliNB

class BernoulliNB(SklearnEstimator, _BernoulliNB):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        binarize: Continuous(min=-1, max=1),
        fit_prior: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _BernoulliNB.__init__(
            self,
            alpha=alpha,
            binarize=binarize,
            fit_prior=fit_prior
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.naive_bayes import CategoricalNB as _CategoricalNB

class CategoricalNB(SklearnEstimator, _CategoricalNB):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_prior: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _CategoricalNB.__init__(
            self,
            alpha=alpha,
            fit_prior=fit_prior
        )

    def run(self, input: MatrixContinuousDense()) -> CategoricalVector():
       return super().run(input)

from sklearn.naive_bayes import ComplementNB as _ComplementNB

class ComplementNB(SklearnEstimator, _ComplementNB):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_prior: Boolean(),
        norm: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _ComplementNB.__init__(
            self,
            alpha=alpha,
            fit_prior=fit_prior,
            norm=norm
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.naive_bayes import GaussianNB as _GaussianNB

class GaussianNB(SklearnEstimator, _GaussianNB):
    def __init__(
        self,
        var_smoothing: Continuous(min=1.0000000000000001e-11, max=1)
    ):
        SklearnEstimator.__init__(self)
        _GaussianNB.__init__(
            self,
            var_smoothing=var_smoothing
        )

    def run(self, input: MatrixContinuousDense()) -> CategoricalVector():
       return super().run(input)

from sklearn.naive_bayes import MultinomialNB as _MultinomialNB

class MultinomialNB(SklearnEstimator, _MultinomialNB):
    def __init__(
        self,
        alpha: Continuous(min=1e-06, max=1),
        fit_prior: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _MultinomialNB.__init__(
            self,
            alpha=alpha,
            fit_prior=fit_prior
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.neighbors._classification import KNeighborsClassifier as _KNeighborsClassifier

class KNeighborsClassifier(SklearnEstimator, _KNeighborsClassifier):
    def __init__(
        self,
        n_neighbors: Discrete(min=2, max=10),
        weights: Categorical('distance', 'uniform'),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        p: Discrete(min=1, max=4),
        metric: Categorical('minkowski')
    ):
        SklearnEstimator.__init__(self)
        _KNeighborsClassifier.__init__(
            self,
            n_neighbors=n_neighbors,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.neighbors._classification import RadiusNeighborsClassifier as _RadiusNeighborsClassifier

class RadiusNeighborsClassifier(SklearnEstimator, _RadiusNeighborsClassifier):
    def __init__(
        self,
        radius: Continuous(min=1e-06, max=1),
        weights: Categorical('distance', 'uniform'),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        p: Discrete(min=1, max=4),
        metric: Categorical('minkowski')
    ):
        SklearnEstimator.__init__(self)
        _RadiusNeighborsClassifier.__init__(
            self,
            radius=radius,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.neighbors._graph import KNeighborsTransformer as _KNeighborsTransformer

class KNeighborsTransformer(SklearnTransformer, _KNeighborsTransformer):
    def __init__(
        self,
        mode: Categorical('connectivity', 'distance'),
        n_neighbors: Discrete(min=2, max=10),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        metric: Categorical('braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'l1', 'l2', 'mahalanobis', 'manhattan', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'),
        p: Discrete(min=1, max=4)
    ):
        SklearnTransformer.__init__(self)
        _KNeighborsTransformer.__init__(
            self,
            mode=mode,
            n_neighbors=n_neighbors,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p
        )

    def run(self, input: MatrixContinuous()) -> MatrixContinuousSparse():
       return super().run(input)

from sklearn.neighbors._graph import RadiusNeighborsTransformer as _RadiusNeighborsTransformer

class RadiusNeighborsTransformer(SklearnTransformer, _RadiusNeighborsTransformer):
    def __init__(
        self,
        mode: Categorical('connectivity', 'distance'),
        radius: Continuous(min=1e-06, max=1),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        metric: Categorical('braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'l1', 'l2', 'manhattan', 'minkowski', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'),
        p: Discrete(min=1, max=4)
    ):
        SklearnTransformer.__init__(self)
        _RadiusNeighborsTransformer.__init__(
            self,
            mode=mode,
            radius=radius,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p
        )

    def run(self, input: MatrixContinuous()) -> MatrixContinuousSparse():
       return super().run(input)

from sklearn.neighbors._lof import LocalOutlierFactor as _LocalOutlierFactor

class LocalOutlierFactor(SklearnEstimator, _LocalOutlierFactor):
    def __init__(
        self,
        n_neighbors: Discrete(min=10, max=40),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        metric: Categorical('braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'l1', 'l2', 'manhattan', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'),
        p: Discrete(min=1, max=4),
        contamination: Categorical('auto'),
        novelty: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _LocalOutlierFactor.__init__(
            self,
            n_neighbors=n_neighbors,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p,
            contamination=contamination,
            novelty=novelty
        )

    def run(self, input: MatrixContinuous()) -> DiscreteVector():
       return super().run(input)

from sklearn.neighbors._nearest_centroid import NearestCentroid as _NearestCentroid

class NearestCentroid(SklearnEstimator, _NearestCentroid):
    def __init__(
        self,

    ):
        SklearnEstimator.__init__(self)
        _NearestCentroid.__init__(
            self,

        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.neighbors._regression import KNeighborsRegressor as _KNeighborsRegressor

class KNeighborsRegressor(SklearnEstimator, _KNeighborsRegressor):
    def __init__(
        self,
        n_neighbors: Discrete(min=2, max=10),
        weights: Categorical('distance', 'uniform'),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        p: Discrete(min=1, max=4),
        metric: Categorical('minkowski')
    ):
        SklearnEstimator.__init__(self)
        _KNeighborsRegressor.__init__(
            self,
            n_neighbors=n_neighbors,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.neighbors._regression import RadiusNeighborsRegressor as _RadiusNeighborsRegressor

class RadiusNeighborsRegressor(SklearnEstimator, _RadiusNeighborsRegressor):
    def __init__(
        self,
        radius: Continuous(min=1e-06, max=1),
        weights: Categorical('distance', 'uniform'),
        algorithm: Categorical('auto', 'ball_tree', 'brute', 'kd_tree'),
        leaf_size: Discrete(min=15, max=60),
        p: Discrete(min=1, max=4),
        metric: Categorical('minkowski')
    ):
        SklearnEstimator.__init__(self)
        _RadiusNeighborsRegressor.__init__(
            self,
            radius=radius,
            weights=weights,
            algorithm=algorithm,
            leaf_size=leaf_size,
            p=p,
            metric=metric
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.preprocessing._data import KernelCenterer as _KernelCenterer

class KernelCenterer(SklearnTransformer, _KernelCenterer):
    def __init__(
        self,

    ):
        SklearnTransformer.__init__(self)
        _KernelCenterer.__init__(
            self,

        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.preprocessing._data import MinMaxScaler as _MinMaxScaler

class MinMaxScaler(SklearnTransformer, _MinMaxScaler):
    def __init__(
        self,

    ):
        SklearnTransformer.__init__(self)
        _MinMaxScaler.__init__(
            self,

        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.preprocessing._data import PowerTransformer as _PowerTransformer

class PowerTransformer(SklearnTransformer, _PowerTransformer):
    def __init__(
        self,
        standardize: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _PowerTransformer.__init__(
            self,
            standardize=standardize
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.preprocessing._data import RobustScaler as _RobustScaler

class RobustScaler(SklearnTransformer, _RobustScaler):
    def __init__(
        self,
        with_centering: Boolean(),
        with_scaling: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _RobustScaler.__init__(
            self,
            with_centering=with_centering,
            with_scaling=with_scaling
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.preprocessing._data import StandardScaler as _StandardScaler

class StandardScaler(SklearnTransformer, _StandardScaler):
    def __init__(
        self,
        with_mean: Boolean(),
        with_std: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _StandardScaler.__init__(
            self,
            with_mean=with_mean,
            with_std=with_std
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.preprocessing._discretization import KBinsDiscretizer as _KBinsDiscretizer

class KBinsDiscretizer(SklearnTransformer, _KBinsDiscretizer):
    def __init__(
        self,
        n_bins: Discrete(min=2, max=10),
        encode: Categorical('onehot', 'ordinal'),
        strategy: Categorical('kmeans', 'quantile', 'uniform')
    ):
        SklearnTransformer.__init__(self)
        _KBinsDiscretizer.__init__(
            self,
            n_bins=n_bins,
            encode=encode,
            strategy=strategy
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousSparse():
       return super().run(input)

from sklearn.preprocessing._encoders import OneHotEncoder as _OneHotEncoder

class OneHotEncoder(SklearnTransformer, _OneHotEncoder):
    def __init__(
        self,
        categories: Categorical('auto'),
        sparse: Boolean(),
        handle_unknown: Categorical('error', 'ignore')
    ):
        SklearnTransformer.__init__(self)
        _OneHotEncoder.__init__(
            self,
            categories=categories,
            sparse=sparse,
            handle_unknown=handle_unknown
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousSparse():
       return super().run(input)

from sklearn.preprocessing._encoders import OrdinalEncoder as _OrdinalEncoder

class OrdinalEncoder(SklearnTransformer, _OrdinalEncoder):
    def __init__(
        self,
        categories: Categorical('auto')
    ):
        SklearnTransformer.__init__(self)
        _OrdinalEncoder.__init__(
            self,
            categories=categories
        )

    def run(self, input: MatrixContinuousDense()) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.preprocessing._label import LabelBinarizer as _LabelBinarizer

class LabelBinarizer(SklearnTransformer, _LabelBinarizer):
    def __init__(
        self,
        neg_label: Discrete(min=-100, max=100),
        pos_label: Discrete(min=0, max=2),
        sparse_output: Boolean()
    ):
        SklearnTransformer.__init__(self)
        _LabelBinarizer.__init__(
            self,
            neg_label=neg_label,
            pos_label=pos_label,
            sparse_output=sparse_output
        )

    def run(self, input: List(Word())) -> MatrixContinuousDense():
       return super().run(input)

from sklearn.svm._classes import LinearSVC as _LinearSVC

class LinearSVC(SklearnEstimator, _LinearSVC):
    def __init__(
        self,
        penalty: Categorical('l2'),
        loss: Categorical('hinge', 'squared_hinge'),
        dual: Boolean(),
        tol: Continuous(min=1e-06, max=1),
        C: Continuous(min=1e-06, max=1),
        multi_class: Categorical('crammer_singer', 'ovr'),
        fit_intercept: Boolean(),
        intercept_scaling: Discrete(min=0, max=2)
    ):
        SklearnEstimator.__init__(self)
        _LinearSVC.__init__(
            self,
            penalty=penalty,
            loss=loss,
            dual=dual,
            tol=tol,
            C=C,
            multi_class=multi_class,
            fit_intercept=fit_intercept,
            intercept_scaling=intercept_scaling
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.svm._classes import LinearSVR as _LinearSVR

class LinearSVR(SklearnEstimator, _LinearSVR):
    def __init__(
        self,
        epsilon: Continuous(min=-1, max=1),
        tol: Continuous(min=1e-06, max=1),
        C: Continuous(min=1e-06, max=1),
        loss: Categorical('epsilon_insensitive', 'squared_epsilon_insensitive'),
        fit_intercept: Boolean(),
        intercept_scaling: Continuous(min=1e-06, max=1),
        dual: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _LinearSVR.__init__(
            self,
            epsilon=epsilon,
            tol=tol,
            C=C,
            loss=loss,
            fit_intercept=fit_intercept,
            intercept_scaling=intercept_scaling,
            dual=dual
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.svm._classes import NuSVC as _NuSVC

class NuSVC(SklearnEstimator, _NuSVC):
    def __init__(
        self,
        nu: Continuous(min=1e-06, max=1),
        kernel: Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree: Discrete(min=1, max=6),
        gamma: Categorical('auto', 'scale'),
        coef0: Continuous(min=-1, max=1),
        shrinking: Boolean(),
        probability: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        cache_size: Discrete(min=100, max=400),
        decision_function_shape: Categorical('ovo', 'ovr'),
        break_ties: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _NuSVC.__init__(
            self,
            nu=nu,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            shrinking=shrinking,
            probability=probability,
            tol=tol,
            cache_size=cache_size,
            decision_function_shape=decision_function_shape,
            break_ties=break_ties
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.svm._classes import NuSVR as _NuSVR

class NuSVR(SklearnEstimator, _NuSVR):
    def __init__(
        self,
        nu: Continuous(min=1e-06, max=1),
        C: Continuous(min=1e-06, max=1),
        kernel: Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree: Discrete(min=1, max=6),
        gamma: Categorical('auto', 'scale'),
        coef0: Continuous(min=-1, max=1),
        shrinking: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        cache_size: Discrete(min=100, max=400)
    ):
        SklearnEstimator.__init__(self)
        _NuSVR.__init__(
            self,
            nu=nu,
            C=C,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            shrinking=shrinking,
            tol=tol,
            cache_size=cache_size
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.svm._classes import OneClassSVM as _OneClassSVM

class OneClassSVM(SklearnEstimator, _OneClassSVM):
    def __init__(
        self,
        kernel: Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree: Discrete(min=1, max=6),
        gamma: Categorical('auto', 'scale'),
        coef0: Continuous(min=-1, max=1),
        tol: Continuous(min=1e-05, max=1),
        nu: Continuous(min=1e-06, max=1),
        shrinking: Boolean(),
        cache_size: Discrete(min=100, max=400)
    ):
        SklearnEstimator.__init__(self)
        _OneClassSVM.__init__(
            self,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            tol=tol,
            nu=nu,
            shrinking=shrinking,
            cache_size=cache_size
        )

    def run(self, input: MatrixContinuous()) -> DiscreteVector():
       return super().run(input)

from sklearn.svm._classes import SVC as _SVC

class SVC(SklearnEstimator, _SVC):
    def __init__(
        self,
        C: Continuous(min=1e-06, max=1),
        degree: Discrete(min=1, max=6),
        gamma: Categorical('auto', 'scale'),
        coef0: Continuous(min=-1, max=1),
        shrinking: Boolean(),
        probability: Boolean(),
        tol: Continuous(min=1e-05, max=1),
        cache_size: Discrete(min=100, max=400),
        decision_function_shape: Categorical('ovo', 'ovr'),
        break_ties: Boolean()
    ):
        SklearnEstimator.__init__(self)
        _SVC.__init__(
            self,
            C=C,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            shrinking=shrinking,
            probability=probability,
            tol=tol,
            cache_size=cache_size,
            decision_function_shape=decision_function_shape,
            break_ties=break_ties
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.svm._classes import SVR as _SVR

class SVR(SklearnEstimator, _SVR):
    def __init__(
        self,
        kernel: Categorical('linear', 'poly', 'rbf', 'sigmoid'),
        degree: Discrete(min=1, max=6),
        gamma: Categorical('auto', 'scale'),
        coef0: Continuous(min=-1, max=1),
        tol: Continuous(min=1e-05, max=1),
        C: Continuous(min=1e-06, max=1),
        epsilon: Continuous(min=0.001, max=1),
        shrinking: Boolean(),
        cache_size: Discrete(min=100, max=400)
    ):
        SklearnEstimator.__init__(self)
        _SVR.__init__(
            self,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            tol=tol,
            C=C,
            epsilon=epsilon,
            shrinking=shrinking,
            cache_size=cache_size
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.tree._classes import DecisionTreeClassifier as _DecisionTreeClassifier

class DecisionTreeClassifier(SklearnEstimator, _DecisionTreeClassifier):
    def __init__(
        self,
        min_samples_split: Discrete(min=1, max=4),
        min_samples_leaf: Discrete(min=0, max=2),
        min_weight_fraction_leaf: Continuous(min=-1, max=1),
        min_impurity_decrease: Continuous(min=-1, max=1),
        ccp_alpha: Continuous(min=-1, max=1)
    ):
        SklearnEstimator.__init__(self)
        _DecisionTreeClassifier.__init__(
            self,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.tree._classes import DecisionTreeRegressor as _DecisionTreeRegressor

class DecisionTreeRegressor(SklearnEstimator, _DecisionTreeRegressor):
    def __init__(
        self,
        min_samples_split: Discrete(min=1, max=4),
        min_samples_leaf: Discrete(min=0, max=2),
        min_weight_fraction_leaf: Continuous(min=-1, max=1),
        min_impurity_decrease: Continuous(min=-1, max=1),
        ccp_alpha: Continuous(min=-1, max=1)
    ):
        SklearnEstimator.__init__(self)
        _DecisionTreeRegressor.__init__(
            self,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)

from sklearn.tree._classes import ExtraTreeClassifier as _ExtraTreeClassifier

class ExtraTreeClassifier(SklearnEstimator, _ExtraTreeClassifier):
    def __init__(
        self,
        min_samples_split: Discrete(min=1, max=4),
        min_samples_leaf: Discrete(min=0, max=2),
        min_weight_fraction_leaf: Continuous(min=-1, max=1),
        min_impurity_decrease: Continuous(min=-1, max=1),
        ccp_alpha: Continuous(min=-1, max=1)
    ):
        SklearnEstimator.__init__(self)
        _ExtraTreeClassifier.__init__(
            self,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )

    def run(self, input: MatrixContinuous()) -> CategoricalVector():
       return super().run(input)

from sklearn.tree._classes import ExtraTreeRegressor as _ExtraTreeRegressor

class ExtraTreeRegressor(SklearnEstimator, _ExtraTreeRegressor):
    def __init__(
        self,
        min_samples_split: Discrete(min=1, max=4),
        min_samples_leaf: Discrete(min=0, max=2),
        min_weight_fraction_leaf: Continuous(min=-1, max=1),
        min_impurity_decrease: Continuous(min=-1, max=1),
        ccp_alpha: Continuous(min=-1, max=1)
    ):
        SklearnEstimator.__init__(self)
        _ExtraTreeRegressor.__init__(
            self,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            min_impurity_decrease=min_impurity_decrease,
            ccp_alpha=ccp_alpha
        )

    def run(self, input: MatrixContinuous()) -> ContinuousVector():
       return super().run(input)
