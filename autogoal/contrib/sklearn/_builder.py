import sklearn
import sklearn.cluster
import sklearn.cross_decomposition
import sklearn.feature_extraction
import textwrap
import datetime
import inspect
import re
import enlighten
import numpy as np

from pathlib import Path

from autogoal.grammar import Discrete, Continuous, Categorical, Boolean


def build_sklearn_wrappers():
    imports = _walk(sklearn)

    manager = enlighten.get_manager()
    counter = manager.counter(total=len(imports), unit="classes")

    with open(Path(__file__).parent / "_generated.py", "w") as fp:
        fp.write(textwrap.dedent(
            f"""
            # AUTOGENERATED ON {datetime.datetime.now()}
            ## DO NOT MODIFY THIS FILE MANUALLY

            """
        ))
        
        for cls in imports:
            counter.update()
            _write_class(cls, fp)

    counter.close()
    manager.stop()


def _write_class(cls, fp):
    args = _get_args(cls)
    s = " " * 4
    args_str = f",\n{s * 4}".join(f"{key}:{value}" for key, value in args.items())
    self_str = f"\n{s * 4}".join(f"self.{key}={key}" for key in args)
    init_str = f",\n{s * 5}".join(f"{key}={key}" for key in args)

    fp.write(textwrap.dedent(
        f"""
        from {cls.__module__} import {cls.__name__}

        class {cls.__name__}({cls.__name__}):
            def __init__(
                self,
                {args_str}
            ):
                {self_str}

                super().__init__(
                    self,
                    {init_str}
                )

        """
    ))

    fp.flush()


def _is_algorithm(cls, verbose=False):
    if hasattr(cls, "fit"):
        return True
    else:
        if verbose:
            warnings.warn("%r doesn't have `fit`" % cls)

    if hasattr(cls, "transform"):
        return True
    else:
        if verbose:
            warnings.warn("%r doesn't have `transform`" % cls)

    return False


def _walk(module, name="sklearn"):
    imports = []

    def _walk_p(module, name="sklearn"):
        all_elements = module.__all__

        for elem in all_elements:

            if elem == "exceptions":
                continue

            name = name + "." + elem

            try:
                obj = getattr(module, elem)

                if isinstance(obj, type):
                    if name.endswith("CV"):
                        continue

                    if not _is_algorithm(obj):
                        continue

                    imports.append(obj)

                _walk_p(obj, name)
            except:
                pass

            try:
                inner_module = importlib.import_module(name)
                _walk_p(inner_module, name)
            except:
                pass

    _walk_p(module, name)

    imports.sort(key=lambda c: (c.__module__, c.__name__))
    return imports


def _find_parameter_values(parameter, cls):
    documentation = []
    lines = cls.__doc__.split("\n")

    while lines:
        l = lines.pop(0)
        if l.strip().startswith(parameter):
            documentation.append(l)
            tabs = l.index(parameter)
            break

    while lines:
        l = lines.pop(0)

        if not l.strip():
            continue

        if l.startswith(" " * (tabs + 1)):
            documentation.append(l)
        else:
            break

    options = set(re.findall(r"'(\w+)'", " ".join(documentation)))
    valid = []
    invalid = []
    skip = set(["deprecated", "auto_deprecated", "precomputed"])

    for opt in options:
        opt = opt.lower()
        if opt in skip:
            continue
        try:
            cls(**{parameter: opt}).fit(np.ones((10, 10)), [True] * 5 + [False] * 5)
            valid.append(opt)
        except Exception as e:
            invalid.append(opt)

    return sorted(valid)


def _get_args(cls):
    specs = inspect.getfullargspec(cls.__init__)

    args = specs.args
    specs = specs.defaults

    if not args or not specs:
        return {}

    args = args[-len(specs) :]

    args_map = {k: v for k, v in zip(args, specs)}

    drop_args = [
        "verbose",
        "random_state",
        "n_jobs",
        "max_iter",
        "class_weight",
        "warm_start",
        "copy_X",
        "copy_x",
        "copy",
        "eps",
    ]

    for arg in drop_args:
        args_map.pop(arg, None)

    result = {}

    for arg, value in args_map.items():
        values = _get_arg_values(arg, value, cls)
        if not values:
            continue
        result[arg] = values

    return result


def _get_arg_values(arg, value, cls):
    if isinstance(value, bool):
        return Boolean()
    if isinstance(value, int):
        return Discrete(*_get_integer_values(arg, value, cls))
    if isinstance(value, float):
        return Continuous(*_get_float_values(arg, value, cls))
    if isinstance(value, str):
        values = _find_parameter_values(arg, cls)
        return Categorical(*values) if values else None

    return None


def _get_integer_values(arg, value, cls):
    if value == 0:
        min_val = -100
        max_val = 100
    else:
        min_val = value // 2
        max_val = 2 * value

    return min_val, max_val


def _get_float_values(arg, value, cls):
    if value == 0:
        min_val = -1
        max_val = 1
    elif 0 < value <= 0.1:
        min_val = value / 100
        max_val = 1
    elif 0 < value <= 1:
        min_val = 1e-6
        max_val = 1
    else:
        min_val = value / 2
        max_val = 2 * value

    return min_val, max_val


if __name__ == "__main__":
    build_sklearn_wrappers()
