import datetime
import inspect
import re
import textwrap
import warnings
from pathlib import Path

import enlighten
import numpy as np
import sklearn
import sklearn.cluster
import sklearn.cross_decomposition
import sklearn.feature_extraction
import sklearn.impute
import sklearn.naive_bayes

from autogoal.contrib.sklearn._utils import get_input_output, is_algorithm
from autogoal.grammar import Boolean, Categorical, Continuous, Discrete

import abc


class SklearnWrapper(metaclass=abc.ABCMeta):
    def __init__(self):
        self._mode = 'train'

    def train(self):
        self._mode = 'train'

    def eval(self):
        self._mode = 'eval'

    def run(self, input):
        if self._mode == 'train':
            return self._train(input)
        elif self._mode == 'eval':
            return self._eval(input)

        raise ValueError("Invalid mode: %s" % self._mode)

    @abc.abstractmethod
    def _train(self, input):
        pass

    @abc.abstractmethod
    def _eval(self, input):
        pass


class SklearnEstimator(SklearnWrapper):
    def _train(self, input):
        X, y = input
        self.fit(X, y)
        return X, y

    def _eval(self, input):
        X, _ = input
        return X, self.predict(X)

    @abc.abstractmethod
    def fit(self, X, y):
        pass

    @abc.abstractmethod
    def predict(self, X):
        pass


class SklearnTransformer(SklearnWrapper):
    def _train(self, input):
        X, y = input
        return self.fit_transform(X), y

    def _eval(self, input):
        X, y = input
        return self.transform(X), y

    @abc.abstractmethod
    def fit_transform(self, X, y=None):
        pass

    @abc.abstractmethod
    def transform(self, X, y=None):
        pass


GENERATION_RULES = dict(
    LatentDirichletAllocation=dict(
        ignore_params=set(['evaluate_every'])
    ),
    RadiusNeighborsClassifier=dict(
        ignore=True,
    ),
    KNeighborsTransformer=dict(
        ignore_params=set(['metric'])
    ),
    RadiusNeighborsTransformer=dict(
        ignore_params=set(['metric'])
    ),
    LocalOutlierFactor=dict(
        ignore_params=set(['metric'])
    ),
    RadiusNeighborsRegressor=dict(
        ignore_params=set(['metric'])
    ),
    LabelBinarizer=dict(
        ignore_params=set(['neg_label', 'pos_label'])
    ),
    HashingVectorizer=dict(
        ignore_params=set(['token_pattern', 'analyzer', 'input', 'decode_error'])
    )
)


def build_sklearn_wrappers():
    imports = _walk(sklearn)

    # manager = enlighten.get_manager()
    # counter = manager.counter(total=len(imports), unit="classes")

    with open(Path(__file__).parent / "_generated.py", "w") as fp:
        fp.write(textwrap.dedent(
            f"""
            # AUTOGENERATED ON {datetime.datetime.now()}
            ## DO NOT MODIFY THIS FILE MANUALLY

            from numpy import inf, nan

            from autogoal.grammar import Continuous, Discrete, Categorical, Boolean
            from autogoal.contrib.sklearn._builder import SklearnEstimator, SklearnTransformer
            from autogoal.kb import *
            """
        ))

        for cls in imports:
            # counter.update()
            _write_class(cls, fp)

    # counter.close()
    # manager.stop()

def _write_class(cls, fp):
    rules = GENERATION_RULES.get(cls.__name__)

    if rules:
        if rules.get('ignore'):
            return

        ignore_args = rules.get('ignore_params', [])
    else:
        ignore_args = []

    args = _get_args(cls)
    inputs, outputs = get_input_output(cls)

    for a in ignore_args:
        args.pop(a)

    if not inputs:
        warnings.warn("Cannot find correct types for %r" % cls)
        return

    s = " " * 4
    args_str = f",\n{s * 4}".join(f"{key}: {value}" for key, value in args.items())
    # self_str = f"\n{s * 4}".join(f"self.{key}={key}" for key in args)
    init_str = f",\n{s * 5}".join(f"{key}={key}" for key in args)
    input_str, output_str = repr(inputs), repr(outputs)
    base_class = 'SklearnEstimator' if is_algorithm(cls) == 'estimator' else 'SklearnTransformer'

    print(cls)

    fp.write(textwrap.dedent(
        f"""
        from {cls.__module__} import {cls.__name__} as _{cls.__name__}

        class {cls.__name__}(_{cls.__name__}, {base_class}):
            def __init__(
                self,
                {args_str}
            ):
                {base_class}.__init__(self)
                _{cls.__name__}.__init__(
                    self,
                    {init_str}
                )

            def run(self, input: {input_str}) -> {output_str}:
               return {base_class}.run(self, input)
        """
    ))

    fp.flush()


def _is_algorithm(cls, verbose=False):
    if hasattr(cls, "fit"):
        return True
    else:
        if verbose:
            warnings.warn("%r doesn't have `fit`" % cls)

    if hasattr(cls, "transform"):
        return True
    else:
        if verbose:
            warnings.warn("%r doesn't have `transform`" % cls)

    return False


def _walk(module, name="sklearn"):
    imports = []

    def _walk_p(module, name="sklearn"):
        all_elements = module.__all__

        for elem in all_elements:

            if elem == "exceptions":
                continue

            name = name + "." + elem

            try:
                obj = getattr(module, elem)

                if isinstance(obj, type):
                    if name.endswith("CV"):
                        continue

                    if not _is_algorithm(obj):
                        continue

                    imports.append(obj)

                _walk_p(obj, name)
            except: # Exception as e:
                pass

            try:
                inner_module = importlib.import_module(name)
                _walk_p(inner_module, name)
            except:
                pass

    _walk_p(module, name)

    imports.sort(key=lambda c: (c.__module__, c.__name__))
    return imports


def _find_parameter_values(parameter, cls):
    documentation = []
    lines = cls.__doc__.split("\n")

    while lines:
        l = lines.pop(0)
        if l.strip().startswith(parameter):
            documentation.append(l)
            tabs = l.index(parameter)
            break

    while lines:
        l = lines.pop(0)

        if not l.strip():
            continue

        if l.startswith(" " * (tabs + 1)):
            documentation.append(l)
        else:
            break

    options = set(re.findall(r"'(\w+)'", " ".join(documentation)))
    valid = []
    invalid = []
    skip = set(["deprecated", "auto_deprecated", "precomputed"])

    for opt in options:
        opt = opt.lower()

        if opt in skip:
            continue

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            if _try(cls, parameter, opt):
                valid.append(opt)
            else:
                invalid.append(opt)

    if valid:
        return Categorical(*sorted(valid))

    return None


def _try(cls, arg, value):
    try:
        cls(**{arg: value}).fit(np.ones((10, 10)), [True] * 5 + [False] * 5)
        return True
    except:
        return False


def _get_args(cls):
    specs = inspect.getfullargspec(cls.__init__)

    args = specs.args
    specs = specs.defaults

    if not args or not specs:
        return {}

    args = args[-len(specs) :]

    args_map = {k: v for k, v in zip(args, specs)}

    drop_args = [
        "verbose",
        "random_state",
        "n_jobs",
        "max_iter",
        "class_weight",
        "warm_start",
        "copy_X",
        "copy_x",
        "copy",
        "eps",
    ]

    for arg in drop_args:
        args_map.pop(arg, None)

    result = {}

    for arg, value in args_map.items():
        values = _get_arg_values(arg, value, cls)
        if not values:
            continue
        result[arg] = values

    return result


def _get_arg_values(arg, value, cls):
    if isinstance(value, bool):
        return Boolean()
    if isinstance(value, int):
        return _get_integer_values(arg, value, cls)
    if isinstance(value, float):
        return Continuous(*_get_float_values(arg, value, cls))
    if isinstance(value, str):
        return _find_parameter_values(arg, cls)

    return None


def _get_integer_values(arg, value, cls):
    min_value = -1000000
    max_value =  1000000

    # binary search for minimum value
    current_value = min_value

    while current_value < value:
        if not _try(cls, arg, current_value):
            next_value = int((current_value + value) / 2)
            if next_value == current_value:
                min_value = value
                break
        else:
            min_value = current_value
            break

    # binary search for maximum value
    current_value = max_value

    while current_value > value:
        if not _try(cls, arg, current_value):
            next_value = int((current_value + value) / 2)
            if next_value == current_value:
                max_value = value
                break
        else:
            max_value = current_value
            break

    if min_value < max_value:
        return Discrete(min=min_value, max=max_value)

    return None


def _get_float_values(arg, value, cls):
    min_value = -1000.0
    max_value =  1000.0

    # binary search for minimum value
    current_value = min_value

    while current_value < value:
        if not _try(cls, arg, current_value):
            next_value = (current_value + value) / 2
            if abs(next_value - current_value) < 1e-6:
                min_value = value
                break
        else:
            min_value = current_value

    # binary search for maximum value
    current_value = max_value

    while current_value > value:
        if not _try(cls, arg, current_value):
            next_value = (current_value + value) / 2
            if next_value == current_value:
                max_value = value
                break
        else:
            max_value = current_value

    if min_value <= max_value - 1.0:
        return Continuous(min=min_value, max=max_value)

    return None


if __name__ == "__main__":
    build_sklearn_wrappers()
