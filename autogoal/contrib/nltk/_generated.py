# AUTOGENERATED ON 2020-01-26 12:10:42.965786
## DO NOT MODIFY THIS FILE MANUALLY

from autogoal.grammar import Continuous, Discrete, Categorical, Boolean
from autogoal.contrib.nltk._builder import (
    NltkStemmer,
    NltkTokenizer,
    NltkLemmatizer,
)
from autogoal.kb._data import *
from autogoal.utils import nice_repr
from numpy import inf, nan

from nltk.stem.cistem import Cistem as _Cistem


@nice_repr
class Cistem(_Cistem, NltkStemmer):
    def __init__(self, case_insensitive: Boolean()):
        NltkStemmer.__init__(self)
        _Cistem.__init__(self, case_insensitive=case_insensitive)

    def run(self, input: Word()) -> Stem():
        return NltkStemmer.run(self, input)


from nltk.stem.isri import ISRIStemmer as _ISRIStemmer


@nice_repr
class ISRIStemmer(_ISRIStemmer, NltkStemmer):
    def __init__(self,):
        NltkStemmer.__init__(self)
        _ISRIStemmer.__init__(self,)

    def run(self, input: Word()) -> Stem():
        return NltkStemmer.run(self, input)


from nltk.stem.lancaster import LancasterStemmer as _LancasterStemmer


@nice_repr
class LancasterStemmer(_LancasterStemmer, NltkStemmer):
    def __init__(self, strip_prefix_flag: Boolean()):
        NltkStemmer.__init__(self)
        _LancasterStemmer.__init__(self, strip_prefix_flag=strip_prefix_flag)

    def run(self, input: Word()) -> Stem():
        return NltkStemmer.run(self, input)


from nltk.stem.porter import PorterStemmer as _PorterStemmer


@nice_repr
class PorterStemmer(_PorterStemmer, NltkStemmer):
    def __init__(self,):
        NltkStemmer.__init__(self)
        _PorterStemmer.__init__(self,)

    def run(self, input: Word()) -> Stem():
        return NltkStemmer.run(self, input)


from nltk.stem.rslp import RSLPStemmer as _RSLPStemmer


@nice_repr
class RSLPStemmer(_RSLPStemmer, NltkStemmer):
    def __init__(self,):
        NltkStemmer.__init__(self)
        _RSLPStemmer.__init__(self,)

    def run(self, input: Word()) -> Stem():
        return NltkStemmer.run(self, input)


from nltk.stem.snowball import SnowballStemmer as _SnowballStemmer


@nice_repr
class SnowballStemmer(_SnowballStemmer, NltkStemmer):
    def __init__(
        self,
        language: Categorical(
            "italian",
            "hungarian",
            "romanian",
            "russian",
            "spanish",
            "swedish",
            "norwegian",
            "dutch",
            "german",
            "finnish",
            "danish",
            "french",
            "arabic",
            "english",
            "portuguese",
        ),
    ):
        NltkStemmer.__init__(self)
        _SnowballStemmer.__init__(self, language=language)

    def run(self, input: Word()) -> Stem():
        return NltkStemmer.run(self, input)


from nltk.stem.wordnet import WordNetLemmatizer as _WordNetLemmatizer


@nice_repr
class WordNetLemmatizer(_WordNetLemmatizer, NltkLemmatizer):
    def __init__(self,):
        NltkLemmatizer.__init__(self)
        _WordNetLemmatizer.__init__(self,)

    def run(self, input: Word()) -> Stem():
        return NltkLemmatizer.run(self, input)


from nltk.tokenize.casual import TweetTokenizer as _TweetTokenizer


@nice_repr
class TweetTokenizer(_TweetTokenizer, NltkTokenizer):
    def __init__(
        self, preserve_case: Boolean(), reduce_len: Boolean(), strip_handles: Boolean()
    ):
        NltkTokenizer.__init__(self)
        _TweetTokenizer.__init__(
            self,
            preserve_case=preserve_case,
            reduce_len=reduce_len,
            strip_handles=strip_handles,
        )

    def run(self, input: Sentence()) -> List(Word()):
        return NltkTokenizer.run(self, input)


from nltk.tokenize.mwe import MWETokenizer as _MWETokenizer


@nice_repr
class MWETokenizer(_MWETokenizer, NltkTokenizer):
    def __init__(self,):
        NltkTokenizer.__init__(self)
        _MWETokenizer.__init__(self,)

    def run(self, input: Sentence()) -> List(Word()):
        return NltkTokenizer.run(self, input)


from nltk.tokenize.punkt import PunktSentenceTokenizer as _PunktSentenceTokenizer


@nice_repr
class PunktSentenceTokenizer(_PunktSentenceTokenizer, NltkTokenizer):
    def __init__(self, verbose: Boolean()):
        NltkTokenizer.__init__(self)
        _PunktSentenceTokenizer.__init__(self, verbose=verbose)

    def run(self, input: Document()) -> List(Sentence()):
        return NltkTokenizer.run(self, input)


from nltk.tokenize.regexp import BlanklineTokenizer as _BlanklineTokenizer


@nice_repr
class BlanklineTokenizer(_BlanklineTokenizer, NltkTokenizer):
    def __init__(self,):
        NltkTokenizer.__init__(self)
        _BlanklineTokenizer.__init__(self,)

    def run(self, input: Document()) -> List(Sentence()):
        return NltkTokenizer.run(self, input)


from nltk.tokenize.regexp import WhitespaceTokenizer as _WhitespaceTokenizer


@nice_repr
class WhitespaceTokenizer(_WhitespaceTokenizer, NltkTokenizer):
    def __init__(self,):
        NltkTokenizer.__init__(self)
        _WhitespaceTokenizer.__init__(self,)

    def run(self, input: Sentence()) -> List(Word()):
        return NltkTokenizer.run(self, input)


from nltk.tokenize.regexp import WordPunctTokenizer as _WordPunctTokenizer


@nice_repr
class WordPunctTokenizer(_WordPunctTokenizer, NltkTokenizer):
    def __init__(self,):
        NltkTokenizer.__init__(self)
        _WordPunctTokenizer.__init__(self,)

    def run(self, input: Sentence()) -> List(Word()):
        return NltkTokenizer.run(self, input)


from nltk.tokenize.sexpr import SExprTokenizer as _SExprTokenizer


@nice_repr
class SExprTokenizer(_SExprTokenizer, NltkTokenizer):
    def __init__(self, strict: Boolean()):
        NltkTokenizer.__init__(self)
        _SExprTokenizer.__init__(self, strict=strict)

    def run(self, input: Document()) -> List(Sentence()):
        return NltkTokenizer.run(self, input)


from nltk.tokenize.simple import LineTokenizer as _LineTokenizer


@nice_repr
class LineTokenizer(_LineTokenizer, NltkTokenizer):
    def __init__(self,):
        NltkTokenizer.__init__(self)
        _LineTokenizer.__init__(self,)

    def run(self, input: Document()) -> List(Sentence()):
        return NltkTokenizer.run(self, input)


from nltk.tokenize.simple import SpaceTokenizer as _SpaceTokenizer


@nice_repr
class SpaceTokenizer(_SpaceTokenizer, NltkTokenizer):
    def __init__(self,):
        NltkTokenizer.__init__(self)
        _SpaceTokenizer.__init__(self,)

    def run(self, input: Sentence()) -> List(Word()):
        return NltkTokenizer.run(self, input)


from nltk.tokenize.simple import TabTokenizer as _TabTokenizer


@nice_repr
class TabTokenizer(_TabTokenizer, NltkTokenizer):
    def __init__(self,):
        NltkTokenizer.__init__(self)
        _TabTokenizer.__init__(self,)

    def run(self, input: Document()) -> List(Sentence()):
        return NltkTokenizer.run(self, input)


from nltk.tokenize.toktok import ToktokTokenizer as _ToktokTokenizer


@nice_repr
class ToktokTokenizer(_ToktokTokenizer, NltkTokenizer):
    def __init__(self,):
        NltkTokenizer.__init__(self)
        _ToktokTokenizer.__init__(self,)

    def run(self, input: Sentence()) -> List(Word()):
        return NltkTokenizer.run(self, input)


from nltk.tokenize.treebank import TreebankWordTokenizer as _TreebankWordTokenizer


@nice_repr
class TreebankWordTokenizer(_TreebankWordTokenizer, NltkTokenizer):
    def __init__(self,):
        NltkTokenizer.__init__(self)
        _TreebankWordTokenizer.__init__(self,)

    def run(self, input: Sentence()) -> List(Word()):
        return NltkTokenizer.run(self, input)
