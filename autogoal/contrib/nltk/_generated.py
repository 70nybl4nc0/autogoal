
# AUTOGENERATED ON 2020-01-11 12:39:40.436484
## DO NOT MODIFY THIS FILE MANUALLY

from autogoal.grammar import Continuous, Discrete, Categorical, Boolean
from numpy import inf, nan

from nltk.parse.corenlp import CoreNLPDependencyParser as _CoreNLPDependencyParser

class CoreNLPDependencyParser(_CoreNLPDependencyParser):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.parse.corenlp import CoreNLPParser as _CoreNLPParser

class CoreNLPParser(_CoreNLPParser):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.stem.cistem import Cistem as _Cistem

class Cistem(_Cistem):
    def __init__(
        self,
        case_insensitive: Boolean()
    ):
        self.case_insensitive=case_insensitive

        super().__init__(
            case_insensitive=case_insensitive
        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def dstem(
        self,
        document
    ):
        return [self.stem(word) for word in document]

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.dstem(x) for x in X]

from nltk.stem.isri import ISRIStemmer as _ISRIStemmer

class ISRIStemmer(_ISRIStemmer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def dstem(
        self,
        document
    ):
        return [self.stem(word) for word in document]

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.dstem(x) for x in X]

from nltk.stem.lancaster import LancasterStemmer as _LancasterStemmer

class LancasterStemmer(_LancasterStemmer):
    def __init__(
        self,
        strip_prefix_flag: Boolean()
    ):
        self.strip_prefix_flag=strip_prefix_flag

        super().__init__(
            strip_prefix_flag=strip_prefix_flag
        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def dstem(
        self,
        document
    ):
        return [self.stem(word) for word in document]

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.dstem(x) for x in X]

from nltk.stem.porter import PorterStemmer as _PorterStemmer

class PorterStemmer(_PorterStemmer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def dstem(
        self,
        document
    ):
        return [self.stem(word) for word in document]

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.dstem(x) for x in X]

from nltk.stem.rslp import RSLPStemmer as _RSLPStemmer

class RSLPStemmer(_RSLPStemmer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def dstem(
        self,
        document
    ):
        return [self.stem(word) for word in document]

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.dstem(x) for x in X]

from nltk.stem.snowball import SnowballStemmer as _SnowballStemmer

class SnowballStemmer(_SnowballStemmer):
    def __init__(
        self,
        ignore_stopwords: Boolean(),
        language: Categorical('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'english', 'arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'german', 'german', 'german')
    ):
        self.ignore_stopwords=ignore_stopwords
        self.language=language

        super().__init__(
            ignore_stopwords=ignore_stopwords,
            language=language
        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def dstem(
        self,
        document
    ):
        return [self.stem(word) for word in document]

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.dstem(x) for x in X]

from nltk.tokenize.casual import TweetTokenizer as _TweetTokenizer

class TweetTokenizer(_TweetTokenizer):
    def __init__(
        self,
        preserve_case: Boolean(),
        reduce_len: Boolean(),
        strip_handles: Boolean()
    ):
        self.preserve_case=preserve_case
        self.reduce_len=reduce_len
        self.strip_handles=strip_handles

        super().__init__(
            preserve_case=preserve_case,
            reduce_len=reduce_len,
            strip_handles=strip_handles
        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.mwe import MWETokenizer as _MWETokenizer

class MWETokenizer(_MWETokenizer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.regexp import BlanklineTokenizer as _BlanklineTokenizer

class BlanklineTokenizer(_BlanklineTokenizer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.regexp import WhitespaceTokenizer as _WhitespaceTokenizer

class WhitespaceTokenizer(_WhitespaceTokenizer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.regexp import WordPunctTokenizer as _WordPunctTokenizer

class WordPunctTokenizer(_WordPunctTokenizer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.sexpr import SExprTokenizer as _SExprTokenizer

class SExprTokenizer(_SExprTokenizer):
    def __init__(
        self,
        strict: Boolean()
    ):
        self.strict=strict

        super().__init__(
            strict=strict
        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.simple import LineTokenizer as _LineTokenizer

class LineTokenizer(_LineTokenizer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.simple import SpaceTokenizer as _SpaceTokenizer

class SpaceTokenizer(_SpaceTokenizer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.simple import TabTokenizer as _TabTokenizer

class TabTokenizer(_TabTokenizer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.sonority_sequencing import SyllableTokenizer as _SyllableTokenizer

class SyllableTokenizer(_SyllableTokenizer):
    def __init__(
        self,
        sonority_hierarchy: Boolean()
    ):
        self.sonority_hierarchy=sonority_hierarchy

        super().__init__(
            sonority_hierarchy=sonority_hierarchy
        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.stanford_segmenter import StanfordSegmenter as _StanfordSegmenter

class StanfordSegmenter(_StanfordSegmenter):
    def __init__(
        self,
        verbose: Boolean()
    ):
        self.verbose=verbose

        super().__init__(
            verbose=verbose
        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.texttiling import TextTilingTokenizer as _TextTilingTokenizer

class TextTilingTokenizer(_TextTilingTokenizer):
    def __init__(
        self,
        w: Discrete(min=10, max=40),
        k: Discrete(min=5, max=20),
        similarity_method: Discrete(min=-100, max=100),
        smoothing_width: Discrete(min=1, max=4),
        smoothing_rounds: Discrete(min=0, max=2),
        cutoff_policy: Discrete(min=0, max=2),
        demo_mode: Boolean()
    ):
        self.w=w
        self.k=k
        self.similarity_method=similarity_method
        self.smoothing_width=smoothing_width
        self.smoothing_rounds=smoothing_rounds
        self.cutoff_policy=cutoff_policy
        self.demo_mode=demo_mode

        super().__init__(
            w=w,
            k=k,
            similarity_method=similarity_method,
            smoothing_width=smoothing_width,
            smoothing_rounds=smoothing_rounds,
            cutoff_policy=cutoff_policy,
            demo_mode=demo_mode
        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.toktok import ToktokTokenizer as _ToktokTokenizer

class ToktokTokenizer(_ToktokTokenizer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]

from nltk.tokenize.treebank import TreebankWordTokenizer as _TreebankWordTokenizer

class TreebankWordTokenizer(_TreebankWordTokenizer):
    def __init__(
        self,

    ):


        super().__init__(

        )

    def fit_transform(
        self, 
        X, 
        y=None
    ):
        self.fit(X, y=None)
        return self.transform(X)

    def fit(
        self, 
        X, 
        y=None
    ):
        pass    

    def transform(
        self, 
        X,
        y=None
    ):
        return [self.tokenize(x) for x in X]
