# `autogoal.contrib.nltk`

## Classes

### [`AffixTagger`](../autogoal.contrib.nltk.AffixTagger)
### [`BigramTagger`](../autogoal.contrib.nltk.BigramTagger)
### [`BlanklineTokenizer`](../autogoal.contrib.nltk.BlanklineTokenizer)
> Tokenize a string, treating any sequence of blank lines as a delimiter.

### [`Cistem`](../autogoal.contrib.nltk.Cistem)
> CISTEM Stemmer for German

### [`ClassifierBasedPOSTagger`](../autogoal.contrib.nltk.ClassifierBasedPOSTagger)
### [`ISRIStemmer`](../autogoal.contrib.nltk.ISRIStemmer)
> ISRI Arabic stemmer based on algorithm: Arabic Stemming without a root dictionary.

### [`LancasterStemmer`](../autogoal.contrib.nltk.LancasterStemmer)
> Lancaster Stemmer

### [`LineTokenizer`](../autogoal.contrib.nltk.LineTokenizer)
> Tokenize a string into its lines, optionally discarding blank lines.

### [`MWETokenizer`](../autogoal.contrib.nltk.MWETokenizer)
> A tokenizer that processes tokenized text and merges multi-word expressions

### [`PerceptronTagger`](../autogoal.contrib.nltk.PerceptronTagger)
> Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal.

### [`PorterStemmer`](../autogoal.contrib.nltk.PorterStemmer)
> A word stemmer based on the Porter stemming algorithm.

### [`PunktSentenceTokenizer`](../autogoal.contrib.nltk.PunktSentenceTokenizer)
> A sentence tokenizer which uses an unsupervised algorithm to build

### [`RSLPStemmer`](../autogoal.contrib.nltk.RSLPStemmer)
> A stemmer for Portuguese.

### [`SExprTokenizer`](../autogoal.contrib.nltk.SExprTokenizer)
> A tokenizer that divides strings into s-expressions.

### [`SnowballStemmer`](../autogoal.contrib.nltk.SnowballStemmer)
> Snowball Stemmer

### [`SpaceTokenizer`](../autogoal.contrib.nltk.SpaceTokenizer)
> Tokenize a string using the space character as a delimiter,

### [`TabTokenizer`](../autogoal.contrib.nltk.TabTokenizer)
> Tokenize a string use the tab character as a delimiter,

### [`TnT`](../autogoal.contrib.nltk.TnT)
> TnT - Statistical POS tagger

### [`ToktokTokenizer`](../autogoal.contrib.nltk.ToktokTokenizer)
> This is a Python port of the tok-tok.pl from

### [`TreebankWordTokenizer`](../autogoal.contrib.nltk.TreebankWordTokenizer)
> The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.

### [`TrigramTagger`](../autogoal.contrib.nltk.TrigramTagger)
### [`TweetTokenizer`](../autogoal.contrib.nltk.TweetTokenizer)
> Tokenizer for tweets.

### [`UnigramTagger`](../autogoal.contrib.nltk.UnigramTagger)
### [`WhitespaceTokenizer`](../autogoal.contrib.nltk.WhitespaceTokenizer)
> Tokenize a string on whitespace (space, tab, newline).

### [`WordNetLemmatizer`](../autogoal.contrib.nltk.WordNetLemmatizer)
> WordNet Lemmatizer

### [`WordPunctTokenizer`](../autogoal.contrib.nltk.WordPunctTokenizer)
> Tokenize a text into a sequence of alphabetic and

